<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>天青色等烟雨</title>
  
  <subtitle>文不在多、有换则新、人不在挤、有来就行</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="yunke.science/"/>
  <updated>2019-03-01T10:42:28.321Z</updated>
  <id>yunke.science/</id>
  
  <author>
    <name>Young</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>V2Ray 配置</title>
    <link href="yunke.science/2019/03/01/v2ray-proxy/"/>
    <id>yunke.science/2019/03/01/v2ray-proxy/</id>
    <published>2019-03-01T10:40:24.000Z</published>
    <updated>2019-03-01T10:42:28.321Z</updated>
    
    <content type="html"><![CDATA[<p>V2Ray 是一个与 Shadowsocks 类似的代理软件，可以用来科学上网（翻墙）学习国外先进科学技术</p><a id="more"></a><p>@[TOC]</p><h2><span id="什么是-v2ray">什么是 V2Ray？</span></h2><p>V2Ray 是 Project V 下的一个工具。Project V 是一个包含一系列构建特定网络环境工具的项目，而 V2Ray 属于最核心的一个。 官方中介绍Project V 提供了单一的内核和多种界面操作方式。内核（V2Ray）用于实际的网络交互、路由等针对网络数据的处理，而外围的用户界面程序提供了方便直接的操作流程。不过从时间上来说，先有 V2Ray 才有 Project V。 如果还是不理解，那么简单地说，<strong>V2Ray 是一个与 Shadowsocks 类似的代理软件，可以用来科学上网（翻墙）学习国外先进科学技术</strong>。</p><h2><span id="v2ray-跟-shadowsocks-有什么区别">V2Ray 跟 Shadowsocks 有什么区别？</span></h2><p>Shadowsocks 功能单一，V2Ray 功能强大。听起来似乎有点贬低 Shadowsocks 呢？当然不！换一个角度来看，Shadowsocks 简单好上手，V2Ray 复杂配置多。</p><h2><span id="既然-v2ray-复杂为什么要用它">既然 V2Ray 复杂，为什么要用它？</span></h2><p>相对来说，V2Ray 有以下优势：</p><ul><li>更完善的协议: V2Ray 使用了新的自行研发的 VMess 协议，改正了 Shadowsocks 一些已有的缺点，更难被墙检测到</li><li>更强大的性能: 网络性能更好，具体数据可以看 V2Ray 官方博客</li><li>更丰富的功能: 以下是部分 V2Ray 的功能</li><li><strong>mKCP</strong>: KCP 协议在 V2Ray 上的实现，不必另行安装 kcptun</li><li><strong>动态端口</strong>：动态改变通信的端口，对抗对长时间大流量端口的限速封锁</li><li>路由功能：可以随意设定指定数据包的流向，去广告、反跟踪都可以</li><li>传出代理：看名字可能不太好理解，其实差不多可以称之为多重代理。类似于 Tor 的代理</li><li>数据包伪装：类似于 Shadowsocks-rss 的混淆，另外对于 mKCP 的数据包也可伪装，伪装常见流量，令识别更困难</li><li>WebSocket 协议：可以 PaaS 平台搭建V2Ray，通过 WebSocket 代理。也可以通过它使用 CDN 中转，抗封锁效果更好</li><li>Mux:多路复用，进一步提高科学上网的并发性能</li></ul><h2><span id="安装">安装</span></h2><p>V2Ray 分服务端、客户端</p><p>服务端部署在linux<br>客户端部署在window <a href="https://github.com/2dust/v2rayN" target="_blank" rel="noopener">https://github.com/2dust/v2rayN</a></p><h3><span id="linux-安装">linux 安装：</span></h3><p><code>bash &lt;(curl -L -s https://install.direct/go.sh)</code></p><p>运行脚本位于系统的以下位置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/etc/systemd/system/v2ray.service: Systemd</span><br><span class="line">/etc/init.d/v2ray: SysV</span><br><span class="line"></span><br><span class="line">编辑 /etc/v2ray/config.json 文件来配置你需要的代理方式；</span><br><span class="line">运行 service v2ray start [CentOS6启动]来启动 V2Ray 进程；</span><br><span class="line">systemctl start v2ray [CentOS7启动]</span><br></pre></td></tr></table></figure><h3><span id="服务端配置">服务端配置</span></h3><p>使用配置生成器，直接生成配置，复制即可</p><p>配置生成器: <a href="https://intmainreturn0.com/v2ray-config-gen/" target="_blank" rel="noopener">https://intmainreturn0.com/v2ray-config-gen/</a></p><img src="/2019/03/01/v2ray-proxy/01.png" title="create_config"><h3><span id="windows客户端安装">windows客户端安装</span></h3><p>window系统，下载 v2rayN-Core.zip</p><p><a href="https://github.com/2dust/v2rayN/releases" target="_blank" rel="noopener">https://github.com/2dust/v2rayN/releases</a></p><p>解压，直接运行v2rayN.exe</p><p>添加VMess服务器,添加配置，确定<br><img src="/2019/03/01/v2ray-proxy/02.png" title="add_vmess_config"></p><p>启用http代理</p><p>在右下角托盘图标，点击右键，选择【启用http代理模式】，在【http代理模式】里面选择【开启PAC，并自动配置PAC（PAC模式）】。</p><img src="/2019/03/01/v2ray-proxy/03.png" title="http_proxy"><p>上网测试。</p><p><a href="https://www.google.com.hk/" target="_blank" rel="noopener">https://www.google.com.hk/</a></p><h3><span id="安卓客户端">安卓客户端</span></h3><p>v2rayNG</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;V2Ray 是一个与 Shadowsocks 类似的代理软件，可以用来科学上网（翻墙）学习国外先进科学技术&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="yunke.science/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="proxy" scheme="yunke.science/tags/proxy/"/>
    
  </entry>
  
  <entry>
    <title>不需要安装软件，优雅使用google搜索</title>
    <link href="yunke.science/2019/03/01/searx-search/"/>
    <id>yunke.science/2019/03/01/searx-search/</id>
    <published>2019-03-01T10:15:25.000Z</published>
    <updated>2019-03-01T10:17:20.680Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/asciimoo/searx" target="_blank" rel="noopener">Searx</a>是一个免费的<strong>互联网元搜索引擎</strong>，汇集了70多种搜索服务的结果。 用户既不被跟踪也不被分析。</p><a id="more"></a><p><strong>元搜索引擎（或聚合器</strong>）是一个使用其他搜索引擎工具搜索数据从而产生自己的数据互联网。元搜索引擎从用户那里获取输入，同时向第三方搜索引擎发送查询以获得结果。收集足够的数据，按其等级格式化并呈现给用户。</p><p>Searx 就类似一个搜索引擎结果汇集器，通过把google、bing等70多种搜索引擎结果汇集，反馈给用户数据。我们通过Searx搜索就等于使用google搜索服务。</p><p><a href="https://stats.searx.xyz/" target="_blank" rel="noopener">https://stats.searx.xyz/</a></p><p><strong>这个网站，用于显示有关公共searx可用网站的最新地址汇总。</strong></p><p>打开网站，可以看到列表中的 <code>Public searx Instances</code> ,找一个可以用的就可以了。</p><p>列出两个当前可用的：</p><p><a href="https://searx.pofilo.fr/" target="_blank" rel="noopener">https://searx.pofilo.fr/</a></p><p><a href="https://stats.searx.xyz/" target="_blank" rel="noopener">https://stats.searx.xyz/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/asciimoo/searx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Searx&lt;/a&gt;是一个免费的&lt;strong&gt;互联网元搜索引擎&lt;/strong&gt;，汇集了70多种搜索服务的结果。 用户既不被跟踪也不被分析。&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="yunke.science/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="其他" scheme="yunke.science/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>Jenkins利用Role-based Authorization Strategy插件管理项目权限</title>
    <link href="yunke.science/2019/03/01/jenkins-role-auth/"/>
    <id>yunke.science/2019/03/01/jenkins-role-auth/</id>
    <published>2019-03-01T09:44:59.000Z</published>
    <updated>2019-03-01T10:17:32.627Z</updated>
    
    <content type="html"><![CDATA[<p>Role-based Authorization Strategy 基于角色的策略来管理用户的权限。</p><a id="more"></a><p>@[TOC]</p><p><code>Role-based Authorization Strategy</code>插件添加了一个新的基于角色的策略，以简化和加强用户管理。该策略允许：</p><ul><li>创建全局角色，例如管理员，工作创建者，匿名者等，允许在全球范围内设置总体，从属，作业，运行，查看和SCM权限。</li><li>创建项目角色，允许仅基于项目设置“作业”和“运行”权限。</li><li>创建从属角色，允许设置与节点相关的权限。</li></ul><p>将这些角色分配给用户。</p><p><strong>项目权限需求</strong></p><ol><li>管理员对所有项目具有管理权限</li><li>开发人员和测试人员仅对对应的开发项目、测试项目拥有构建、配置权限</li></ol><h2><span id="安装插件">安装插件</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Role-based Authorization Strategy</span><br></pre></td></tr></table></figure><h2><span id="创建普通用户">创建普通用户</span></h2><p>系统管理-》管理用户</p><p>创建两个用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">devuser</span><br><span class="line">qauser</span><br></pre></td></tr></table></figure><h2><span id="激活角色管理策略">激活角色管理策略</span></h2><p>系统管理-》全局安全配置</p><p>授权策略选择<code>【Role-Based Strategy】</code>，保存</p><h3><span id="创建角色">创建角色</span></h3><p>系统管理-》 <code>Manage and Assign Roles</code></p><p>选择 【Manage Roles 管理角色】</p><p>角色分全局角色和项目角色:</p><ul><li>全局角色赋予全局权限</li><li>项目角色对项目赋权限</li></ul><p>创建一个全局角色 anyone ，赋予全局read读权限</p><p>创建两个项目角色devuser, qauser<br>Pattern 支持正则表达式，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">devuser Pattern .*dev.*</span><br><span class="line">qauser Pattern .*qa.*</span><br></pre></td></tr></table></figure><h3><span id="分配角色">分配角色</span></h3><p>在<code>Globle roles</code> 中，对用户勾选<code>anyone</code>角色</p><p>在<code>Ttem roles</code> 中，选择用户与对应的角色</p><p>保存，退出，测试</p><p>参考：</p><ol><li><a href="https://blog.51cto.com/zengestudy/1782494" target="_blank" rel="noopener">https://blog.51cto.com/zengestudy/1782494</a></li><li><a href="https://wiki.jenkins.io/display/JENKINS/Role+Strategy+Plugin" target="_blank" rel="noopener">https://wiki.jenkins.io/display/JENKINS/Role+Strategy+Plugin</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Role-based Authorization Strategy 基于角色的策略来管理用户的权限。&lt;/p&gt;
    
    </summary>
    
      <category term="jenkins" scheme="yunke.science/categories/jenkins/"/>
    
    
      <category term="jenkins" scheme="yunke.science/tags/jenkins/"/>
    
  </entry>
  
  <entry>
    <title>pod 优先级和抢占机制</title>
    <link href="yunke.science/2018/11/16/k8s-pod-priority/"/>
    <id>yunke.science/2018/11/16/k8s-pod-priority/</id>
    <published>2018-11-16T03:13:12.000Z</published>
    <updated>2018-11-16T03:22:12.104Z</updated>
    
    <content type="html"><![CDATA[<p>Pod可以具有优先级。优先级表示Pod相对于其他Pod的重要性。如果无法调度Pod，则调度程序会尝试抢占（逐出）较低优先级的Pod，以便可以调度待处理的Pod。</p><a id="more"></a><p>@[TOC]</p><blockquote><p>在Kubernetes 1.9及更高版本中，优先级(Priority)还会影响Pod的调度顺序和节点上的资源抢占驱逐。</p></blockquote><blockquote><p>自Kubernetes 1.11以来，Pod优先级和抢占已移至beta版，并且在此版本及更高版本中默认启用。</p></blockquote><blockquote><p>在Kubernetes版本中，Pod优先级和抢占仍然是alpha级别的功能，您需要明确启用它。要在旧版本的Kubernetes中使用这些功能，请按照Kubernetes版本的文档中的说明进行操作，方法是转到Kubernetes版本的文档存档版本。</p></blockquote><h2><span id="如何使用优先级和抢占">如何使用优先级和抢占</span></h2><p>要在Kubernetes 1.11及更高版本中使用优先级和抢占，请按照下列步骤操作：</p><p>添加一个或多个<code>PriorityClasses</code>。</p><p>创建<code>Pod</code>，添加<code>priorityClassName</code>并将其设置为添加的<code>PriorityClasses</code>之一。当然，您不需要直接创建<code>Pod</code>; 通常，您将添加<code>priorityClassName</code>到集合对象的<code>Pod</code>模板，如<code>Deployment</code>。</p><p>如果您尝试该功能然后决定禁用它，则必须删除<code>PodPriority</code>命令行标志或将其设置为<code>false</code>，然后重新启动<code>API</code>服务器和调度程序。禁用此功能后，现有<code>Pod</code>将保留其优先级字段，但禁用抢占，并忽略优先级字段。如果禁用该功能，则<code>priorityClassName</code>无法在新Pod中设置。</p><h2><span id="如何禁用抢占">如何禁用抢占</span></h2><blockquote><p>注意：在Kubernetes 1.11中，关键的容器（DaemonSet容器除外，它们仍由DaemonSet控制器调度）依赖于在群集处于资源压力下时调度程序抢占。因此，如果您决定禁用抢占，则需要运行旧版本的<code>Rescheduler</code>。下面提供了更多相关信息。</p></blockquote><p>在Kubernetes 1.11及更高版本中，抢占机制由<code>kube-scheduler</code>标志控制<code>disablePreemption</code>，该标志默认设置为<code>false</code>。</p><p>此选项仅在组件配置中可用，并且在旧式命令行选项中不可用。以下是禁用抢占的示例组件配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: componentconfig/v1alpha1</span><br><span class="line">kind: KubeSchedulerConfiguration</span><br><span class="line">algorithmSource:</span><br><span class="line">  provider: DefaultProvider</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">disablePreemption: true</span><br></pre></td></tr></table></figure><h2><span id="priorityclass-介绍"><code>PriorityClass</code> 介绍</span></h2><p><code>PriorityClass</code>是一个非命名空间的对象，它定义从优先级类名到优先级的整数值的映射。该名称<code>name</code>在<code>PriorityClass</code>对象的元数据字段中指定。该值在必填<code>value</code>字段中指定。==值越高，优先级越高。==</p><p><code>PriorityClass</code>对象可以具有小于或等于10亿的任何32位整数值。较大的数字保留给通常不会被抢占或驱逐的关键系统<code>Pod</code>。集群管理员应为他们想要的每个这样的映射创建一个<code>PriorityClass</code>对象。</p><p><code>PriorityClass</code>还有两个可选字段：<code>globalDefault</code>和<code>description</code>。该<code>globalDefault</code>字段表示此<code>PriorityClass</code>的值应该用于没有 <code>priorityClassName</code>的<code>Pod</code>。系统中只能存在一个<code>globalDefault</code>设置为<code>true</code>的<code>PriorityClass</code> 。如果没有<code>globalDefault</code>设置的<code>PriorityClass</code> ，则具有<code>no</code>的<code>Pod</code>的优先级 <code>priorityClassName</code>为零。</p><p>该<code>description</code>字段是任意字符串。它旨在告诉用户何时应该使用此<code>PriorityClass</code>。</p><h3><span id="有关podpriority和现有集群的说明">有关PodPriority和现有集群的说明</span></h3><ul><li><p>如果升级现有群集并启用此功能，则现有Pod的优先级实际上为零。</p></li><li><p>添加<code>globalDefault</code>设置为的<code>PriorityClass true</code>不会更改现有Pod的优先级。此类<code>PriorityClass</code>的值仅用于添加<code>PriorityClass</code>后创建的<code>Pod</code>。</p></li><li><p>如果删除<code>PriorityClass</code>，则使用已删除的<code>PriorityClass</code>名称的现有<code>Pod</code>保持不变，但您无法创建使用已删除的<code>PriorityClass</code>名称的Pod</p></li></ul><h3><span id="示例priorityclass">示例PriorityClass</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: scheduling.k8s.io/v1beta1</span><br><span class="line">kind: PriorityClass</span><br><span class="line">metadata:</span><br><span class="line">  name: high-priority</span><br><span class="line">value: 1000000</span><br><span class="line">globalDefault: false</span><br><span class="line">description: &quot;This priority class should be used for XYZ service pods only.&quot;</span><br></pre></td></tr></table></figure><h2><span id="pod优先级配置">Pod优先级配置</span></h2><p>拥有一个或多个<code>PriorityClasses</code>之后，您可以创建在其规范中指定其中一个<code>PriorityClass</code>名称的<code>Pod</code>。优先级许可控制器使用该<code>priorityClassName</code>字段并填充优先级的整数值。如果未找到<code>PriorityClasses</code>，则拒绝创建<code>Pod</code>。</p><p>以下YAML是使用在前面的示例中创建的<code>PriorityClass</code>的<code>Pod</code>配置的示例。优先级许可控制器检查规范并将Pod的优先级解析为<code>1000000</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    env: test</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">  priorityClassName: high-priority</span><br></pre></td></tr></table></figure><h2><span id="pod优先级对调度顺序的影响">Pod优先级对调度顺序的影响</span></h2><p>在Kubernetes 1.9及更高版本中，当启用<code>Pod</code>优先级时，调度程序按其优先级对挂起的Pod进行排序，并将挂起的Pod置于调度队列中优先级较低的其他挂起的Pod之前。因此，如果满足其调度要求，则可以比具有较低优先级的Pod更早地调度更高优先级的Pod。如果无法安排此类Pod，则调度程序将继续并尝试安排其他较低优先级的Pod。</p><h2><span id="抢占">抢占</span></h2><p>创建Pod时，它们会进入队列并等待安排。调度程序从队列中选择一个Pod并尝试在节点上安排它。如果未找到满足Pod的所有指定要求的节点，则会为挂起的Pod触发抢占逻辑。假设待处理的Pod为 P，抢占逻辑尝试找到一个节点，其中删除一个或多个优先级低于P的Pod将使得能够在该节点上调度P. 如果找到这样的节点，则从节点中删除一个或多个较低优先级的Pod。在Pod消失后，可以在节点上安排P.</p><h3><span id="用户暴露的信息">用户暴露的信息</span></h3><p>当Pod P抢占节点N上的一个或多个<code>nominatedNodeNamePod</code>时，Pod P的状态字段被设置为节点N的名称。该字段帮助调度器跟踪为Pod P保留的资源，并且还向用户提供有关其集群中的抢占的信息。</p><p>请注意，Pod P不一定安排到“指定节点”。在受害者Pod被抢先后，他们将获得优雅的终止期。如果在调度正在等待受害的pod终止而另一个节点可用时，调度程序将使用其它节点来安排Pod P.结果 <code>nominatedNodeName</code>和<code>nodeName</code> Pod规范的并不总是相同的。此外，如果调度程序在节点N上抢占Pod，但是然后比Pod P更高优先级的Pod到达，则调度程序可以将节点N提供给新的更高优先级Pod。在这种情况下，调度程序清除<code>nominatedNodeNamePod</code> P.通过执行此操作，调度程序使Pod P有资格抢占另一个节点上的Pod。</p><h2><span id="pod优先级和qos的交互">Pod优先级和QoS的交互</span></h2><p>Pod优先级和 QoS 是两个正交特征，几乎没有交互，也没有基于其QoS类设置Pod优先级的默认限制。调度程序的抢占逻辑在选择抢占目标时会考虑QoS。Preemption考虑Pod优先级并尝试选择具有最低优先级的一组目标。只有在删除最低优先级Pod不足以允许调度程序调度preemptor Pod或者最低优先级Pod受到保护时，才会考虑优先级较高的Pod进行抢占 PodDisruptionBudget。</p><p>考虑QoS和Pod优先级的唯一组件是 Kubelet资源外驱逐。kubelet首先根据他们对饥饿资源的使用是否超过请求，然后按优先级，然后通过消耗饥饿计算资源相对于Pods的调度请求来排除Pods。Kubelet资源外驱逐不会驱逐使用不超过其请求的Pod。如果优先级较低的Pod未超过其请求，则不会被驱逐。另一个优先级高于其请求的Pod可能被驱逐。</p><h2><span id="参考">参考</span></h2><ol><li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pod可以具有优先级。优先级表示Pod相对于其他Pod的重要性。如果无法调度Pod，则调度程序会尝试抢占（逐出）较低优先级的Pod，以便可以调度待处理的Pod。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>jenkins pipeline when 内置判断条件</title>
    <link href="yunke.science/2018/11/05/pipeline-when/"/>
    <id>yunke.science/2018/11/05/pipeline-when/</id>
    <published>2018-11-05T10:32:02.000Z</published>
    <updated>2018-11-05T10:33:01.567Z</updated>
    
    <content type="html"><![CDATA[<p><code>when</code>指令允许<code>Pipeline</code>根据给定条件确定是否应执行该阶段。</p><a id="more"></a><p>@[TOC]</p><h2><span id="when指令概述"><code>when</code>指令概述</span></h2><p><code>when</code>指令允许<code>Pipeline</code>根据给定条件确定是否应执行该阶段。该<code>when</code>指令必须至少包含一个条件。如果<code>when</code>指令包含多个条件，则所有子条件必须返回<code>true</code>才能执行该阶段。这与子条件嵌套在<code>allOf</code>条件中的情况相同（参见下面的示例）。如果使用<code>anyOf</code>条件，请注意一旦找到第一个<code>true</code>条件，条件就会跳过剩余的测试条件。</p><p>更复杂的条件结构可使用嵌套条件建：<code>not</code>，<code>allOf</code>或<code>anyOf</code>。嵌套条件可以嵌套到任意深度。</p><ul><li>是否必须： 非必须</li><li>参数： 没有</li><li>所在位置: 在<code>stage</code>指令内</li></ul><h2><span id="内置条件">内置条件</span></h2><h3><span id="branch">branch</span></h3><p>当正在构建的分支与给定的分支模式匹配时执行阶段。如 <code>when { branch &#39;master&#39; }</code> . 这仅在多分支<code>Pipeline</code>中有效。</p><h3><span id="buildingtag">buildingTag</span></h3><p>构建构建标记时执行阶段。例: <code>when { buildingTag() }</code></p><h3><span id="changelog">changelog</span></h3><p>如果构建的SCM更新日志包含给定的正则表达式模式，则执行该阶段 例如: <code>when { changelog &#39;.*^\\[DEPENDENCY\\] .+$&#39; }</code></p><h3><span id="changeset">changeset</span></h3><p>如果构建的SCM变更集包含与给定字符串或glob匹配的一个或多个文件，则执行该阶段。 例如: <code>when { changeset &quot;**/*.js&quot; }</code></p><p>默认情况下，路径匹配不区分大小写，可以使用caseSensitive参数关闭, 例如: <code>when { changeset glob: &quot;ReadMe.*&quot;, caseSensitive: true }</code></p><h3><span id="changerequest">changeRequest</span></h3><p>如果当前构建用于“更改请求”（GitHub和Bitbucket上的<code>Pull Request</code>，GitLab上的<code>Merge</code>请求或Gerrit中的<code>change</code>等），则执行该阶段。 如果没有传递任何参数，则每个更改请求都会运行该阶段， 例如: <code>when { changeRequest() }</code>.</p><p>通过向变更请求添加带参数的过滤器属性，可以使阶段仅在匹配的变更请求上运行。 可能的属性是<code>id，target，branch，fork，url，title，author，authorDisplayName和authorEmail</code>。 其中每个对应一个<code>CHANGE_*</code>环境变量， 例如: <code>when { changeRequest target: &#39;master&#39; }</code>.</p><p>可以在属性之后添加可选参数比较器，以指定如何评估匹配的任何模式：<code>EQUALS</code>用于简单字符串比较（默认值），<code>GLOB</code>用于ANT样式路径glob（与例如变更集相同）或<code>REGEXP</code>用于常规 表达匹配。例如: <code>when { changeRequest authorEmail: &quot;[\\w_-.]+@example.com&quot;, comparator: &#39;REGEXP&#39; }</code></p><h3><span id="environment">environment</span></h3><p>当指定的环境变量设置为给定值时执行阶段， 例如: <code>when { environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; }</code></p><h3><span id="equals">equals</span></h3><p>当期望值等于实际值时执行阶段， 例如: <code>when { equals expected: 2, actual: currentBuild.number }</code></p><h3><span id="expression">expression</span></h3><p>当指定的Groovy表达式求值为<code>true</code>时执行阶段，例如: <code>when { expression { return params.DEBUG_BUILD } }</code> .</p><p>请注意，从表达式返回字符串时，必须将它们转换为布尔值或返回<code>null</code>表示为<code>false</code>。 简单地返回<code>0</code>或<code>false</code>仍将评估为<code>true</code>。</p><h3><span id="tag">tag</span></h3><p>如果<code>TAG_NAME</code>变量与给定模式匹配，则执行该阶段。例如: <code>when { tag &quot;release-*&quot; }</code>. </p><p>如果提供了空模式，则如果<code>TAG_NAME</code>变量存在，则将执行该阶段（与buildingTag（）相同）。</p><p>可以在属性之后添加可选参数比较器，以指定如何评估匹配的任何模式：<code>EQUALS</code>用于简单字符串比较（默认值），<code>GLOB</code>用于ANT样式路径glob（与例如变更集相同）或<code>REGEXP</code>用于常规 表达匹配。 例如: <code>when { tag pattern: &quot;release-\\d+&quot;, comparator: &quot;REGEXP&quot;}</code></p><h3><span id="not">not</span></h3><p>嵌套条件为false时执行阶段。 必须包含一个条件。 例如: <code>when { not { branch &#39;master&#39; } }</code></p><h3><span id="allof">allOf</span></h3><p>当所有嵌套条件都为真时执行阶段。 必须至少包含一个条件。例如: <code>when { allOf { branch &#39;master&#39;; environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; } }</code></p><h3><span id="anyof">anyOf</span></h3><p>至少有一个嵌套条件为真时执行阶段。 必须至少包含一个条件。例如: <code>when { anyOf { branch &#39;master&#39;; branch &#39;staging&#39; } }</code></p><p>在进入<code>stage&#39;s agent</code>之前评估<code>when</code><br>默认情况下，进入该阶段的<code>agent</code>后，对该阶段的<code>when</code>条件进行评估。 但是，可以通过在<code>when</code>块中指定<code>beforeAgent</code>选项来更改此设置。 如果<code>beforeAgent</code>设置为<code>true</code>，则首先评估<code>when</code>条件，并且仅当<code>when</code>条件的计算结果为<code>true</code>时才进入<code>agent</code>。</p><h2><span id="示例">示例</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line">Jenkinsfile (Declarative Pipeline)</span><br><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&apos;Example Build&apos;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Hello World&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&apos;Example Deploy&apos;) &#123;</span><br><span class="line">            when &#123;</span><br><span class="line">                branch &apos;production&apos;</span><br><span class="line">            &#125;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Deploying&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">Jenkinsfile (Declarative Pipeline)</span><br><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&apos;Example Build&apos;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Hello World&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&apos;Example Deploy&apos;) &#123;</span><br><span class="line">            when &#123;</span><br><span class="line">                branch &apos;production&apos;</span><br><span class="line">                environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos;</span><br><span class="line">            &#125;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Deploying&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">Jenkinsfile (Declarative Pipeline)</span><br><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&apos;Example Build&apos;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Hello World&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&apos;Example Deploy&apos;) &#123;</span><br><span class="line">            when &#123;</span><br><span class="line">                allOf &#123;</span><br><span class="line">                    branch &apos;production&apos;</span><br><span class="line">                    environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Deploying&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">Jenkinsfile (Declarative Pipeline)</span><br><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&apos;Example Build&apos;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Hello World&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&apos;Example Deploy&apos;) &#123;</span><br><span class="line">            when &#123;</span><br><span class="line">                branch &apos;production&apos;</span><br><span class="line">                anyOf &#123;</span><br><span class="line">                    environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos;</span><br><span class="line">                    environment name: &apos;DEPLOY_TO&apos;, value: &apos;staging&apos;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Deploying&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">Jenkinsfile (Declarative Pipeline)</span><br><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&apos;Example Build&apos;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Hello World&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&apos;Example Deploy&apos;) &#123;</span><br><span class="line">            when &#123;</span><br><span class="line">                expression &#123; BRANCH_NAME ==~ /(production|staging)/ &#125;</span><br><span class="line">                anyOf &#123;</span><br><span class="line">                    environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos;</span><br><span class="line">                    environment name: &apos;DEPLOY_TO&apos;, value: &apos;staging&apos;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Deploying&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">Jenkinsfile (Declarative Pipeline)</span><br><span class="line">pipeline &#123;</span><br><span class="line">    agent none</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&apos;Example Build&apos;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Hello World&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&apos;Example Deploy&apos;) &#123;</span><br><span class="line">            agent &#123;</span><br><span class="line">                label &quot;some-label&quot;</span><br><span class="line">            &#125;</span><br><span class="line">            when &#123;</span><br><span class="line">                beforeAgent true</span><br><span class="line">                branch &apos;production&apos;</span><br><span class="line">            &#125;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Deploying&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="在pipeline中使用tag">在<code>Pipeline</code>中使用<code>tag</code></span></h2><p>使用标签来驱动<code>Jenkins</code>管道中的行为。考虑以下设计<code>Jenkinsfile</code>，其中包含构建，测试和部署的三个基本阶段：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&apos;Build&apos;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                sh &apos;make package&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&apos;Test&apos;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                sh &apos;make check&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&apos;Deploy&apos;) &#123;</span><br><span class="line">            when &#123; tag &quot;release-*&quot; &#125;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Deploying only because this commit is tagged...&apos;</span><br><span class="line">                sh &apos;make deploy&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>特别值得注意的是 <code>when Deploy</code>阶段的<code>tag</code>条件是应用标准。这意味着只有在Git中与<code>release-*</code> Ant样式通配符匹配的标记触发<code>Pipeline</code>时，才会执行该阶段。</p><h2><span id="参考">参考</span></h2><ol><li><a href="https://jenkins.io/doc/book/pipeline/syntax/#when" target="_blank" rel="noopener">https://jenkins.io/doc/book/pipeline/syntax/#when</a></li><li><a href="https://jenkins.io/blog/2018/05/16/pipelines-with-git-tags/" target="_blank" rel="noopener">https://jenkins.io/blog/2018/05/16/pipelines-with-git-tags/</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;when&lt;/code&gt;指令允许&lt;code&gt;Pipeline&lt;/code&gt;根据给定条件确定是否应执行该阶段。&lt;/p&gt;
    
    </summary>
    
      <category term="jenkins" scheme="yunke.science/categories/jenkins/"/>
    
    
      <category term="jenkins" scheme="yunke.science/tags/jenkins/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes job monitor</title>
    <link href="yunke.science/2018/10/24/k8s-job-monitor/"/>
    <id>yunke.science/2018/10/24/k8s-job-monitor/</id>
    <published>2018-10-24T09:50:30.000Z</published>
    <updated>2018-10-24T09:54:27.867Z</updated>
    
    <content type="html"><![CDATA[<p>使用Kubernetes cron作业，可以定期执行（批处理）作业。 有了显示器仪表板，更容易看出哪些作业正在运行，以及它们的最新状态是“成功”还是“失败”。</p><p>前端源于令人敬畏的<code>Jenkins Build Monitor</code>插件。 该应用程序使用kubectl在容器内部<br>从Kubernetes检索数据。</p><a id="more"></a><p>@[TOC]</p><p><img src="https://raw.githubusercontent.com/pietervogelaar/kubernetes-job-monitor/master/docs/kubernetes-job-monitor.png" alt="Kubernetes job monitor"></p><h2><span id="installation">Installation</span></h2><h3><span id="kubeconfig-secret">Kubeconfig secret</span></h3><p>The Kubernetes job monitor uses kubectl to retrieve data from the Kubernetes cluster, which requires authentication.<br>The configuration for the admin user that is located at <code>/etc/kubernetes/admin.conf</code> on the Kubernetes master can be<br>used. It’s off course also possible to create a user that only has read access to jobs (of one namespace or<br>all namespaces).</p><p>Convert the user configuration to one base64 encoded line:</p><pre><code>cat /etc/kubernetes/admin.conf | base64 | tr -d &apos;\n&apos;</code></pre><p>Create the <code>secret.yaml</code> manifest:</p><pre><code>---apiVersion: v1kind: Secretmetadata:  name: kubeconfigtype: Opaquedata:  config: thebase64encodedlinehere</code></pre><p>Apply in the same namespace as the Kubernetes job monitor:</p><pre><code>kubectl apply -f secret.yaml</code></pre><p>This secret will be mounted inside the container so that kubectl can use it.</p><h3><span id="deployment">Deployment</span></h3><pre><code>kubectl apply -f https://raw.githubusercontent.com/pietervogelaar/kubernetes-job-monitor/master/.kubernetes/kubernetes-job-monitor.yaml</code></pre><p><strong>Note</strong>: You should review the manifest above, to configure the correct host and Kubernetes dashboard URL for<br>deep linking.</p><h2><span id="usage">Usage</span></h2><p>By default the Kubernetes job monitor shows the latest status of all jobs (created by cron jobs) it can find. A couple<br>of query parameters are available.</p><table><thead><tr><th>Parameter</th><th>Description</th></tr></thead><tbody><tr><td>title</td><td>The title of the monitor dashboard, which is displayed at the top and used as page title</td></tr><tr><td>namespace</td><td>Show only jobs in this namespace</td></tr><tr><td>selector</td><td>Show only jobs that match this selector (e.g. key1=value1,key2=value2)</td></tr></tbody></table><h3><span id="examples">Examples</span></h3><ul><li><code>kubernetes-job-monitor.local/namespace=namespace-a</code></li><li><code>kubernetes-job-monitor.local/title=My Job Monitor&amp;namespace=namespace-a&amp;selector=group=one</code></li><li><code>kubernetes-job-monitor.local/title=My Job Monitor&amp;selector=group=two</code></li></ul><h3><span id="demo">Demo</span></h3><pre><code>kubectl create namespace namespace-akubectl create namespace namespace-bkubectl apply -f https://raw.githubusercontent.com/pietervogelaar/kubernetes-job-monitor/master/.kubernetes/test-cron-jobs.yaml</code></pre><h2><span id="references">References</span></h2><ol><li><a href="https://hub.docker.com/r/pietervogelaar/kubernetes-job-monitor/" target="_blank" rel="noopener">https://hub.docker.com/r/pietervogelaar/kubernetes-job-monitor/</a></li><li><a href="https://github.com/pietervogelaar/kubernetes-job-monitor" target="_blank" rel="noopener">https://github.com/pietervogelaar/kubernetes-job-monitor</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用Kubernetes cron作业，可以定期执行（批处理）作业。 有了显示器仪表板，更容易看出哪些作业正在运行，以及它们的最新状态是“成功”还是“失败”。&lt;/p&gt;
&lt;p&gt;前端源于令人敬畏的&lt;code&gt;Jenkins Build Monitor&lt;/code&gt;插件。 该应用程序使用kubectl在容器内部&lt;br&gt;从Kubernetes检索数据。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>k8s通过文件将Pod信息暴露给容器的方式</title>
    <link href="yunke.science/2018/10/22/expose-pod-info/"/>
    <id>yunke.science/2018/10/22/expose-pod-info/</id>
    <published>2018-10-22T09:16:53.000Z</published>
    <updated>2018-10-22T09:19:50.697Z</updated>
    
    <content type="html"><![CDATA[<p><code>Pod</code>使用<code>DownwardAPIVolumeFile</code>将有关其自身的信息公开给Pod中运行的Container。<code>DownwardAPIVolumeFile</code>可以公开<code>Pod</code>字段和<code>Container</code>字段。</p><a id="more"></a><p>@[TOC]</p><p>有两种方法可以将Pod和Container字段公开给正在运行的Container：</p><ol><li>环境变量</li><li>DownwardAPIVolumeFiles</li></ol><h2><span id="环境变量的方式">环境变量的方式</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: dapi-envars-fieldref</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: test-container</span><br><span class="line">      image: k8s.gcr.io/busybox</span><br><span class="line">      command: [ &quot;sh&quot;, &quot;-c&quot;]</span><br><span class="line">      args:</span><br><span class="line">      - while true; do</span><br><span class="line">          echo -en &apos;\n&apos;;</span><br><span class="line">          printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE;</span><br><span class="line">          printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT;</span><br><span class="line">          sleep 10;</span><br><span class="line">        done;</span><br><span class="line">      env:</span><br><span class="line">        - name: MY_NODE_NAME</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: spec.nodeName</span><br><span class="line">        - name: MY_POD_NAME</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.name</span><br><span class="line">        - name: MY_POD_NAMESPACE</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.namespace</span><br><span class="line">        - name: MY_POD_IP</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: status.podIP</span><br><span class="line">        - name: MY_POD_SERVICE_ACCOUNT</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: spec.serviceAccountName</span><br><span class="line">  restartPolicy: Never</span><br></pre></td></tr></table></figure><h2><span id="使用downwardapivolumefiles">使用<code>DownwardAPIVolumeFiles</code></span></h2><h3><span id="存储pod字段">存储Pod字段</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-downwardapi-volume-example</span><br><span class="line">  labels:</span><br><span class="line">    zone: us-est-coast</span><br><span class="line">    cluster: test-cluster1</span><br><span class="line">    rack: rack-22</span><br><span class="line">  annotations:</span><br><span class="line">    build: two</span><br><span class="line">    builder: john-doe</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: client-container</span><br><span class="line">      image: k8s.gcr.io/busybox</span><br><span class="line">      command: [&quot;sh&quot;, &quot;-c&quot;]</span><br><span class="line">      args:</span><br><span class="line">      - while true; do</span><br><span class="line">          if [[ -e /etc/podinfo/labels ]]; then</span><br><span class="line">            echo -en &apos;\n\n&apos;; cat /etc/podinfo/labels; fi;</span><br><span class="line">          if [[ -e /etc/podinfo/annotations ]]; then</span><br><span class="line">            echo -en &apos;\n\n&apos;; cat /etc/podinfo/annotations; fi;</span><br><span class="line">          sleep 5;</span><br><span class="line">        done;</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: podinfo</span><br><span class="line">          mountPath: /etc/podinfo</span><br><span class="line">          readOnly: false</span><br><span class="line">  volumes:</span><br><span class="line">    - name: podinfo</span><br><span class="line">      downwardAPI:</span><br><span class="line">        items:</span><br><span class="line">          - path: &quot;labels&quot;</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.labels</span><br><span class="line">          - path: &quot;annotations&quot;</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.annotations</span><br></pre></td></tr></table></figure><p><code>fieldRef</code>支持的字段：</p><ul><li>spec.nodeName - 节点的名称</li><li>status.hostIP - 节点的IP</li><li>metadata.name - pod的名字</li><li>metadata.namespace - pod的命名空间</li><li>status.podIP - pod的IP地址</li><li>spec.serviceAccountName - pod的服务帐户名称</li><li>metadata.uid - pod的UID</li><li>metadata.labels[‘<key>‘] - pod标签的值<key>（例如metadata.labels[‘mylabel’]）; 可在Kubernetes 1.9+中找到</key></key></li><li><p>metadata.annotations[‘<key>‘] - pod的注释的值<key>（例如metadata.annotations[‘myannotation’]）; 可在Kubernetes 1.9+中找到</key></key></p></li><li><p>metadata.labels - 所有pod的标签，格式为label-key=”escaped-label-value”每行一个label</p></li><li>metadata.annotations - 所有pod的注释，格式为annotation-key=”escaped-annotation-value”每行一个annotation</li></ul><h3><span id="存储容器字段">存储容器字段</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-downwardapi-volume-example-2</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: client-container</span><br><span class="line">      image: k8s.gcr.io/busybox:1.24</span><br><span class="line">      command: [&quot;sh&quot;, &quot;-c&quot;]</span><br><span class="line">      args:</span><br><span class="line">      - while true; do</span><br><span class="line">          echo -en &apos;\n&apos;;</span><br><span class="line">          if [[ -e /etc/podinfo/cpu_limit ]]; then</span><br><span class="line">            echo -en &apos;\n&apos;; cat /etc/podinfo/cpu_limit; fi;</span><br><span class="line">          if [[ -e /etc/podinfo/cpu_request ]]; then</span><br><span class="line">            echo -en &apos;\n&apos;; cat /etc/podinfo/cpu_request; fi;</span><br><span class="line">          if [[ -e /etc/podinfo/mem_limit ]]; then</span><br><span class="line">            echo -en &apos;\n&apos;; cat /etc/podinfo/mem_limit; fi;</span><br><span class="line">          if [[ -e /etc/podinfo/mem_request ]]; then</span><br><span class="line">            echo -en &apos;\n&apos;; cat /etc/podinfo/mem_request; fi;</span><br><span class="line">          sleep 5;</span><br><span class="line">        done;</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          memory: &quot;32Mi&quot;</span><br><span class="line">          cpu: &quot;125m&quot;</span><br><span class="line">        limits:</span><br><span class="line">          memory: &quot;64Mi&quot;</span><br><span class="line">          cpu: &quot;250m&quot;</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: podinfo</span><br><span class="line">          mountPath: /etc/podinfo</span><br><span class="line">          readOnly: false</span><br><span class="line">  volumes:</span><br><span class="line">    - name: podinfo</span><br><span class="line">      downwardAPI:</span><br><span class="line">        items:</span><br><span class="line">          - path: &quot;cpu_limit&quot;</span><br><span class="line">            resourceFieldRef:</span><br><span class="line">              containerName: client-container</span><br><span class="line">              resource: limits.cpu</span><br><span class="line">              divisor: 1m</span><br><span class="line">          - path: &quot;cpu_request&quot;</span><br><span class="line">            resourceFieldRef:</span><br><span class="line">              containerName: client-container</span><br><span class="line">              resource: requests.cpu</span><br><span class="line">              divisor: 1m</span><br><span class="line">          - path: &quot;mem_limit&quot;</span><br><span class="line">            resourceFieldRef:</span><br><span class="line">              containerName: client-container</span><br><span class="line">              resource: limits.memory</span><br><span class="line">              divisor: 1Mi</span><br><span class="line">          - path: &quot;mem_request&quot;</span><br><span class="line">            resourceFieldRef:</span><br><span class="line">              containerName: client-container</span><br><span class="line">              resource: requests.memory</span><br><span class="line">              divisor: 1Mi</span><br></pre></td></tr></table></figure><p><code>resourceFieldRef</code> 支持的字段：</p><ul><li>Container的CPU限制：limits.cpu</li><li>Container的CPU请求：requests.cpu</li><li>Container的内存限制：limits.memory</li><li>Container的内存请求：requests.memory</li></ul><h3><span id="projected-使用-downwardapi"><code>projected</code> 使用 <code>downwardAPI</code></span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: volume-test</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: container-test</span><br><span class="line">    image: busybox</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: all-in-one</span><br><span class="line">      mountPath: &quot;/projected-volume&quot;</span><br><span class="line">      readOnly: true</span><br><span class="line">  volumes:</span><br><span class="line">  - name: all-in-one</span><br><span class="line">    projected:</span><br><span class="line">      sources:</span><br><span class="line">      - secret:</span><br><span class="line">          name: mysecret</span><br><span class="line">          items:</span><br><span class="line">            - key: username</span><br><span class="line">              path: my-group/my-username</span><br><span class="line">      - downwardAPI:</span><br><span class="line">          items:</span><br><span class="line">            - path: &quot;labels&quot;</span><br><span class="line">              fieldRef:</span><br><span class="line">                fieldPath: metadata.labels</span><br><span class="line">            - path: &quot;cpu_limit&quot;</span><br><span class="line">              resourceFieldRef:</span><br><span class="line">                containerName: container-test</span><br><span class="line">                resource: limits.cpu</span><br><span class="line">      - configMap:</span><br><span class="line">          name: myconfigmap</span><br><span class="line">          items:</span><br><span class="line">            - key: config</span><br><span class="line">              path: my-group/my-config</span><br></pre></td></tr></table></figure><p>.downwardAPI.fieldRef.fieldPath` 支持的类型：</p><ul><li>“metadata.annotations”</li><li>“metadata.labels”</li><li>“metadata.name”</li><li>“metadata.namespace”</li><li>“metadata.uid”</li></ul><h2><span id="参考">参考</span></h2><ol><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/</a></li><li><a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/</a></li><li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#projected" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/storage/volumes/#projected</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;Pod&lt;/code&gt;使用&lt;code&gt;DownwardAPIVolumeFile&lt;/code&gt;将有关其自身的信息公开给Pod中运行的Container。&lt;code&gt;DownwardAPIVolumeFile&lt;/code&gt;可以公开&lt;code&gt;Pod&lt;/code&gt;字段和&lt;code&gt;Container&lt;/code&gt;字段。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>weave-net</title>
    <link href="yunke.science/2018/09/29/weave-net/"/>
    <id>yunke.science/2018/09/29/weave-net/</id>
    <published>2018-09-29T01:24:53.000Z</published>
    <updated>2018-09-29T01:26:54.689Z</updated>
    
    <content type="html"><![CDATA[<p>Weave Net创建了一个虚拟网络，可以跨多个主机连接Docker容器并启用它们的自动发现。使用Weave Net，由多个容器组成的基于便携式微服务的应用程序可以在任何地方运行：在一个主机，多个主机上，甚至跨云提供商和数据中心。应用程序使用网络就像容器全部插入同一网络交换机一样，无需配置端口映射，中继或链接。</p><a id="more"></a><p>@[TOC]</p><blockquote><p>Weave Net上的应用程序容器提供的服务可以暴露给外部世界，无论它们在何处运行。类似地，可以打开现有的内部系统以接受来自应用程序容器的连接，而不管它们的位置如何。</p></blockquote><h2><span id="安装">安装</span></h2><p>在安装Weave Net之前，应确保防火墙未阻止以下端口：TCP 6783和UDP 6783/6784。有关详细信息，请参见<a href="https://www.weave.works/docs/net/latest/faq#ports" target="_blank" rel="noopener">常见问题解答</a>。</p><p>可以使用单个命令将Weave Net安装到启用CNI的Kubernetes集群上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &apos;\n&apos;)&quot;</span><br></pre></td></tr></table></figure><p>几秒钟后，Weave Net pod应该在每个节点上运行，您创建的任何其他pod将自动连接到Weave网络。</p><blockquote><p>注意：此命令需要Kubernetes 1.4或更高版本，我们建议您的主节点至少具有两个CPU核心。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：如果使用Weave Net之前完全安装Weave Net 的Weave CNI插件，则必须先将其卸载，然后再应用Weave-kube插件。关闭Kubernetes，并在所有节点上执行以下操作：</span><br><span class="line">weave reset</span><br><span class="line">删除您在启动时运行Weave所做的任何单独规定，例如systemd单位</span><br><span class="line">rm /opt/cni/bin/weave-*</span><br><span class="line">然后重新启动Kubernetes并按上述方法安装插件。</span><br></pre></td></tr></table></figure><h2><span id="升级daemonset">升级DaemonSet</span></h2><p>DaemonSet 定义指定滚动更新，因此当您应用新版本时，Kubernetes将自动逐个重新启动Weave Net窗格。</p><h2><span id="cpu和内存要求">CPU和内存要求</span></h2><p>Kubernetes管理 每个节点上的资源，并且仅调度pod以在具有足够可用资源的节点上运行。</p><p>典型的Kubernetes安装的组件（主节点运行etcd，调度程序，api-server等）占用了大约95％的CPU，几乎没有空间运行其他任何东西。要使Weave Net的所有功能都能正常工作，它必须在每个节点上运行，包括主节点。</p><p>解决此问题的最佳方法是使用具有至少两个CPU核心的计算机。但是，如果您是第一次安装Kubernetes和Weave Net，您可能不会意识到这一要求。出于这个原因，Weave Net作为DaemonSet启动，其规范为每个容器保留至少1％的CPU。这使Weave Net能够在单CPU节点上无缝启动。</p><p>根据工作负载，Weave Net可能需要超过1％的CPU。DaemonSet中设置的百分比是最小值而不是限制。这个最小设置允许Weave Net利用可用的CPU，并在需要时“突发”超过该限制。</p><h2><span id="pod-eviction">Pod Eviction</span></h2><p>如果节点耗尽CPU，内存或磁盘，Kubernetes可能会决定驱逐 一个或多个pod。它可能会选择驱逐Weave Net pod，这会破坏pod网络的运营。</p><p>您可以通过更改DaemonSet以获得更大的请求以及相同值的限制来减少驱逐的机会。</p><p>这导致Kubernetes应用“保证”而不是“可爆”政策。但是，无法对磁盘空间进行类似的请求，因此请注意此问题并监视您的资源以确保它们保持在100％以下。</p><h2><span id="pod网络">Pod网络</span></h2><p>Weave Net提供了一个网络，可以将所有pod连接在一起，实现Kubernetes模型。</p><p>Kubernetes使用容器网络接口 （CNI）将pod连接到Weave Net。</p><p>Kubernetes在pod网络之上实现了许多网络功能。这包括 服务， 通过DNS的服务发现 和进入群集的Ingress。使用Kubernetes插件时，WeaveDNS被禁用。</p><h2><span id="网络策略">网络策略</span></h2><p>Kubernetes网络策略允许您根据命名空间和标签安全地隔离pod。有关在Kubernetes中配置网络策略的详细信息，请参阅 <a href="https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/" target="_blank" rel="noopener">演练</a> 和<a href="https://v1-7.docs.kubernetes.io/docs/api-reference/v1.7/#networkpolicy-v1-networking" target="_blank" rel="noopener">NetworkPolicy API对象定义</a></p><p>注意：从Weave Net 1.9版开始，网络策略控制器允许所有多播流量。由于多个pod可能使用单个多播地址，因此我们无法实现规则来单独隔离它们。您可以通过在YAML配置中添加–allow-mcast=false参数来关闭此行为（阻止所有多播流量）weave-npc。</p><h2><span id="阅读日志">阅读日志</span></h2><p>从列表输出中选择一个pod kubectl get pods并询问如下日志：</p><p><code>$ kubectl logs -n kube-system weave-net-1jkl6 weave</code></p><h2><span id="需要注意的事项">需要注意的事项</span></h2><ol><li>不要打开<code>--masquerade-allkube-proxy</code>：这将改变每个<code>pod-to-pod</code>对话的源地址，这将使得无法正确实施限制哪些pod可以通话的网络策略。</li><li>如果你<code>--cluster-cidr</code>在<code>kube-proxy</code>上设置了选项，请确保它与<code>IPALLOC_RANGE</code>给定的<code>Weave Net</code> 相匹配（见下文）。</li><li>必须在每个节点上启用IP转发，以便pod访问另一个网络上的Kubernetes服务或其他IP地址。检查一下<code>sysctl net.ipv4.ip_forward</code>; 结果应该是1。（请注意，启用IP转发可能会产生安全隐患）。</li><li>Weave Net可以在minikube v0.28或更高版本上运行，默认的CNI配置随minikube一起被禁用。有关 详细信息，请参阅＃3124。</li></ol><h2><span id="更改配置选项">更改配置选项</span></h2><p>通过在URL末尾追加此命令 <code>&amp;password-secret=weave-passwd</code>- 生成一个YAML文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSLo weave-daemonset.yaml &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &apos;\n&apos;)&amp;password-secret=weave-passwd&quot;</span><br></pre></td></tr></table></figure></p><ul><li>version：Weave Net的版本。默认值：latest，即最新版本。</li><li>password-secret：包含密码的Kubernetes秘密的名称。 注意：Kubernetes密码必须与包含密码的文件名相对应</li><li>known-peers：以逗号分隔的主机列表。默认值：空。</li><li>trusted-subnets：逗号分隔的CIDR列表。默认值：空。</li><li>disable-npc：boolean（true|false）。默认值：false。</li><li>env.NAME=VALUE：添加环境变量NAME并将其设置为VALUE。</li><li>seLinuxOptions.NAME=VALUE：添加SELinux选项NAME并将其设置为VALUE，例如seLinuxOptions.type=spc_t</li></ul><p>您可以设置的变量列表是：</p><ul><li>CHECKPOINT_DISABLE - 如果设置为1，则禁用检查新的Weave Net版本（默认为空，即启用检查）</li><li>CONN_LIMIT - 对等体之间连接数的软限制。默认为30。</li><li>HAIRPIN_MODE- Weave Net默认为在veth附加的容器的桥侧启用发夹。如果你需要禁用发夹，例如你的内核是启用了发夹的恐慌之一，那么你可以通过设- 置禁用它HAIRPIN_MODE=false。</li><li>IPALLOC_RANGE- Weave Net使用的IP地址范围及其所在的子网（CIDR格式;默认10.32.0.0/12）</li><li>EXPECT_NPC - 设置为0以禁用网络策略控制器（默认打开）</li><li>KUBE_PEERS - Kubernetes集群中对等体的地址列表（默认是从api-server获取列表）</li><li>IPALLOC_INIT- 设置IP地址管理器的初始化模式 （默认为共识KUBE_PEERS）</li><li>WEAVE_EXPOSE_IP - 将用作网关的IP地址从Weave网络设置为主机网络 - 如果您将插件配置为静态pod，这将非常有用。</li><li>WEAVE_METRICS_ADDR - Weave Net守护程序将提供Prometheus样式度量标准的地址和端口（默认为0.0.0.0:6782）</li><li>WEAVE_STATUS_ADDR - Weave Net守护程序将为状态请求提供服务的地址和端口（默认为禁用）</li><li>WEAVE_MTU- Weave Net默认为1376字节，但如果您的基础网络有更严格的限制，您可以设置更小的大小，或者如果您的网络支持巨型帧，您可以设置更- 大的大小以获得更好的性能 - 有关详细信息，请参阅此处。</li><li>NO_MASQ_LOCAL- 设置为1以在访问带注释的服务时保留客户端源IP地址service.spec.externalTrafficPolicy=Local。该功能仅适用于Weave IPAM（默认）。</li></ul><p>指定IPALLOC_RANGE 示例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSLo weave-daemonset.yaml &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &apos;\n&apos;)&amp;env.IPALLOC_RANGE=10.42.0.0/16&quot;</span><br></pre></td></tr></table></figure><p>yaml其中包含：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: weave</span><br><span class="line">      command:</span><br><span class="line">        - /home/weave/launch.sh</span><br><span class="line">      env:</span><br><span class="line">        - name: IPALLOC_RANGE</span><br><span class="line">          value: 10.42.0.0/16</span><br></pre></td></tr></table></figure></p><h2><span id="手动编辑yaml文件">手动编辑YAML文件</span></h2><p>无论您是cloud.weave.works从我们的发布页面保存了YAML文件还是从我们的发布页面下载了静态YAML文件，您都可以手动编辑它以满足您的需求。</p><p>例如，</p><p>通过将它们添加到command:YAML文件中的数组，可以向Weave路由器进程提供其他参数，<br>可以通过上面列出的环境变量设置其他参数; 这些可以像这样插入到YAML文件中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">        - name: weave</span><br><span class="line">          env:</span><br><span class="line">            - name: IPALLOC_RANGE</span><br><span class="line">              value: 10.0.0.0/16</span><br></pre></td></tr></table></figure><h2><span id="参考">参考</span></h2><ol><li>From <a href="https://www.weave.works/docs/net/latest/kubernetes/kube-addon/" target="_blank" rel="noopener">Integrating Kubernetes via the Addon</a></li><li>kubeadm <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network" target="_blank" rel="noopener">Installing a pod network add-on</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Weave Net创建了一个虚拟网络，可以跨多个主机连接Docker容器并启用它们的自动发现。使用Weave Net，由多个容器组成的基于便携式微服务的应用程序可以在任何地方运行：在一个主机，多个主机上，甚至跨云提供商和数据中心。应用程序使用网络就像容器全部插入同一网络交换机一样，无需配置端口映射，中继或链接。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 1.11.3 使用kubeadm创建高可用的ETCD群集</title>
    <link href="yunke.science/2018/09/27/k8s-1-11-3-etcd/"/>
    <id>yunke.science/2018/09/27/k8s-1-11-3-etcd/</id>
    <published>2018-09-27T05:54:51.000Z</published>
    <updated>2018-09-27T05:57:45.166Z</updated>
    
    <content type="html"><![CDATA[<p>Kubeadm默认在由master节点上的kubelet管理的静态pod中运行单个成员etcd集群。这不是高可用性设置，因为etcd集群只包含一个成员，并且无法维持任何成员变得不可用。</p><a id="more"></a><p>@[TOC]</p><blockquote><p>本节将介绍创建由三个成员组成的高可用性etcd集群的过程，该集群可在使用kubeadm设置kubernetes集群时用作外部etcd。</p></blockquote><blockquote><p><a href="https://kubernetes.io/docs/setup/independent/high-availability/" target="_blank" rel="noopener">Creating Highly Available Clusters with kubeadm</a></p></blockquote><p>使用kubeadm设置高可用性Kubernetes集群主要是创建高可用的ETCD群集，有两种不同方法：</p><ol><li>etcd 与 master节点一起部署。只需较少的主机，etcd与master部署在同一位置。</li><li>etcd 单独部署，使用外部独立的etcd集群。需要较多的主机，master与etcd分开部署。</li></ol><h2><span id="etcd-与master-一起部署">etcd 与master 一起部署</span></h2><h3><span id="第一台master节点主机上执行配置">第一台master节点主机上执行配置</span></h3><ol><li>创建 <code>kubeadm-config.yaml</code> 模版文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.x</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https://127.0.0.1:2379,https://CP0_IP:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https://CP0_IP:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https://CP0_IP:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https://CP0_IP:2380&quot;</span><br><span class="line">      initial-cluster: &quot;CP0_HOSTNAME=https://CP0_IP:2380&quot;</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - CP0_HOSTNAME</span><br><span class="line">      - CP0_IP</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - CP0_HOSTNAME</span><br><span class="line">      - CP0_IP</span><br></pre></td></tr></table></figure><blockquote><p>以上模版文件仅显示配置了etcd配置，其他配置没有显示</p></blockquote><ol start="2"><li>替换 <code>kubernetesVersion: v1.11.x</code> 中的<code>x</code>为<a href="https://storage.googleapis.com/kubernetes-release/release/stable-1.11.txt" target="_blank" rel="noopener">最近可用的版本</a>，如<code>kubernetesVersion: v1.11.3</code></li><li><p>替换模版中的变量</p><ul><li><code>CP0_HOSTNAME</code></li><li><code>CP0_IP</code></li></ul></li><li><p>运行初始化</p><blockquote><p>kubeadm 过程中可能会连接外网<code>gcr.io</code>查询当前稳定版本，由于无法连接<code>gcr.io</code>导致执行失败。<br>添加代理<code>export http_proxy=http://IP:PORT</code> ,执行完以后取消代理 <code>unset http_proxy</code></p></blockquote></li></ol><p><code>kubeadm init --config kubeadm-config.yaml</code></p><h3><span id="复制第一台生成的证书及配置文件到其他master主机相同位置">复制第一台生成的证书及配置文件到其他master主机相同位置</span></h3><ul><li>/etc/kubernetes/pki/ca.crt</li><li>/etc/kubernetes/pki/ca.key</li><li>/etc/kubernetes/pki/sa.key</li><li>/etc/kubernetes/pki/sa.pub</li><li>/etc/kubernetes/pki/front-proxy-ca.crt</li><li>/etc/kubernetes/pki/front-proxy-ca.key</li><li>/etc/kubernetes/pki/etcd/ca.crt</li><li>/etc/kubernetes/pki/etcd/ca.key</li><li>/etc/kubernetes/admin.conf</li></ul><h3><span id="第二台master节点主机上执行配置">第二台master节点主机上执行配置</span></h3><ol><li>创建 <code>kubeadm-config.yaml</code> 模版文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.x</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https://127.0.0.1:2379,https://CP1_IP:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https://CP1_IP:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https://CP1_IP:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https://CP1_IP:2380&quot;</span><br><span class="line">      initial-cluster: &quot;CP0_HOSTNAME=https://CP0_IP:2380,CP1_HOSTNAME=https://CP1_IP:2380&quot;</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - CP1_HOSTNAME</span><br><span class="line">      - CP1_IP</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - CP1_HOSTNAME</span><br><span class="line">      - CP1_IP</span><br></pre></td></tr></table></figure><blockquote><p>以上模版文件仅显示配置了etcd配置，其他配置没有显示</p></blockquote><ol start="2"><li>替换 <code>kubernetesVersion: v1.11.x</code> 中的<code>x</code>为<a href="https://storage.googleapis.com/kubernetes-release/release/stable-1.11.txt" target="_blank" rel="noopener">最近可用的版本</a>，如<code>kubernetesVersion: v1.11.3</code></li><li><p>替换模版中的变量</p><ul><li><code>CP0_HOSTNAME</code></li><li><code>CP0_IP</code></li><li><code>CP1_HOSTNAME</code></li><li><code>CP1_IP</code></li></ul></li><li><p>确认复制第一台主机目录 <code>/etc/kubernetes/pki/</code> 及文件 <code>/etc/kubernetes/admin.conf</code> 到本主机相同位置</p></li><li><p>运行<code>kubeadm phase</code> 命令引导kubelet:</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase certs all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml</span><br><span class="line">systemctl start kubelet</span><br></pre></td></tr></table></figure><ol start="6"><li>运行命令，添加节点到<code>etcd</code>集群<blockquote><p>按实际修改以下IP 和 HOSTNAME</p></blockquote></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">export CP0_IP=10.0.0.7</span><br><span class="line">export CP0_HOSTNAME=cp0</span><br><span class="line">export CP1_IP=10.0.0.8</span><br><span class="line">export CP1_HOSTNAME=cp1</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/admin.conf </span><br><span class="line">kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP1_HOSTNAME&#125; https://$&#123;CP1_IP&#125;:2380</span><br><span class="line">kubeadm alpha phase etcd local --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure><blockquote><p>在将节点添加到正在运行的集群之后，以及在将新节点加入etcd集群之前，此命令会导致etcd集群在短时间内不可用</p></blockquote><ol start="7"><li>部署其他控制组件并将节点标记为master主节点：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure><h3><span id="第三台master节点主机上执行配置">第三台master节点主机上执行配置</span></h3><ol><li>创建 <code>kubeadm-config.yaml</code> 模版文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.x</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https://127.0.0.1:2379,https://CP2_IP:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https://CP2_IP:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https://CP2_IP:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https://CP2_IP:2380&quot;</span><br><span class="line">      initial-cluster: &quot;CP0_HOSTNAME=https://CP0_IP:2380,CP1_HOSTNAME=https://CP1_IP:2380,CP2_HOSTNAME=https://CP2_IP:2380&quot;</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - CP2_HOSTNAME</span><br><span class="line">      - CP2_IP</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - CP2_HOSTNAME</span><br><span class="line">      - CP2_IP</span><br></pre></td></tr></table></figure><blockquote><p>以上模版文件仅显示配置了etcd配置，其他配置没有显示</p></blockquote><ol start="2"><li>替换 <code>kubernetesVersion: v1.11.x</code> 中的<code>x</code>为<a href="https://storage.googleapis.com/kubernetes-release/release/stable-1.11.txt" target="_blank" rel="noopener">最近可用的版本</a>，如<code>kubernetesVersion: v1.11.3</code></li><li>替换模版中的变量</li></ol><ul><li><code>CP0_HOSTNAME</code></li><li><code>CP0_IP</code></li><li><code>CP1_HOSTNAME</code></li><li><code>CP1_IP</code></li><li><code>CP2_HOSTNAME</code></li><li><code>CP2_IP</code></li></ul><ol start="4"><li><p>确认复制第一台master主机目录 <code>/etc/kubernetes/pki/</code> 及文件 <code>/etc/kubernetes/admin.conf</code> 到本主机相同位置</p></li><li><p>运行<code>kubeadm phase</code> 命令引导kubelet:</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase certs all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml</span><br><span class="line">systemctl start kubelet</span><br></pre></td></tr></table></figure><ol start="6"><li>运行命令，添加节点到<code>etcd</code>集群<blockquote><p>按实际修改以下IP 和 HOSTNAME</p></blockquote></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">export CP0_IP=10.0.0.7</span><br><span class="line">export CP0_HOSTNAME=cp0</span><br><span class="line">export CP2_IP=10.0.0.8</span><br><span class="line">export CP2_HOSTNAME=cp1</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/admin.conf </span><br><span class="line">kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP2_HOSTNAME&#125; https://$&#123;CP2_IP&#125;:2380</span><br><span class="line">kubeadm alpha phase etcd local --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure><blockquote><p>在将节点添加到正在运行的集群之后，以及在将新节点加入etcd集群之前，此命令会导致etcd集群在短时间内不可用</p></blockquote><ol start="7"><li>部署其他控制组件并将节点标记为master主节点：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure><p>整个集群部署完成。</p><h2><span id="使用-kubeadm-部署单独的-etcd-集群并使用kubelet管理">使用 kubeadm 部署单独的 etcd 集群，并使用kubelet管理</span></h2><blockquote><p><a href="https://kubernetes.io/docs/setup/independent/setup-ha-etcd-with-kubeadm/" target="_blank" rel="noopener">Set up a high availability etcd cluster with kubeadm</a></p></blockquote><p>准备工作：</p><ol start="0"><li>准备三个单独的主机，不能部署kubernetes ，仅部署 <code>etcd</code></li><li>三个主机可以通过端口2379和2380相互通信。本文档假定这些默认端口。但是，它们可以通过kubeadm配置文件进行配置。</li><li>每个主机必须安装<code>docker</code>，<code>kubelet</code>和<code>kubeadm</code>。</li><li>一些在主机之间复制文件的基础结构 例如ssh，scp 可以满足这个要求</li></ol><h3><span id="设置群集">设置群集</span></h3><p>一般方法是在一个节点上生成所有证书，并仅将必要的文件分发给其他节点。</p><h3><span id="1-将kubelet配置为etcd的服务管理器">1. 将kubelet配置为etcd的服务管理器。</span></h3><p>(每台主机上执行)<br>运行etcd比运行kubernetes更简单，因此您必须通过创建一个具有更高优先级的新文件来覆盖kubeadm提供的kubelet单元文件。</p><p><code>cgroup-driver=systemd</code> 要与 <code>docker info中的Cgroup Driver</code> 相匹配。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=&quot;KUBELET_CGROUP_ARGS=--cgroup-driver=systemd&quot;</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet $KUBELET_CGROUP_ARGS --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true</span><br><span class="line">Restart=always</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure><h3><span id="2-为kubeadm创建配置文件">2. 为kubeadm创建配置文件。</span></h3><p>(仅在HOST0主机上执行)<br>使用以下脚本为每个将在其上运行etcd成员的主机生成一个kubeadm配置文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># Update HOST0, HOST1, and HOST2 with the IPs or resolvable names of your hosts</span><br><span class="line">export HOST0=10.0.0.6</span><br><span class="line">export HOST1=10.0.0.7</span><br><span class="line">export HOST2=10.0.0.8</span><br><span class="line"></span><br><span class="line"># Create temp directories to store files that will end up on other hosts.</span><br><span class="line">mkdir -p /tmp/$&#123;HOST0&#125;/ /tmp/$&#123;HOST1&#125;/ /tmp/$&#123;HOST2&#125;/</span><br><span class="line"></span><br><span class="line">ETCDHOSTS=($&#123;HOST0&#125; $&#123;HOST1&#125; $&#123;HOST2&#125;)</span><br><span class="line">NAMES=(&quot;infra0&quot; &quot;infra1&quot; &quot;infra2&quot;)</span><br><span class="line"></span><br><span class="line">for i in &quot;$&#123;!ETCDHOSTS[@]&#125;&quot;; do</span><br><span class="line">HOST=$&#123;ETCDHOSTS[$i]&#125;</span><br><span class="line">NAME=$&#123;NAMES[$i]&#125;</span><br><span class="line">cat &lt;&lt; EOF &gt; /tmp/$&#123;HOST&#125;/kubeadmcfg.yaml</span><br><span class="line">apiVersion: &quot;kubeadm.k8s.io/v1alpha2&quot;</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">etcd:</span><br><span class="line">    local:</span><br><span class="line">        serverCertSANs:</span><br><span class="line">        - &quot;$&#123;HOST&#125;&quot;</span><br><span class="line">        peerCertSANs:</span><br><span class="line">        - &quot;$&#123;HOST&#125;&quot;</span><br><span class="line">        extraArgs:</span><br><span class="line">            initial-cluster: infra0=https://$&#123;ETCDHOSTS[0]&#125;:2380,infra1=https://$&#123;ETCDHOSTS[1]&#125;:2380,infra2=https://$&#123;ETCDHOSTS[2]&#125;:2380</span><br><span class="line">            initial-cluster-state: new</span><br><span class="line">            name: $&#123;NAME&#125;</span><br><span class="line">            listen-peer-urls: https://$&#123;HOST&#125;:2380</span><br><span class="line">            listen-client-urls: https://$&#123;HOST&#125;:2379</span><br><span class="line">            advertise-client-urls: https://$&#123;HOST&#125;:2379</span><br><span class="line">            initial-advertise-peer-urls: https://$&#123;HOST&#125;:2380</span><br><span class="line">EOF</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3><span id="3-生成证书颁发机构">3. 生成证书颁发机构</span></h3><p>如果您已有CA，那么唯一的操作是将CA crt和 key文件复制到<code>/etc/kubernetes/pki/etcd/ca.crt</code>和 <code>/etc/kubernetes/pki/etcd/ca.key</code>。复制这些文件后，继续执行下一步“为每个成员创建证书”。</p><p>如果您还没有CA，则在<code>$HOST0</code>（生成kubeadm的配置文件的位置）上运行此命令。</p><p><code>kubeadm alpha phase certs etcd-ca</code></p><p>这会创建两个文件</p><ul><li><code>/etc/kubernetes/pki/etcd/ca.crt</code></li><li><code>/etc/kubernetes/pki/etcd/ca.key</code></li></ul><h3><span id="4-为每个成员创建证书">4. 为每个成员创建证书</span></h3><p>(仅在HOST0主机上执行)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase certs etcd-server --config=/tmp/$&#123;HOST2&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-peer --config=/tmp/$&#123;HOST2&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/$&#123;HOST2&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/$&#123;HOST2&#125;/kubeadmcfg.yaml</span><br><span class="line">cp -R /etc/kubernetes/pki /tmp/$&#123;HOST2&#125;/</span><br><span class="line"># cleanup non-reusable certificates</span><br><span class="line">find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete</span><br><span class="line"></span><br><span class="line">kubeadm alpha phase certs etcd-server --config=/tmp/$&#123;HOST1&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-peer --config=/tmp/$&#123;HOST1&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/$&#123;HOST1&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/$&#123;HOST1&#125;/kubeadmcfg.yaml</span><br><span class="line">cp -R /etc/kubernetes/pki /tmp/$&#123;HOST1&#125;/</span><br><span class="line">find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete</span><br><span class="line"></span><br><span class="line">kubeadm alpha phase certs etcd-server --config=/tmp/$&#123;HOST0&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-peer --config=/tmp/$&#123;HOST0&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/$&#123;HOST0&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/$&#123;HOST0&#125;/kubeadmcfg.yaml</span><br><span class="line"># No need to move the certs because they are for HOST0</span><br><span class="line"></span><br><span class="line"># clean up certs that should not be copied off this host</span><br><span class="line">find /tmp/$&#123;HOST2&#125; -name ca.key -type f -delete</span><br><span class="line">find /tmp/$&#123;HOST1&#125; -name ca.key -type f -delete</span><br></pre></td></tr></table></figure><h3><span id="5-复制证书和kubeadm配置">5. 复制证书和kubeadm配置</span></h3><p>已生成证书，现在必须将它们移动到各自的主机。<br>将 <code>kubeadmcfg.yaml</code> 和 <code>pki</code> 目录拷贝到对应的主机上<code>/etc/kubernetes/pki</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">USER=ubuntu</span><br><span class="line"> HOST=$&#123;HOST1&#125;</span><br><span class="line"> scp -r /tmp/$&#123;HOST&#125;/* $&#123;USER&#125;@$&#123;HOST&#125;:</span><br><span class="line"> ssh $&#123;USER&#125;@$&#123;HOST&#125;</span><br><span class="line"> USER@HOST $ sudo -Es</span><br><span class="line"> root@HOST $ chown -R root:root pki</span><br><span class="line"> root@HOST $ mv pki /etc/kubernetes/</span><br></pre></td></tr></table></figure><h3><span id="6-确保存在所有预期文件">6. 确保存在所有预期文件</span></h3><p>所需文件的完整列表<code>$HOST0</code>是：</p><h3><span id="7-创建静态pod清单">7. 创建静态pod清单</span></h3><p>现在证书和配置已到位，是时候创建清单了。在每个主机上运行kubeadm命令以生成etcd的静态清单。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@HOST0 $ kubeadm alpha phase etcd local --config=/tmp/$&#123;HOST0&#125;/kubeadmcfg.yaml</span><br><span class="line">root@HOST1 $ kubeadm alpha phase etcd local --config=/tmp/$&#123;HOST1&#125;/kubeadmcfg.yaml</span><br><span class="line">root@HOST2 $ kubeadm alpha phase etcd local --config=/tmp/$&#123;HOST2&#125;/kubeadmcfg.yaml</span><br></pre></td></tr></table></figure><p>查看是否生成文件 <code>/etc/kubernetes/manifests/etcd.yaml</code> .</p><h3><span id="8-过几分钟查看-etcd-pod-的状态">8. 过几分钟，查看 etcd pod 的状态</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># docker ps | grep etcd</span><br><span class="line">e29577f035f6        b8df3b177be2                               &quot;etcd --advertise-...&quot;   24 seconds ago      Up 23 seconds                                                                  k8s_etcd_etcd-test01_kube-system_61e08b291cb3ea4992e74c5a47f0b8e0_12</span><br><span class="line">c9312e7f67af        k8s.gcr.io/pause:3.1                       &quot;/pause&quot;                 24 minutes ago      Up 24 minutes                                                                  k8s_POD_etcd-test01_kube-system_61e08b291cb3ea4992e74c5a47f0b8e0_0</span><br></pre></td></tr></table></figure><p>有两个容器正在运行，如果 <code>pod k8s_etcd_etcd</code>启动失败，使用 <code>docker logs -f k8s_etcd_etcd..</code> 查看启动错误日志。</p><h3><span id="9-检查群集运行状况">9. 检查群集运行状况</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it \</span><br><span class="line">--net host \</span><br><span class="line">-v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.18 etcdctl \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--endpoints https://$&#123;HOST0&#125;:2379 cluster-health</span><br><span class="line"></span><br><span class="line">member 327e88294e54db55 is healthy: got healthy result from https://++:2379</span><br><span class="line">cluster is healthy</span><br></pre></td></tr></table></figure><h2><span id="使用-kubeadm-生成etcd证书并使用systemctl管理">使用 <code>kubeadm</code> 生成<code>etcd</code>证书，，并使用<code>systemctl</code>管理</span></h2><blockquote><p>使用 <code>systemctl</code>管理 <code>etcd</code> ，不占用 <code>kubelet</code>，可以在主机上同时作为kubernetes 节点。</p></blockquote><h3><span id="1-使用以下脚本为每个将在其上运行etcd成员的主机生成一个etcdservice服务启动文件">1. 使用以下脚本为每个将在其上运行etcd成员的主机生成一个<code>etcd.service</code>服务启动文件。</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">export HOST0=10.0.0.6</span><br><span class="line">export HOST1=10.0.0.7</span><br><span class="line">export HOST2=10.0.0.8</span><br><span class="line"></span><br><span class="line"># Create temp directories to store files that will end up on other hosts.</span><br><span class="line">mkdir -p /tmp/$&#123;HOST0&#125;/ /tmp/$&#123;HOST1&#125;/ /tmp/$&#123;HOST2&#125;/</span><br><span class="line"></span><br><span class="line">ETCDHOSTS=($&#123;HOST0&#125; $&#123;HOST1&#125; $&#123;HOST2&#125;)</span><br><span class="line">NAMES=(&quot;infra0&quot; &quot;infra1&quot; &quot;infra2&quot;)</span><br><span class="line"></span><br><span class="line">for i in &quot;$&#123;!ETCDHOSTS[@]&#125;&quot;; do</span><br><span class="line">HOST=$&#123;ETCDHOSTS[$i]&#125;</span><br><span class="line">NAME=$&#123;NAMES[$i]&#125;</span><br><span class="line">cat &lt;&lt; EOF &gt; /tmp/$&#123;HOST&#125;/etcd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line">Documentation=https://github.com/coreos</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">WorkingDirectory=/var/lib/etcd/</span><br><span class="line">ExecStart=/usr/local/bin/etcd \\</span><br><span class="line">  --name=$&#123;NAME&#125; \\</span><br><span class="line">  --cert-file=/etc/kubernetes/pki/etcd/server.crt \\</span><br><span class="line">  --key-file=/etc/kubernetes/pki/etcd/server.key \\</span><br><span class="line">  --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt \\</span><br><span class="line">  --peer-key-file=/etc/kubernetes/pki/etcd/peer.key \\</span><br><span class="line">  --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt \\</span><br><span class="line">  --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt \\</span><br><span class="line">  --initial-advertise-peer-urls=https://$&#123;HOST&#125;:2380 \\</span><br><span class="line">  --listen-peer-urls=https://$&#123;HOST&#125;:2380 \\</span><br><span class="line">  --listen-client-urls=https://$&#123;HOST&#125;:2379,http://127.0.0.1:2379 \\</span><br><span class="line">  --advertise-client-urls=https://$&#123;HOST&#125;:2379 \\</span><br><span class="line">  --initial-cluster=infra0=https://$&#123;ETCDHOSTS[0]&#125;:2380,infra1=https://$&#123;ETCDHOSTS[1]&#125;:2380,infra2=https://$&#123;ETCDHOSTS[2]&#125;:2380 \\</span><br><span class="line">  --initial-cluster-state=new \\</span><br><span class="line">  --client-cert-auth=true \\</span><br><span class="line">  --peer-client-cert-auth=true  \\</span><br><span class="line">  --snapshot-count=10000   \\</span><br><span class="line">  --data-dir=/var/lib/etcd</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line">done</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">把三个 etcd.service 文件复制到对应主机的 /etc/systemd/system/ 目录下</span><br></pre></td></tr></table></figure><h3><span id="2-使用-kubeadm-生成etcd证书">2.  使用 <code>kubeadm</code> 生成<code>etcd</code>证书</span></h3><blockquote><p>仅生成一套证书，供etcd集群节点公用、</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">export HOST0=10.0.0.6</span><br><span class="line">export HOST1=10.0.0.7</span><br><span class="line">export HOST2=10.0.0.8</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /tmp/kubeadm-etcd-cfg.yaml</span><br><span class="line">apiVersion: &quot;kubeadm.k8s.io/v1alpha2&quot;</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">etcd:</span><br><span class="line">    local:</span><br><span class="line">        serverCertSANs:</span><br><span class="line">        - &quot;127.0.0.1&quot;</span><br><span class="line">        - &quot;$&#123;HOST0&#125;&quot;</span><br><span class="line">        - &quot;$&#123;HOST1&#125;&quot;</span><br><span class="line">        - &quot;$&#123;HOST2&#125;&quot;</span><br><span class="line">        peerCertSANs:</span><br><span class="line">        - &quot;127.0.0.1&quot;</span><br><span class="line">        - &quot;$&#123;HOST0&#125;&quot;</span><br><span class="line">        - &quot;$&#123;HOST1&#125;&quot;</span><br><span class="line">        - &quot;$&#123;HOST2&#125;&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4><span id="创建根证书">创建根证书</span></h4><p><code>kubeadm alpha phase certs etcd-ca</code></p><p>这会创建两个文件</p><ol><li>/etc/kubernetes/pki/etcd/ca.crt</li><li>/etc/kubernetes/pki/etcd/ca.key</li></ol><h4><span id="创建证书">创建证书</span></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase certs etcd-server --config=/tmp/kubeadm-etcd-cfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-peer --config=/tmp/kubeadm-etcd-cfg.yaml</span><br><span class="line">kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/kubeadm-etcd-cfg.yaml</span><br></pre></td></tr></table></figure><h4><span id="复制证书到其他两台主机对应目录">复制证书到其他两台主机对应目录</span></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pki/</span><br><span class="line">├── apiserver-etcd-client.crt</span><br><span class="line">├── apiserver-etcd-client.key</span><br><span class="line">└── etcd</span><br><span class="line">    ├── ca.crt</span><br><span class="line">    ├── ca.key</span><br><span class="line">    ├── peer.crt</span><br><span class="line">    ├── peer.key</span><br><span class="line">    ├── server.crt</span><br><span class="line">    └── server.key</span><br><span class="line"></span><br><span class="line">1 directory, 8 files</span><br></pre></td></tr></table></figure><h3><span id="3-启动etcd服务验证服务">3. 启动etcd服务,验证服务</span></h3><p>启动 etcd 服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable etcd</span><br><span class="line">systemctl start etcd</span><br><span class="line">systemctl status etcd</span><br></pre></td></tr></table></figure><p>验证服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">etcdctl \</span><br><span class="line">  --endpoints=https://$&#123;NODE_IP&#125;:2379  \</span><br><span class="line">  --ca-file=/etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">  --cert-file=/etc/kubernetes/pki/etcd/server.crt \</span><br><span class="line">  --key-file=/etc/kubernetes/pki/etcd/server.key \</span><br><span class="line">  cluster-health</span><br></pre></td></tr></table></figure><h2><span id="kubernetes-配置外部-etcd">Kubernetes 配置外部 etcd</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">etcd:</span><br><span class="line">  external:</span><br><span class="line">    endpoints:</span><br><span class="line">    - https://etcd_HOST0:2379</span><br><span class="line">    - https://etcd_HOST1:2379</span><br><span class="line">    - https://etcd_HOST2:2379</span><br><span class="line">    caFile: /etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">    certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt</span><br><span class="line">    keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubeadm默认在由master节点上的kubelet管理的静态pod中运行单个成员etcd集群。这不是高可用性设置，因为etcd集群只包含一个成员，并且无法维持任何成员变得不可用。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 1.11.3 安装</title>
    <link href="yunke.science/2018/09/27/k8s-1-11-install/"/>
    <id>yunke.science/2018/09/27/k8s-1-11-install/</id>
    <published>2018-09-27T05:50:42.000Z</published>
    <updated>2018-09-27T05:54:15.024Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes 1.11.3 发布，容器编排工具</p><a id="more"></a><p>@[TOC]</p><h2><span id="初始化">初始化</span></h2><h3><span id="设置主机名">设置主机名</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl  set-hostname docker01</span><br></pre></td></tr></table></figure><h3><span id="配置-dns">配置 dns</span></h3><blockquote><p>echo nameserver 114.114.114.114&gt;&gt;/etc/resolv.conf</p></blockquote><h3><span id="停止防火墙">停止防火墙</span></h3><p>systemctl disable firewalld &amp;&amp; systemctl stop firewalld &amp;&amp; systemctl status firewalld</p><h3><span id="关闭swap">关闭Swap</span></h3><blockquote><p>swapoff -a<br>sed  ‘s/.<em>swap.</em>/#&amp;/‘ /etc/fstab</p></blockquote><h3><span id="selinux">Selinux</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ -f /etc/selinux/config ] &amp;&amp; sed -i &apos;s/^SELINUX=.*/SELINUX=disabled/g&apos; /etc/selinux/config</span><br><span class="line">[ -f /etc/sysconfig/selinux ] &amp;&amp; sed -i &apos;s/^SELINUX=.*/SELINUX=disabled/g&apos; /etc/sysconfig/selinux</span><br><span class="line">[ -x /usr/sbin/setenforce ] &amp;&amp; /usr/sbin/setenforce 0</span><br></pre></td></tr></table></figure><h3><span id="配置-dns">配置 dns</span></h3><blockquote><p>echo nameserver 114.114.114.114&gt;&gt;/etc/resolv.conf</p></blockquote><h3><span id="修改内核参数">修改内核参数</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">vm.swappiness=0</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sysctl -p /etc/sysctl.d/k8s.conf</span><br><span class="line"></span><br><span class="line">echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-iptables</span><br><span class="line">echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-ip6tables</span><br></pre></td></tr></table></figure><p><code>swapoff  -a</code></p><h2><span id="k8s-rpm-install">k8s rpm install</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg</span><br><span class="line">       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">指定版本下载安装：</span><br><span class="line">离线下载rpm包</span><br><span class="line">mkdir k8srpm &amp;&amp; cd k8srpm</span><br><span class="line">yum install -y yum-utils</span><br><span class="line">yumdownloader kubectl-1.11.3 kubelet-1.11.3 kubeadm-1.11.3  kubernetes-cni</span><br><span class="line"></span><br><span class="line">yum install  ./*.rpm -y</span><br><span class="line"></span><br><span class="line">systemctl enable kubelet.service</span><br></pre></td></tr></table></figure><h2><span id="关于-kubelet-cgroup-driver">关于 kubelet cgroup driver</span></h2><p>当前版本，使用Docker时，kubeadm会自动检测<code>kubelet</code>的cgroup驱动程序，并在运行时将其设置在<code>/var/lib/kubelet/kubeadm-flags.env</code>文件中。</p><p>因此，无需修改 kubelet cgroup driver</p><h2><span id="准备镜像">准备镜像</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">docker pull k8s.gcr.io/kube-apiserver-amd64:v1.11.3</span><br><span class="line">docker pull k8s.gcr.io/kube-controller-manager-amd64:v1.11.3</span><br><span class="line">docker pull k8s.gcr.io/kube-proxy-amd64:v1.11.3</span><br><span class="line">docker pull k8s.gcr.io/kube-scheduler-amd64:v1.11.3</span><br><span class="line">docker pull k8s.gcr.io/pause:3.1</span><br><span class="line">docker pull k8s.gcr.io/coredns:1.1.3</span><br><span class="line">docker pull quay.io/calico/node:v3.1.3</span><br><span class="line">docker pull quay.io/calico/cni:v3.1.3</span><br><span class="line">docker pull k8s.gcr.io/etcd-amd64:3.2.18</span><br></pre></td></tr></table></figure><h2><span id="kube-proxy-使用-ipvs-模式">kube-proxy 使用 ipvs 模式</span></h2><ol><li>对于Kubernetes v1.10及更高版本，功能门默认 <code>SupportIPVSProxyMode</code>设置为<code>true</code>。但是，在v1.10之前版本用您需要 <code>--feature-gates=SupportIPVSProxyMode=true</code> 。</li><li>指定 <code>proxy-mode=ipvs</code></li><li>安装需要的内核模块和软件包<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">安装</span><br><span class="line">yum install ipvsadm -y</span><br><span class="line">加载</span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br><span class="line">检查</span><br><span class="line">cut -f1 -d &quot; &quot;  /proc/modules | grep -e ip_vs -e nf_conntrack_ipv4</span><br><span class="line">输出</span><br><span class="line">ip_vs_sh</span><br><span class="line">ip_vs_wrr</span><br><span class="line">ip_vs_rr</span><br><span class="line">nf_conntrack_ipv4</span><br><span class="line">ip_vs</span><br></pre></td></tr></table></figure></li></ol><blockquote><p><a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/proxy/ipvs" target="_blank" rel="noopener">详细IPVS</a></p></blockquote><blockquote><p><a href="https://blog.csdn.net/cloudvtech/article/details/79942121" target="_blank" rel="noopener">kubernetes系列之六：安装kubernets v1.10.0和ipvs mode kube-proxy</a></p></blockquote><h2><span id="kubeadm-init-初始化">Kubeadm Init 初始化</span></h2><p><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file" target="_blank" rel="noopener">将kubeadm init与配置文件一起使用</a></p><p><code>kubeadm init</code>使用配置文件而不是命令行标志进行配置，而某些更高级的功能可能仅作为配置文件选项提供。该文件在–config选项中传递。</p><p>在<code>Kubernetes 1.11</code>及更高版本中，可以使用==kubeadm config print-default==命令打印出默认配置 。这是建议您在旧迁移v1alpha1配置v1alpha2使用kubeadm配置迁移命令，因为v1alpha1会在Kubernetes 1.12被删除。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># cat kubeadm-init.conf</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">imageRepository: k8s.gcr.io</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 192.168.1.241</span><br><span class="line">  bindPort: 6443</span><br><span class="line">  controlPlaneEndpoint: &quot;&quot;</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: &quot;ipvs&quot;</span><br><span class="line">kubeletConfiguration:</span><br><span class="line">  baseConfig:</span><br><span class="line">    clusterDNS:</span><br><span class="line">    - 10.96.0.10</span><br><span class="line">    clusterDomain: cluster.local</span><br><span class="line">kubernetesVersion: v1.11.3</span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  podSubnet: &quot;10.42.0.0/16&quot;</span><br><span class="line">  serviceSubnet: 10.96.0.0/12</span><br></pre></td></tr></table></figure><blockquote><p>这里没有配置etcd ，<code>kubeadm init</code> 自动安装etcd。<br>Kubeadm默认在mater节点上的kubelet管理的静态pod中运行单节点的etcd集群。这不是高可用性设置，因为etcd集群只包含一个成员，并且无法维持任何成员变得不可用。</p></blockquote><p>以上主要修改2点：</p><ol><li>kubeProxy 启用 ipvs</li><li>指定 pod 子网段为 <code>10.42.0.0/16</code></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"># kubeadm init --config kubeadm-init.conf  </span><br><span class="line">[init] using Kubernetes version: v1.11.3</span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">I0920 15:57:53.117081   30624 kernel_validator.go:81] Validating kernel version</span><br><span class="line">I0920 15:57:53.117279   30624 kernel_validator.go:96] Validating kernel config</span><br><span class="line">[preflight/images] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight/images] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class="line">[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[preflight] Activating the kubelet service</span><br><span class="line">[certificates] Generated ca certificate and key.</span><br><span class="line">[certificates] Generated apiserver certificate and key.</span><br><span class="line">[certificates] apiserver serving cert is signed for DNS names [localhost.localdomain kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.241]</span><br><span class="line">[certificates] Generated apiserver-kubelet-client certificate and key.</span><br><span class="line">[certificates] Generated sa key and public key.</span><br><span class="line">[certificates] Generated front-proxy-ca certificate and key.</span><br><span class="line">[certificates] Generated front-proxy-client certificate and key.</span><br><span class="line">[certificates] Generated etcd/ca certificate and key.</span><br><span class="line">[certificates] Generated etcd/server certificate and key.</span><br><span class="line">[certificates] etcd/server serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [127.0.0.1 ::1]</span><br><span class="line">[certificates] Generated etcd/peer certificate and key.</span><br><span class="line">[certificates] etcd/peer serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [192.168.1.241 127.0.0.1 ::1]</span><br><span class="line">[certificates] Generated etcd/healthcheck-client certificate and key.</span><br><span class="line">[certificates] Generated apiserver-etcd-client certificate and key.</span><br><span class="line">[certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot;</span><br><span class="line">[controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;</span><br><span class="line">[controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span><br><span class="line">[controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span><br><span class="line">[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span><br><span class="line">[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot; </span><br><span class="line">[init] this might take a minute or longer if the control plane images have to be pulled</span><br><span class="line">[apiclient] All control plane components are healthy after 49.001944 seconds</span><br><span class="line">[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.11&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[markmaster] Marking the node localhost.localdomain as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class="line">[markmaster] Marking the node localhost.localdomain as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;localhost.localdomain&quot; as an annotation</span><br><span class="line">[bootstraptoken] using token: wadx1n.9gfusllys13qk1yj</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of machines by running the following on each node</span><br><span class="line">as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 192.168.1.241:6443 --token se4mgd.ukmg2hxxg42gt65t --discovery-token-ca-cert-hash sha256:0a737675e1e37aa4025077b27ced8053fe84c363df11c506bfb512b88408697e</span><br></pre></td></tr></table></figure><p>启动完毕</p><h2><span id="卸载-kubeadm-安装">卸载 kubeadm 安装</span></h2><p>要撤消kubeadm所执行的操作，您应首先排空节点，并确保节点在关闭之前为空。</p><p>使用适当的凭据与主人交谈，运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets</span><br><span class="line">kubectl delete node &lt;node name&gt;</span><br></pre></td></tr></table></figure><p>然后，在要删除的节点上，重置所有kubeadm安装状态：</p><p><code>kubeadm reset</code></p><p>如果您希望重新开始，只需运行<code>kubeadm init</code>或<code>kubeadm join</code>使用适当的参数。</p><h2><span id="验证集群">验证集群</span></h2><ol><li>按照提示，复制配置文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line"># kubectl  -n kube-system get po -o wide</span><br><span class="line">NAME                             READY     STATUS    RESTARTS   AGE       IP              NODE      NOMINATED NODE</span><br><span class="line">coredns-78fcdf6894-2p6gp         0/1       Pending   0          2m        &lt;none&gt;          &lt;none&gt;    &lt;none&gt;</span><br><span class="line">coredns-78fcdf6894-wnz4p         0/1       Pending   0          2m        &lt;none&gt;          &lt;none&gt;    &lt;none&gt;</span><br><span class="line">etcd-test01                      1/1       Running   0          1m        192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-apiserver-test01            1/1       Running   0          1m        192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-controller-manager-test01   1/1       Running   0          1m        192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-proxy-8wjp8                 1/1       Running   0          2m        192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-scheduler-test01            1/1       Running   0          1m        192.168.1.241   test01    &lt;none&gt;</span><br></pre></td></tr></table></figure></li></ol><p>由于还没有部署网络，<code>coredns</code> 处于 <code>Pending</code> 状态。</p><ol start="2"><li><p>验证 cgroup-driver 与 docker cgroup-driver 是否匹配：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /var/lib/kubelet/kubeadm-flags.env</span><br><span class="line">KUBELET_KUBEADM_ARGS=--cgroup-driver=systemd --cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/cni/net.d --network-plugin=cni</span><br></pre></td></tr></table></figure></li><li><p>验证 kube-proxy IPVS 模式</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># ipvsadm -L -n</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  10.96.0.1:443 rr</span><br><span class="line">  -&gt; 192.168.1.241:6443           Masq    1      0          0         </span><br><span class="line">TCP  10.96.0.10:53 rr</span><br><span class="line">UDP  10.96.0.10:53 rr</span><br></pre></td></tr></table></figure><h2><span id="安装calico网络">安装calico网络</span></h2><p><a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network" target="_blank" rel="noopener">安装pod网络附加组件</a></p><p>有关使用Calico的更多信息，请参阅Kubernetes上的 Calico 快速入门，为策略和网络安装Calico以及其他相关资源。</p><p>calico 默认pod网络为<code>192.168.0.0/16</code>, 上面kubeadm初始化时指定了 <code>networking.podSubnet: &quot;10.42.0.0/16&quot;</code> 。<br>这里需要修改<code>calico.yaml</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">加载rbac策略</span><br><span class="line">kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">修改 CALICO_IPV4POOL_CIDR</span><br><span class="line">wget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml</span><br><span class="line">sed -i &apos;s/192.168.0.0/10.42.0.0/g&apos; calico.yaml</span><br><span class="line">grep 0.0 calico.yaml </span><br><span class="line"></span><br><span class="line">kubectl apply -f calico.yaml</span><br></pre></td></tr></table></figure><p>查看容器状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># kubectl  -n kube-system get po -o wide</span><br><span class="line">NAME                             READY     STATUS    RESTARTS   AGE       IP              NODE      NOMINATED NODE</span><br><span class="line">calico-node-sfxvc                2/2       Running   0          32s       192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">coredns-78fcdf6894-2p6gp         1/1       Running   0          28m       10.42.0.2       test01    &lt;none&gt;</span><br><span class="line">coredns-78fcdf6894-wnz4p         1/1       Running   0          28m       10.42.0.3       test01    &lt;none&gt;</span><br><span class="line">etcd-test01                      1/1       Running   0          27m       192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-apiserver-test01            1/1       Running   0          27m       192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-controller-manager-test01   1/1       Running   0          27m       192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-proxy-8wjp8                 1/1       Running   0          28m       192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-scheduler-test01            1/1       Running   0          27m       192.168.1.241   test01    &lt;none&gt;</span><br></pre></td></tr></table></figure><p>增加了一个 <code>calico</code> 容器，<code>coredns</code> 启动成功。</p><p>查看 ipvs 列表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># ipvsadm -L -n</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  10.96.0.1:443 rr</span><br><span class="line">  -&gt; 192.168.1.241:6443           Masq    1      4          0         </span><br><span class="line">TCP  10.96.0.10:53 rr</span><br><span class="line">  -&gt; 10.42.0.2:53                 Masq    1      0          0         </span><br><span class="line">  -&gt; 10.42.0.3:53                 Masq    1      0          0         </span><br><span class="line">TCP  10.99.127.226:5473 rr</span><br><span class="line">UDP  10.96.0.10:53 rr</span><br><span class="line">  -&gt; 10.42.0.2:53                 Masq    1      0          0         </span><br><span class="line">  -&gt; 10.42.0.3:53                 Masq    1      0          0</span><br></pre></td></tr></table></figure><h2><span id="去除对master的调度隔离">去除对master的调度隔离</span></h2><p>默认情况下，出于安全原因，您的群集不会在<code>master</code>服务器上安排容器。如果您希望能够在<code>master</code>服务器上安排<code>pod</code>，例如，对于用于开发的单机<code>Kubernetes</code>集群，请运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubectl taint nodes --all node-role.kubernetes.io/master-</span><br><span class="line">node/test01 untainted</span><br></pre></td></tr></table></figure><p>这将从<code>node-role.kubernetes.io/master</code>包含主节点的任何节点中删除<code>taint</code>，这意味着调度程序将能够在任何地方安排<code>pod</code>。</p><h2><span id="部署服务测试">部署服务测试</span></h2><blockquote><p>cloudnativelabs/whats-my-ip 是一个每次请求，返回当前主机的主机名和IP的服务</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run myip --image=cloudnativelabs/whats-my-ip --replicas=3 --port=8080</span><br></pre></td></tr></table></figure><p>创建一个服务，类型为NodePort<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment myip --port=8080 --target-port=8080 --type=NodePort</span><br></pre></td></tr></table></figure></p><p>启动一个busybox镜像容器 用于测试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run tools --image=byrnedo/alpine-curl --replicas=1 --command -- sleep 99999999</span><br></pre></td></tr></table></figure><p>查看服务状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># kubectl  get po -o wide</span><br><span class="line">NAME                     READY     STATUS    RESTARTS   AGE       IP           NODE      NOMINATED NODE</span><br><span class="line">myip-5fc5cf6476-cfwjn    1/1       Running   0          15m       10.42.0.7    test01    &lt;none&gt;</span><br><span class="line">myip-5fc5cf6476-g55f7    1/1       Running   0          15m       10.42.0.8    test01    &lt;none&gt;</span><br><span class="line">myip-5fc5cf6476-wvxvt    1/1       Running   0          15m       10.42.0.6    test01    &lt;none&gt;</span><br><span class="line">tools-7c7dcbc894-74m6z   1/1       Running   0          1m        10.42.0.10   test01    &lt;none&gt;</span><br><span class="line"></span><br><span class="line">[root@test01 ~]# kubectl  get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          17h</span><br><span class="line">myip         NodePort    10.101.143.54   &lt;none&gt;        8080:30319/TCP   3m</span><br></pre></td></tr></table></figure><p>测试myip服务状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># kubectl  exec -it tools-7c7dcbc894-74m6z  /bin/sh</span><br><span class="line">测试 dns 是否正常解析</span><br><span class="line">/ # nslookup myip</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      myip</span><br><span class="line">Address 1: 10.101.143.54 myip.default.svc.cluster.local</span><br><span class="line">访问服务测试，连续访问十次</span><br><span class="line">/ # for i in $(seq 1 10);do curl myip:8080 ;done</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-cfwjn IP:10.42.0.7</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-wvxvt IP:10.42.0.6</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-g55f7 IP:10.42.0.8</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-cfwjn IP:10.42.0.7</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-wvxvt IP:10.42.0.6</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-g55f7 IP:10.42.0.8</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-cfwjn IP:10.42.0.7</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-wvxvt IP:10.42.0.6</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-g55f7 IP:10.42.0.8</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-cfwjn IP:10.42.0.7</span><br></pre></td></tr></table></figure><h2><span id="hairpin-mode">Hairpin Mode</span></h2><p>许多网络附加组件尚未启用<code>Hairpin</code>模式 ，<code>Hairpin</code>模式允许pod通过其服务IP访问自己。这是与CNI相关的问题 。请联系网络附加提供商以获取他们对发夹模式支持的最新状态。</p><p>calico 网络经过以上测试，pod可以通过其服务IP访问自己，无需配置 <code>Hairpin Mode</code> .</p><h2><span id="添加节点">添加节点</span></h2><p>节点是运行工作负载（容器和容器等）的位置。要向群集添加新节点，先执行以上的准备工作</p><p>然后执行 <code>master</code> 节点 <code>kubeadm init</code> 输出的最后一句, 即可加入集群：</p><p><code>kubeadm join 192.168.1.241:6443 --token se4mgd.ukmg2hxxg42gt65t --discovery-token-ca-cert-hash sha256:0a737675e1e37aa4025077b27ced8053fe84c363df11c506bfb512b88408697e</code></p><p><strong>如果不知道以上命令，怎么查询 token 和 discovery-token-ca-cert-has 呢？</strong></p><ol><li><p>如果没有<code>token</code>，执行以下查询获得</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># kubeadm token list</span><br><span class="line">TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION   EXTRA GROUPS</span><br><span class="line">se4mgd.ukmg2hxxg42gt65t   5h        2018-09-21T17:32:06+08:00   authentication,signing   &lt;none&gt;        system:bootstrappers:kubeadm:default-node-token</span><br></pre></td></tr></table></figure></li><li><p>默认情况下，令牌在24小时后过期。如果在当前令牌过期后将节点加入群集，则可以通过在主节点上运行以下命令来创建新令牌：</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubeadm token create</span><br><span class="line">ih6qhw.tbkp26l64xivcca7</span><br></pre></td></tr></table></figure><ol start="3"><li>如果没有<code>discovery-token-ca-cert-hash</code>,执行以下查询获得。<br><code>--discovery-token-ca-cert-hash</code> 的值可以配合多个<code>token</code>以重复使用。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | \</span><br><span class="line">&gt;    openssl dgst -sha256 -hex | sed &apos;s/^.* //&apos;</span><br><span class="line">0a737675e1e37aa4025077b27ced8053fe84c363df11c506bfb512b88408697e</span><br></pre></td></tr></table></figure><ol start="4"><li>通过创建新的token时，打印 join 命令：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubeadm token create --print-join-command</span><br><span class="line">kubeadm join 192.168.1.241:6443 --token 771jdx.k0jtjvzgfiu8k4uv --discovery-token-ca-cert-hash sha256:0a737675e1e37aa4025077b27ced8053fe84c363df11c506bfb512b88408697e</span><br></pre></td></tr></table></figure><p>几秒钟后，您应该注意到kubectl get nodes在master服务器上运行时输出中的此节点。</p><p><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/" target="_blank" rel="noopener">kubeadm join 详细</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes 1.11.3 发布，容器编排工具&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>CKA常见问题解答</title>
    <link href="yunke.science/2018/09/19/cka-faq/"/>
    <id>yunke.science/2018/09/19/cka-faq/</id>
    <published>2018-09-19T07:19:24.000Z</published>
    <updated>2018-09-19T07:55:56.695Z</updated>
    
    <content type="html"><![CDATA[<p>认证的Kubernetes管理员（CKA）和认证的Kubernetes应用程序开发人员（CKAD）常见问题解答</p><p>考试费用是多少？<br>考试费用为300美元。</p><a id="more"></a><p>@[TOC]</p><h2><span id="认证的kubernetes管理员cka和认证的kubernetes应用程序开发人员ckad常见问题解答">认证的Kubernetes管理员（CKA）和认证的Kubernetes应用程序开发人员（CKAD）常见问题解答</span></h2><h3><span id="考试费用是多少">考试费用是多少？</span></h3><p>考试费用为300美元。</p><h3><span id="考试需要多长时间">考试需要多长时间？</span></h3><p>根据候选人的经验水平，CKA考试大约需要3个小时才能完成，CKAD大约需要2个小时才能完成。</p><p>考生最多需要3小时（CKA）/ 2小时（CKAD）才能完成考试。</p><h3><span id="考试如何进行">考试如何进行？</span></h3><p>在考试期间通过流式音频，视频和屏幕共享源远程监控认证考试。屏幕共享需要允许监管人员查看候选人的桌面（包括所有监视器）。如果随后需要审核，则音频，视频和屏幕共享源将被存储一段有限的时间。</p><h3><span id="参加考试的系统要求是什么">参加考试的系统要求是什么？</span></h3><p>考试通过网络摄像头，音频和远程屏幕观看在线提供并由监管人员密切监控。考生必须提供自己的前端硬件才能参加考试，包括一台电脑：</p><ul><li>Chrome或Chromium浏览器</li><li>可靠的互联网接入</li><li>摄像头</li><li>麦克风</li></ul><p>除了所需的硬件之外，放置硬件（即台式机或笔记本电脑）的工作站必须露出干净的表面，顶部或下方没有障碍物。考生应确保他们的网络摄像头能够被移动，以防监考人员要求候选人平移以检查周围环境可能违反考试政策的情况。</p><p>考生应运行考试委托合作伙伴提供的兼容性检查工具，以验证其硬件是否符合最低要求。选择“Linux Foundation”作为考试赞助商，选择“CKA”或“CKAD”作为考试。（目前，仅支持Chrome和Chromium浏览器。）</p><h3><span id="我在考试期间可以访问哪些资源">我在考试期间可以访问哪些资源？</span></h3><p>候选人可以使用他们的<code>Chrome</code>或<code>Chromium</code>浏览器打开一个额外的标签，以便访问<a href="https://kubernetes.io/docs/" target="_blank" rel="noopener">https://kubernetes.io/docs/</a>及其子域名或<a href="https://kubernetes.io/blog/" target="_blank" rel="noopener">https://kubernetes.io/blog/</a>上的资源。不能打开其他选项卡，也不能导航到其他站点。==上面允许的网站可能包含指向外部网站的链接，候选人有责任不点击任何导致他们导航到不允许的域的链接。==</p><h3><span id="我可以在考试期间做笔记吗">我可以在考试期间做笔记吗？</span></h3><p>是的，但仅使用可在考试控制台顶部菜单栏中访问的记事本功能。请注意，在考试结束后，此处输入的注释将不会被保留或访问。如果您需要帮助操作记事本，请询问您的监考人员。</p><h3><span id="参加考试的id要求是什么">参加考试的ID要求是什么？</span></h3><p>考生必须在考试开始前提供政府颁发的带照片的身份证明。任何包含候选人照片和拉丁字母中候选人姓名的未过期政府身份证可用于身份验证。政府签发的带照片的可接受形式的示例包括但不限于：</p><ul><li>护照</li><li>政府颁发的驾驶执照/许可证</li><li>国民身份证</li><li>州或省颁发的身份证</li></ul><p>请查看候选手册以获取更多信息。如果您对照片ID是否可以接受有疑问，请联系：<a href="mailto:certificationsupport@cncf.io" target="_blank" rel="noopener">certificationsupport@cncf.io</a>。</p><p>要注册考试，您需要一个Linux Foundation ID。如果您还没有Linux Foundation ID，请在以下位置创建一个：<a href="https：//identity.linuxfoundation.org/">https：//identity.linuxfoundation.org/</a>。完成后，您可以在出现提示时使用新的Linux Foundation ID凭据登录。</p><h3><span id="我的考试成绩如何">我的考试成绩如何？</span></h3><p>考试通常在完成后24小时内自动评分。结果将在考试完成后36小时内通过电子邮件发送。结果也将在My Portal上提供。</p><p>考试成绩分级。在考试中执行目标的方法可能不止一种，除非另有说明，否则候选人可以选择任何可用的路径来执行目标，只要它产生正确的结果即可。</p><h3><span id="提供什么语言的考试">提供什么语言的考试？</span></h3><p>CKA和CKAD考试目前以英语授课。</p><h3><span id="我能重考吗">我能重考吗？</span></h3><p>对于直接从云计算本地计算基金会购买的考试，如果未达到及格分数并且候选人未被视为没有资格获得认证或重新考试，则每次考试购买将获得一（1）次免费重考。除非在考试中另有说明，否则==免费重考必须在原始考试购买之日起12个月内完成==。在免费重考已经用尽或者免费重考的截止日期已经过去之后，候选人可以注册并支付再次参加考试，而不会给予此类额外重考。</p><p>对于通过授权培训合作伙伴（ATP）进行的购买，请联系ATP以获取免费重考的资格。</p><h3><span id="我的认证有效期多长">我的认证有效期多长？</span></h3><p>Cloud Native Computing Foundation认证有效期为2年，候选人可通过填写下面的续订要求选项保持认证有效。续订要求必须在认证到期之前完成。</p><h3><span id="如何续订认证">如何续订认证？</span></h3><p>考生可以选择重考并通过相同的考试，以保证其证书有效。该认证将从重考和通过考试之日起2年内有效。</p><h3><span id="我如何从认证中受益">我如何从认证中受益？</span></h3><p>认证Kubernetes管理员（CKA）认证旨在确保认证持有者具备履行Kubernetes管理员职责的技能，知识和能力。CKA认证允许经过认证的管理员在就业市场中快速建立自己的信誉和价值，并允许公司更快地雇用高质量的团队来支持他们的发展。  </p><p>认证Kubernetes应用程序开发人员（CKAD）认证旨在确保CKAD具备履行Kubernetes应用程序开发人员职责的技能，知识和能力。经过认证的<code>Kubernetes Application Developer</code>可以定义应用程序资源并使用核心原语来构建，监视和排除Kubernetes中可伸缩应用程序和工具的故障。</p><h3><span id="我的组织如何从为员工提供cka认证中受益">我的组织如何从为员工提供CKA认证中受益？</span></h3><p>拥有三名或更多CKA认证管理员的组织，Kubernetes社区中可演示的活动（包括积极贡献）和支持企业最终用户的商业模式都有资格成为Kubernetes认证服务提供商（KCSP）。</p><h3><span id="什么是kubernetes认证服务提供商kcsp计划">什么是Kubernetes认证服务提供商（KCSP）计划？</span></h3><p>KCSP计划是一个经过资格审核的预审服务提供商，他们在帮助企业成功采用Kubernetes方面拥有丰富的经验。您可以在此处了解有关该计划以及如何申请的更多信息。</p><h3><span id="是否有培训准备认证考试">是否有培训准备认证考试？</span></h3><p>是! 对于CKA，Linux基金会提供免费的<a href="https://training.linuxfoundation.org/linux-courses/system-administration-training/introduction-to-kubernetes" target="_blank" rel="noopener">Kubernetes入门课程</a>，该课程介绍了Kubernetes的许多关键概念。接下来的课程，<a href="https://training.linuxfoundation.org/linux-courses/system-administration-training/kubernetes-fundamentals" target="_blank" rel="noopener">Kubernetes基础知识（LFS258）</a>，以此介绍性材料为基础，直接达到Kubernetes认证管理员考试的要求。对于CKAD，<code>LFD259 - Kubernetes for Developers</code>即将推出。</p><blockquote><p>文章翻译自 <a href="https://www.cncf.io/certification/cka/faq/" target="_blank" rel="noopener">https://www.cncf.io/certification/cka/faq/</a></p></blockquote><p>2018.09.19</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;认证的Kubernetes管理员（CKA）和认证的Kubernetes应用程序开发人员（CKAD）常见问题解答&lt;/p&gt;
&lt;p&gt;考试费用是多少？&lt;br&gt;考试费用为300美元。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>redis 安全</title>
    <link href="yunke.science/2018/08/28/redis-security/"/>
    <id>yunke.science/2018/08/28/redis-security/</id>
    <published>2018-08-28T03:08:58.000Z</published>
    <updated>2018-08-28T03:12:23.903Z</updated>
    
    <content type="html"><![CDATA[<p>本文档从Redis的角度介绍了安全性主题：Redis提供的访问控制，代码安全问题，可以通过选择恶意输入和其他类似主题从外部触发的攻击。</p><p>对于与安全相关的联系人，请在GitHub上打开一个问题，或者当您认为保持通信安全性非常重要时，请使用本文档末尾的GPG密钥。</p><a id="more"></a><p>@[TOC]</p><h2><span id="redis通用安全模型">Redis通用安全模型</span></h2><p>Redis旨在由受信任环境中的受信任客户端访问。 这意味着通常不会将Redis实例直接暴露给Internet，或者是不受信任的客户端可以直接访问Redis TCP端口或UNIX套接字的环境。</p><p>例如，在使用Redis作为数据库，缓存或消息传递系统实现的Web应用程序的公共上下文中，应用程序前端（Web端）内的客户端将查询Redis以生成页面或执行请求的操作或由Web应用程序用户触发。</p><p>在这种情况下，Web应用程序连接Redis和不受信任的客户端（访问Web应用程序的用户浏览器）之间的访问。</p><p>这是一个具体示例，但是，通常，对Redis的不受信任访问应始终由实现ACL的层，验证用户输入以及决定对Redis实例执行哪些操作来调解。</p><p>一般而言，Redis未针对最大安全性进行优化，而是为了获得最佳性能和简单性。</p><h2><span id="网络安全">网络安全</span></h2><p>除了网络中受信任的客户端之外，每个人都应该拒绝访问Redis端口，因此运行Redis的服务器只能由使用Redis实现应用程序的计算机直接访问。</p><p>在直接暴露于互联网的单台计算机的常见情况下，例如虚拟化的Linux实例（Linode，EC2，…），应该对Redis端口进行防火墙以防止来自外部的访问。 客户端仍然可以使用 <code>localhost</code> 访问Redis。</p><p>请注意，可以通过在redis.conf文件中添加如下行来将Redis绑定到单个接口：</p><p><code>bind 127.0.0.1</code></p><p>由于Redis的性质，未能从外部保护Redis端口会产生很大的安全影响。 例如，外部攻击者可以使用单个<code>FLUSHALL</code>命令删除整个数据集。</p><h2><span id="保护模式">保护模式</span></h2><p>遗憾的是，许多用户无法保护Redis实例不被外部网络访问。 许多实例只是在公共IP上暴露在互联网上。 由于这个原因，<strong>从版本3.2.0开始，当使用默认配置（绑定所有接口）并且没有任何密码来执行Redis以访问它时，它进入称为保护模式的特殊模式 。</strong> 在此模式下，Redis仅回复来自环回接口的查询，并回复与其他地址连接的其他客户端的错误，解释正在发生的事情以及如何正确配置Redis。</p><p>我们希望保护模式能够严重降低因未经正确管理而执行的未受保护的Redis实例导致的安全问题，但系统管理员仍然可以忽略Redis提供的错误，只需禁用保护模式或手动绑定所有接口。</p><h2><span id="身份验证功能">身份验证功能</span></h2><p>虽然Redis没有尝试实现访问控制，但它提供了一个很小的身份验证层，可选择打开编辑<code>redis.conf</code>文件。</p><p>启用授权层后，Redis将拒绝未经身份验证的客户端的任何查询。 客户端可以通过发送<code>AUTH</code>命令后跟密码来验证自身。</p><p>密码由系统管理员以明文形式在redis.conf文件中设置。 它应该足够长以防止暴力攻击有两个原因：</p><ul><li>Redis在提供查询方面非常快。 每秒许多密码可以由外部客户端测试。</li><li>Redis密码存储在redis.conf文件内部和客户端配置中，因此不需要系统管理员记住，因此可能会很长。</li></ul><p>认证层的目标是可选地提供冗余层。 如果防火墙或任何其他用于保护Redis免受外部攻击者攻击的系统失败，外部客户端仍然无法在不知道验证密码的情况下访问Redis实例。</p><p>与其他所有Redis命令一样，AUTH命令以未加密的方式发送，因此它不能防止有足够访问权限的攻击者执行窃听。</p><h2><span id="数据加密支持">数据加密支持</span></h2><p>Redis不支持加密。 为了实现受信任方可以通过Internet或其他不受信任的网络访问Redis实例的设置，应该实施额外的保护层，例如SSL代理。 我们建议使用<a href="https://github.com/Tarsnap/spiped" target="_blank" rel="noopener">spiped</a> 。</p><h2><span id="禁用特定命令">禁用特定命令</span></h2><p><strong>可以在Redis中禁用命令或将它们重命名为不可授权的名称，以便普通客户端仅限于指定的命令集。</strong></p><p>例如，虚拟化服务器提供商可以提供托管的Redis实例服务。 在这种情况下，普通用户可能无法调用<code>Redis CONFIG</code>命令来更改实例的配置，但提供和删除实例的系统应该能够这样做。</p><p>在这种情况下，可以命令重命名或完全隐藏命令。 此功能作为可在<code>redis.conf</code>配置文件中使用的语句提供。 例如：</p><p><code>rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52</code></p><p>在上面的示例中， CONFIG命令已重命名为不可引用的名称。 也可以通过将其重命名为空字符串来完全禁用它（或任何其他命令），如下例所示：</p><p> <code>rename-command CONFIG &quot;&quot;</code> </p><h2><span id="由外部客户精心选择的输入触发的攻击">由外部客户精心选择的输入触发的攻击</span></h2><p>即使没有外部访问实例，攻击者也可以从外部触发一类攻击。 这种攻击的一个例子是能够将数据插入Redis，从而触发Redis内部实现的数据结构的病态（最坏情况）算法复杂性。</p><p>例如，攻击者可以通过Web表单提供一组字符串，这些字符串已知为同一个桶散列到哈希表中，以便将O（1）预期时间（平均时间）转换为O（N ）最糟糕的情况是，消耗的CPU比预期的多，最终导致拒绝服务。</p><p>为防止此特定攻击，Redis对散列函数使用每执行伪随机种子。</p><p>Redis使用qsort算法实现SORT命令。 目前，该算法不是随机的，因此可以通过仔细选择正确的输入集来触发二次最坏情况行为。</p><h2><span id="字符串转义和nosql注入">字符串转义和NoSQL注入</span></h2><p>Redis协议没有字符串转义的概念，因此在正常情况下使用普通客户端库无法注入。 该协议使用前缀长度字符串，完全是二进制安全的。</p><p>由EVAL和EVALSHA命令执行的Lua脚本遵循相同的规则，因此这些命令也是安全的。</p><p>虽然这将是一个非常奇怪的用例，但应用程序应该避免使用从不受信任的源获取的字符串来组成Lua脚本的主体。</p><h2><span id="代码安全性">代码安全性</span></h2><p>在传统的Redis设置中，允许客户端完全访问命令集，但访问实例永远不会导致能够控制运行Redis的系统。</p><p>在内部，Redis使用所有众所周知的实践来编写安全代码，防止缓冲区溢出，格式化错误和其他内存损坏问题。 但是，使用CONFIG命令控制服务器配置的能力使客户端能够更改程序的工作目录和转储文件的名称。 这允许客户端在随机路径上编写RDB Redis文件，这是一个安全问题 ，可能很容易导致破坏系统和/或运行不受信任的代码，因为Redis正在运行相同的用户。</p><p>Redis不需要root权限即可运行。 <strong>建议将其作为无特权的redis用户运行</strong>，仅用于此目的。 Redis作者目前正在研究添加新配置参数以防止<code>CONFIG SET / GET</code>目录和其他类似运行时配置指令的可能性。 这将阻止客户端强制服务器在任意位置写入Redis转储文件。</p><h2><span id="gpg密钥">GPG密钥</span></h2><blockquote><p>翻译自： <a href="https://redis.io/topics/security" target="_blank" rel="noopener">https://redis.io/topics/security</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文档从Redis的角度介绍了安全性主题：Redis提供的访问控制，代码安全问题，可以通过选择恶意输入和其他类似主题从外部触发的攻击。&lt;/p&gt;
&lt;p&gt;对于与安全相关的联系人，请在GitHub上打开一个问题，或者当您认为保持通信安全性非常重要时，请使用本文档末尾的GPG密钥。&lt;/p&gt;
    
    </summary>
    
      <category term="redis" scheme="yunke.science/categories/redis/"/>
    
    
      <category term="redis" scheme="yunke.science/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>使用NGINX 1.13.9引入HTTP2服务器推送</title>
    <link href="yunke.science/2018/08/27/http2-server-push/"/>
    <id>yunke.science/2018/08/27/http2-server-push/</id>
    <published>2018-08-27T08:03:16.000Z</published>
    <updated>2018-08-27T08:25:26.683Z</updated>
    
    <content type="html"><![CDATA[<p>2018年2月20日发布的NGINX 1.13.9包括对HTTP/2 服务器推送的支持。<br>在 HTTP/2 规范中定义的服务器推送允许服务器提前将资源推送到远程客户端，预期客户端可能很快就会请求这些资源。 通过这样做，您可以在一个RTT或更多RTT页面加载操作中减少RTT的数量（往返时间 - 请求和响应所需的时间），从而为用户提供更快的响应。</p><p>服务器推送可用于为客户端填充style sheets，图片和呈现网页所需的其他资源。 你只需要注意只推送所需的资源，不要推送客户端可能已经缓存的资源。</p><a id="more"></a><p>@[TOC]</p><h2><span id="配置-http2-服务器推送">配置 <code>HTTP/2</code> 服务器推送</span></h2><p>要将资源与页面加载一起推送，请使用http2_push指令，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    # Ensure that HTTP/2 is enabled for the server        </span><br><span class="line">    listen 443 ssl http2;</span><br><span class="line"></span><br><span class="line">    ssl_certificate ssl/certificate.pem;</span><br><span class="line">    ssl_certificate_key ssl/key.pem;</span><br><span class="line"></span><br><span class="line">    root /var/www/html;</span><br><span class="line"></span><br><span class="line">    # whenever a client requests demo.html, also push</span><br><span class="line">    # /style.css, /image1.jpg and /image2.jpg</span><br><span class="line">    location = /demo.html &#123;</span><br><span class="line">        http2_push /style.css;</span><br><span class="line">        http2_push /image1.jpg;</span><br><span class="line">        http2_push /image2.jpg;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="验证-http2-服务器推送">验证 <code>HTTP/2</code> 服务器推送</span></h2><h3><span id="使用浏览器开发工具">使用浏览器开发工具</span></h3><p>以Google Chrome浏览器为例， 在图中，Chrome开发者工具的“ 网络”选项卡上的“ 启动器”列表示，作为/demo.html请求的一部分，已将多个资源推送到客户端。</p><p><img src="https://cdn-1.wp.nginx.com/wp-content/uploads/2018/02/http2-server-push-chrome-dev-tools.png" alt="image"></p><h3><span id="使用命令行客户端验证nghttp">使用命令行客户端验证(nghttp)</span></h3><p>除了Web浏览器工具之外，您还可以使用nghttp2.org项目中的nghttp命令行客户端来验证服务器推送是否有效。 您可以从 <a href="https://github.com/nghttp2/nghttp2" target="_blank" rel="noopener">GitHub</a> 下载nghttp命令行客户端，或者在可用的情况下安装相应的操作系统软件包。 对于Ubuntu，请使用nghttp2-client软件包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ nghttp -ans https://example.com/demo.html</span><br><span class="line">id  responseEnd requestStart  process code size request path</span><br><span class="line"> 13    +84.25ms       +136us  84.11ms  200  492 /demo.html</span><br><span class="line">  2    +84.33ms *   +84.09ms    246us  200  266 /style.css</span><br><span class="line">  4   +261.94ms *   +84.12ms 177.83ms  200  40K /image2.jpg</span><br><span class="line">  6   +685.95ms *   +84.12ms 601.82ms  200 173K /image1.jpg</span><br></pre></td></tr></table></figure><h2><span id="自动向客户端推送资源">自动向客户端推送资源</span></h2><p>在许多情况下，列出您希望在 <code>NGINX</code> 配置文件中推送的资源是不方便的 - 甚至是不可能的。 出于这个原因，NGINX还支持拦截Link预加载 <code>header</code> 的约定，然后推送这些 <code>header</code> 中标识的资源。 要启用预加载，请在配置中包含 <code>http2_push_preload</code> 指令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    # Ensure that HTTP/2 is enabled for the server        </span><br><span class="line">    listen 443 ssl http2;</span><br><span class="line"></span><br><span class="line">    ssl_certificate ssl/certificate.pem;</span><br><span class="line">    ssl_certificate_key ssl/key.pem;</span><br><span class="line"></span><br><span class="line">    root /var/www/html;</span><br><span class="line"></span><br><span class="line">    # Intercept Link header and initiate requested Pushes</span><br><span class="line">    location = /myapp &#123;</span><br><span class="line">        proxy_pass http://upstream;</span><br><span class="line">        http2_push_preload on;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>例如，当NGINX作为proxy（用于HTTP，FastCGI或其他流量类型）运行时，上游服务器可以将这样的Link头添加到其响应中：</p><p><code>Link: &lt;/style.css&gt;; as=style; rel=preload</code></p><p>NGINX拦截此标头并开始服务器推送<code>/style.css</code> 。 Link头中的路径必须是绝对路径 - 不支持<code>./style.css</code>之类的相对路径。 该路径可以选择包括查询字符串。</p><p>要推送多个对象，您可以提供多个Link标题，或者更好的是，将所有对象包含在以==逗号==分隔的列表中：</p><p><code>Link: &lt;/style.css&gt;; as=style; rel=preload, &lt;/favicon.ico&gt;; as=image; rel=preload</code></p><p>如果您不希望NGINX推送预加载的资源，请将<code>nopush</code>参数添加到标头中：</p><p><code># Resource is not pushed Link: &lt;/nginx.png&gt;; as=image; rel=preload; nopush</code></p><p>启用<code>http2_push_preload</code> ，您还可以通过在NGINX配置中设置响应头来启动预加载服务器推送：</p><p><code>add_header Link &quot;&lt;/style.css&gt;; as=style; rel=preload&quot;;</code> </p><h2><span id="有选择地将资源推送给客户">有选择地将资源推送给客户</span></h2><p><code>HTTP/2</code>规范没有解决确定是否推送资源的挑战。 显然，如果您知道他们可能需要资源并且他们不太可能已经缓存了资源，那么最好将资源推送给客户端。</p><p>一种可能的方法是仅在首次访问网站时将资源推送到客户端。 例如，您可以测试是否存在会话cookie，并有条件地设置Link头，因此仅在会话cookie不存在时才预加载资源。</p><p>假设客户端表现良好并在后续请求中包含cookie，使用以下配置，NGINX仅在每个浏览器会话中将资源推送到客户端一次：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 443 ssl http2 default_server;</span><br><span class="line"></span><br><span class="line">    ssl_certificate ssl/certificate.pem;</span><br><span class="line">    ssl_certificate_key ssl/key.pem;</span><br><span class="line"></span><br><span class="line">    root /var/www/html;</span><br><span class="line">    http2_push_preload on;</span><br><span class="line"></span><br><span class="line">    location = /demo.html &#123;</span><br><span class="line">        add_header Set-Cookie &quot;session=1&quot;;</span><br><span class="line">        add_header Link $resources;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">map $http_cookie $resources &#123;</span><br><span class="line">    &quot;~*session=1&quot; &quot;&quot;;</span><br><span class="line">    default &quot;&lt;/style.css&gt;; as=style; rel=preload, &lt;/image1.jpg&gt;; as=image; rel=preload, </span><br><span class="line">             &lt;/image2.jpg&gt;; as=style; rel=preload&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="测试http2服务器推送的效果">测试HTTP/2服务器推送的效果</span></h2><p>为了衡量服务器推送的效果，我们创建了一个简单的测试页面/demo.html ，它引用了一个单独的样式表/style.css 。 样式表进一步引用了两个图片。 我们使用三种不同的配置测试了页面加载时间：</p><ul><li>顺序GET （无优化） - 浏览器在发现需要时加载资源</li><li>预加载提示 - 预加载提示（ <code>Link headers</code>）包含在第一个响应中，告诉浏览器加载依赖项</li><li>服务器推送 （仅限<code>HTTP/2</code>） - 依赖性被抢先推送到浏览器</li></ul><p><img src="https://cdn-1.wp.nginx.com/wp-content/uploads/2018/02/http2-server-push-test-configurations.png" alt="image"></p><p>我们使用HTTP，HTTPS或<code>HTTP/2</code>对每个配置进行了多次测试运行。 前两个配置适用于所有三个协议，服务器仅适用于 <code>HTTP/2</code>。</p><p>使用Chrome开发人员工具测量行为。 评估和平均每种配置的最常见行为，并将时间与链路的RTT（使用ping测量）相关联，以说明每种方法的机械效果。</p><p><img src="https://cdn-1.wp.nginx.com/wp-content/uploads/2018/02/http2-server-push-testing-results.png" alt="image"></p><h3><span id="结论">结论</span></h3><p>此测试非常简单，以突出预加载提示和服务器推送的机制。 在简单的情况下，服务器推送比预加载提示多1-RTT改进，与未优化的顺序GET请求和相关资源的发现相比，具有更大的改进。</p><p>更现实的用例具有更多变量：多个依赖资源，多个源，甚至通过推送已经缓存或不立即需要的资源来浪费带宽的可能性。 浏览器不一致也会影响性能。 您的结果肯定会因这个简单的测试而异。</p><p>例如，Chrome团队已发布了有关何时部署服务器推送的一些详细建议，并已在更复杂的站点上进行测量，以比较无优化，预加载提示和服务器推送<code>HTTP/2</code>的影响。 对于考虑在生产中部署<code>HTTP/2</code>服务器推送的任何人来说，他们的<code>HTTP/2</code>推送报告的规则值得阅读。</p><p>实用的结论是，如果您可以提前确定需要哪些资源，那么让上游服务器发送预加载提示确实有好处。 推动这些资源的额外好处很小但可以衡量，但可能会导致带宽浪费和所需资源的延迟。 您应该仔细测试和监视任何服务器推送配置。</p><h2><span id="附录http2推送如何工作">附录：<code>HTTP/2</code>推送如何工作？</span></h2><p><code>HTTP/2</code>服务器推送通常用于在客户端请求资源时抢先发送相关资源。 例如，如果客户端请求网页，则服务器可以将依赖的样式表，字体和图像推送到客户端。</p><p>当客户端建立<code>HTTP/2</code>连接时，服务器可以选择通过连接发起一个或多个服务器推送响应。 这些推送发送客户端未明确请求的资源。</p><p>客户端可以拒绝推送（通过发送RST_STREAM帧）或接受它。 客户端将推送的内容存储在与<code>HTTP/2</code>连接相关联的本地“推送缓存”中。</p><p>稍后，当客户端使用已建立的<code>HTTP/2</code>连接请求资源时，它会检查连接的推送缓存，以查找对请求的已完成或正在传输的响应。 它优先使用缓存资源来为资源发出新的<code>HTTP/2</code>请求。</p><p>任何推送的资源都保留在每个连接的推送缓存中，直到（a）使用它或（b）<code>HTTP/2</code>连接关闭：</p><p>如果使用资源，则客户端将获取副本，并删除推送缓存中的条目。 如果资源是可缓存的，则客户端可以将其副本缓存在其HTTP页面缓存中。<br>如果<code>HTTP/2</code>连接因任何原因而关闭，则会删除其本地推送缓存。<br>这有几个含义：</p><p>即使推送的内容更新鲜，浏览器中HTTP页面缓存中的内容也优先于推送缓存中的内容使用。<br><code>HTTP/2</code>连接可以在不同的页面加载之间共享。 在不同页面加载中请求时，可以使用由于一个页面加载而推送的资源。<br>具有凭据的请求使用与没有凭据的请求不同的<code>HTTP/2</code>连接; 例如，如果浏览器对资源发出非凭证请求，则可能找不到使用跨源请求（凭证）推送的资源。<br>您可以在Jake Archibald的<code>HTTP/2</code>推送中查看更详细的问题列表，这比我认为的博客文章更难 。</p><p><code>HTTP/2</code>服务器推送是一项有趣的功能。 确保彻底测试您的<code>HTTP/2</code>服务器推送配置，并准备回退到预加载提示，以防这提供更可预测的，缓存感知行为。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2018年2月20日发布的NGINX 1.13.9包括对HTTP/2 服务器推送的支持。&lt;br&gt;在 HTTP/2 规范中定义的服务器推送允许服务器提前将资源推送到远程客户端，预期客户端可能很快就会请求这些资源。 通过这样做，您可以在一个RTT或更多RTT页面加载操作中减少RTT的数量（往返时间 - 请求和响应所需的时间），从而为用户提供更快的响应。&lt;/p&gt;
&lt;p&gt;服务器推送可用于为客户端填充style sheets，图片和呈现网页所需的其他资源。 你只需要注意只推送所需的资源，不要推送客户端可能已经缓存的资源。&lt;/p&gt;
    
    </summary>
    
      <category term="nginx" scheme="yunke.science/categories/nginx/"/>
    
    
      <category term="nginx" scheme="yunke.science/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title>kubectl概述</title>
    <link href="yunke.science/2018/07/10/kubectl-overview/"/>
    <id>yunke.science/2018/07/10/kubectl-overview/</id>
    <published>2018-07-10T07:14:04.000Z</published>
    <updated>2018-11-06T02:08:32.136Z</updated>
    
    <content type="html"><![CDATA[<p><code>kubectl</code> 是一个命令行界面，用于运行针对Kubernetes集群的命令。 本概述介绍kubectl语法，描述命令操作，并提供常见示例。</p><a id="more"></a><p>@[TOC]</p><h2><span id="语法说明">语法说明</span></h2><p>使用以下语法从终端窗口运行 <code>kubectl</code> 命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl [command] [TYPE] [NAME] [flags]</span><br></pre></td></tr></table></figure><p>其中<code>command</code>，<code>TYPE</code>，<code>NAME</code>和<code>flags</code>是：</p><p>*<code>command</code>：指定要对一个或多个资源执行的操作，例如<code>create</code>，<code>get</code>，<code>describe</code>，<code>delete</code>。</p><p>*<code>TYPE</code>：指定[资源类型]（#resource-types）。 资源类型不区分大小写，您可以指定单数，复数或缩写形式。 例如，以下命令产生相同的输出：</p><pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod pod1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods pod1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po pod1</span></span><br></pre></td></tr></table></figure></code></pre><ul><li><p><code>NAME</code>：指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息，例如<code>$ kubectl get pods</code>。</p><p> 在多个资源上执行操作时，您可以按类型和名称指定每个资源，或指定一个或多个文件：</p><ul><li><p>按类型和名称指定资源:</p><ul><li><p>如果资源类型相同，则对资源进行分组：<code>TYPE1 name1 name2 name&lt;#&gt;</code>。<br><br>示例：<code>$ kubectl get pod example-pod1 example-pod2</code></p></li><li><p>分别指定多种资源类型：<code>TYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE&lt;#&gt;/name&lt;#&gt;</code>。<br><br>示例：<code>$ kubectl get pod/example-pod1 replicationcontroller/example-rc1</code></p></li></ul></li><li><p>使用一个或多个文件指定资源：<code>-f file1 -f file2 -f file&lt;#&gt;</code></p><ul><li>[使用YAML而不是JSON]（/ docs / concepts / configuration / overview /＃general-config-tips），因为YAML往往更加用户友好，特别是对于配置文件。<br>示例：<code>$ kubectl get pod -f ./pod.yaml</code></li></ul></li></ul></li><li><p><code>flags</code>：指定可选标志。 例如，您可以使用<code>-s</code>或<code>--server</code>标志来指定Kubernetes API服务器的地址和端口。<br><br><strong>重要</strong>：您从命令行指定的标志会覆盖默认值和任何相应的环境变量。</p></li></ul><p>如果您需要帮助，只需从终端窗口运行<code>kubectl help</code>即可。</p><h2><span id="操作">操作</span></h2><p>下表包含所有 <code>kubectl</code> 操作的简短描述和一般语法：</p><table><thead><tr><th>操作</th><th>语法</th><th>描述</th></tr></thead><tbody><tr><td><code>annotate</code></td><td>`kubectl annotate (-f FILENAME \</td><td>TYPE NAME \</td><td>TYPE/NAME) KEY_1=VAL_1 … KEY_N=VAL_N [–overwrite] [–all] [–resource-version=version] [flags]`</td><td>添加或更新一个或多个资源的注释。</td></tr><tr><td><code>api-versions</code></td><td><code>kubectl api-versions [flags]</code></td><td>列出可用的API版本。</td></tr><tr><td><code>apply</code></td><td><code>kubectl apply -f FILENAME [flags]</code></td><td>将配置更改应用于文件或stdin中的资源。</td></tr><tr><td><code>attach</code></td><td><code>kubectl attach POD -c CONTAINER [-i] [-t] [flags]</code></td><td>附加到正在运行的容器，以查看输出流或与容器（stdin）进行交互。</td></tr><tr><td><code>autoscale</code></td><td>`kubectl autoscale (-f FILENAME \</td><td>TYPE NAME \</td><td>TYPE/NAME) [–min=MINPODS] –max=MAXPODS [–cpu-percent=CPU] [flags]`</td><td>自动缩放由复制控制器管理的pod集。</td></tr><tr><td><code>cluster-info</code></td><td><code>kubectl cluster-info [flags]</code></td><td>显示有关群集中主服务器和服务的端点信息。</td></tr><tr><td><code>config</code></td><td><code>kubectl config SUBCOMMAND [flags]</code></td><td>修改kubeconfig文件。 有关详细信息，请参阅各个子命令。</td></tr><tr><td><code>create</code></td><td><code>kubectl create -f FILENAME [flags]</code></td><td>从文件或终端输入创建资源</td></tr><tr><td><code>delete</code></td><td>`kubectl delete (-f FILENAME \</td><td>TYPE [NAME \</td><td>/NAME \</td><td>-l label \</td><td>–all]) [flags]`</td><td>从文件，标准输入或指定标签选择器，名称，资源选择器或资源中删除资源。</td></tr><tr><td><code>describe</code></td><td>`kubectl describe (-f FILENAME \</td><td>TYPE [NAME_PREFIX \</td><td>/NAME \</td><td>-l label]) [flags]`</td><td>显示资源详细信息</td></tr><tr><td><code>edit</code></td><td>`kubectl edit (-f FILENAME \</td><td>TYPE NAME \</td><td>TYPE/NAME) [flags]`</td><td>更新一个或多个资源，使用默认的编辑器</td></tr><tr><td><code>exec</code></td><td><code>kubectl exec POD [-c CONTAINER] [-i] [-t] [flags] [-- COMMAND [args...]]</code></td><td>在pod 容器中执行命令</td></tr><tr><td><code>explain</code></td><td><code>kubectl explain [--include-extended-apis=true] [--recursive=false] [flags]</code></td><td>获取各种资源的文档。 例如pod，节点，服务等。</td></tr><tr><td><code>expose</code></td><td>`kubectl expose (-f FILENAME \</td><td>TYPE NAME \</td><td>TYPE/NAME) [–port=port] [–protocol=TCP\</td><td>UDP] [–target-port=number-or-name] [–name=name] [—-external-ip=external-ip-of-service] [–type=type] [flags]`</td><td>将复制控制器，服务或pod公开为新的Kubernetes服务。</td></tr><tr><td><code>get</code></td><td>`kubectl get (-f FILENAME \</td><td>TYPE [NAME \</td><td>/NAME \</td><td>-l label]) [–watch] [–sort-by=FIELD] [[-o \</td><td>–output]=OUTPUT_FORMAT] [flags]`</td><td>列出一个或多个资源</td></tr><tr><td><code>label</code></td><td>`kubectl label (-f FILENAME \</td><td>TYPE NAME \</td><td>TYPE/NAME) KEY_1=VAL_1 … KEY_N=VAL_N [–overwrite] [–all] [–resource-version=version] [flags]`</td><td>增加或者更新资源标签</td></tr><tr><td><code>logs</code></td><td><code>kubectl logs POD [-c CONTAINER] [--follow] [flags]</code></td><td>打印pod 容器的日志</td></tr><tr><td><code>patch</code></td><td>`kubectl patch (-f FILENAME \</td><td>TYPE NAME \</td><td>TYPE/NAME) –patch PATCH [flags]`</td><td>使用策略合并修补程序更新资源的一个或多个字段。</td></tr><tr><td><code>port-forward</code></td><td><code>kubectl port-forward POD [LOCAL_PORT:]REMOTE_PORT [...[LOCAL_PORT_N:]REMOTE_PORT_N] [flags]</code></td><td>转发本地端口到一个pod</td></tr><tr><td><code>proxy</code></td><td><code>kubectl proxy [--port=PORT] [--www=static-dir] [--www-prefix=prefix] [--api-prefix=prefix] [flags]</code></td><td>运行代理到Kubernetes API服务器。</td></tr><tr><td><code>replace</code></td><td><code>kubectl replace -f FILENAME</code></td><td>从文件或标准输入中替换资源。</td></tr><tr><td><code>rolling-update</code></td><td>`kubectl rolling-update OLD_CONTROLLER_NAME ([NEW_CONTROLLER_NAME] –image=NEW_CONTAINER_IMAGE \</td><td>-f NEW_CONTROLLER_SPEC) [flags]`</td><td>通过逐步替换指定的复制控制器及其pod来执行滚动更新。</td></tr><tr><td><code>run</code></td><td><code>kubectl run NAME --image=image [--env=&quot;key=value&quot;] [--port=port] [--replicas=replicas] [--dry-run=bool] [--overrides=inline-json] [flags]</code></td><td>在群集上运行指定的映像。</td></tr><tr><td><code>scale</code></td><td>`kubectl scale (-f FILENAME \</td><td>TYPE NAME \</td><td>TYPE/NAME) –replicas=COUNT [–resource-version=version] [–current-replicas=count] [flags]`</td><td>更新指定的复制控制器的大小。</td></tr><tr><td><code>stop</code></td><td><code>kubectl stop</code></td><td>废弃的命令: 查看 <code>kubectl delete</code>.</td></tr><tr><td><code>version</code></td><td><code>kubectl version [--client] [flags]</code></td><td>更新指定的复制控制器的大小。…</td></tr></tbody></table><p>切记：有关命令操作的更多信息，请参阅[kubectl]（/ docs / user-guide / kubectl /）参考文档。</p><h2><span id="资源类型">资源类型</span></h2><p>下表包含所有受支持的资源类型及其缩写别名的列表：</p><table><thead><tr><th>Resource type</th><th>Abbreviated alias</th></tr></thead><tbody><tr><td><code>apiservices</code></td><td></td></tr><tr><td><code>certificatesigningrequests</code></td><td><code>csr</code></td></tr><tr><td><code>clusters</code></td><td></td></tr><tr><td><code>clusterrolebindings</code></td><td></td></tr><tr><td><code>clusterroles</code></td><td></td></tr><tr><td><code>componentstatuses</code></td><td><code>cs</code></td></tr><tr><td><code>configmaps</code></td><td><code>cm</code></td></tr><tr><td><code>controllerrevisions</code></td><td></td></tr><tr><td><code>cronjobs</code></td><td></td></tr><tr><td><code>customresourcedefinition</code></td><td><code>crd</code></td></tr><tr><td><code>daemonsets</code></td><td><code>ds</code></td></tr><tr><td><code>deployments</code></td><td><code>deploy</code></td></tr><tr><td><code>endpoints</code></td><td><code>ep</code></td></tr><tr><td><code>events</code></td><td><code>ev</code></td></tr><tr><td><code>horizontalpodautoscalers</code></td><td><code>hpa</code></td></tr><tr><td><code>ingresses</code></td><td><code>ing</code></td></tr><tr><td><code>jobs</code></td><td></td></tr><tr><td><code>limitranges</code></td><td><code>limits</code></td></tr><tr><td><code>namespaces</code></td><td><code>ns</code></td></tr><tr><td><code>networkpolicies</code></td><td><code>netpol</code></td></tr><tr><td><code>nodes</code></td><td><code>no</code></td></tr><tr><td><code>persistentvolumeclaims</code></td><td><code>pvc</code></td></tr><tr><td><code>persistentvolumes</code></td><td><code>pv</code></td></tr><tr><td><code>poddisruptionbudget</code></td><td><code>pdb</code></td></tr><tr><td><code>podpreset</code></td><td></td></tr><tr><td><code>pods</code></td><td><code>po</code></td></tr><tr><td><code>podsecuritypolicies</code></td><td><code>psp</code></td></tr><tr><td><code>podtemplates</code></td><td></td></tr><tr><td><code>replicasets</code></td><td><code>rs</code></td></tr><tr><td><code>replicationcontrollers</code></td><td><code>rc</code></td></tr><tr><td><code>resourcequotas</code></td><td><code>quota</code></td></tr><tr><td><code>rolebindings</code></td><td></td></tr><tr><td><code>roles</code></td><td></td></tr><tr><td><code>secrets</code></td><td></td></tr><tr><td><code>serviceaccounts</code></td><td><code>sa</code></td></tr><tr><td><code>services</code></td><td><code>svc</code></td></tr><tr><td><code>statefulsets</code></td><td></td></tr><tr><td><code>storageclasses</code></td><td></td></tr></tbody></table><h2><span id="输出选项">输出选项</span></h2><p>有关如何格式化或排序某些命令的输出的信息，请使用以下部分。 有关哪些命令支持各种输出选项的详细信息，请参阅<code>[kubectl](/docs/user-guide/kubectl/)</code>参考文档。</p><h3><span id="formatting-output">Formatting output</span></h3><p>所有<code>kubectl</code>命令的默认输出格式是人类可读的纯文本格式。 要以特定格式将详细信息输出到终端窗口，可以将<code>-o</code>或<code>-output</code>标志添加到支持的<code>kubectl</code>命令。</p><h4><span id="语法">语法</span></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl [command] [TYPE] [NAME] -o=&lt;output_format&gt;</span><br></pre></td></tr></table></figure><p>根据<code>kubectl</code>操作，支持以下输出格式：</p><table><thead><tr><th>输出格式</th><th>描述</th></tr></thead><tbody><tr><td><code>-o=custom-columns=&lt;spec&gt;</code></td><td>使用逗号分隔的[自定义列]列表（#custom-columns）打印表。</td></tr><tr><td><code>-o=custom-columns-file=&lt;filename&gt;</code></td><td>使用<code>&lt;filename&gt;</code>文件中的[custom columns]（#custom-columns）模板打印表。</td></tr><tr><td><code>-o=json</code></td><td>输出JSON格式的API对象。</td></tr><tr><td><code>-o=jsonpath=&lt;template&gt;</code></td><td>打印[jsonpath]（/docs/reference/kubectl/jsonpath/）表达式中定义的字段。</td></tr><tr><td><code>-o=jsonpath-file=&lt;filename&gt;</code></td><td>Print the fields defined by the <a href="/docs/reference/kubectl/jsonpath/">jsonpath</a> expression in the <code>&lt;filename&gt;</code> file.</td></tr><tr><td><code>-o=name</code></td><td>只输出name列</td></tr><tr><td><code>-o=wide</code></td><td>以纯文本格式输出，包含任何其他信息。 对于pod，包含节点名称。</td></tr><tr><td><code>-o=yaml</code></td><td>yaml格式输出</td></tr></tbody></table><h5><span id="示例">示例</span></h5><p>输出yaml格式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod web-pod-13je7 -o=yaml</span></span><br></pre></td></tr></table></figure><h4><span id="自定义列">自定义列</span></h4><p>要定义自定义列并仅将所需的详细信息输出到表中，可以使用<code>custom-columns</code>选项。 您可以选择内联定义自定义列或使用模板文件： <code>-o=custom-columns=&lt;spec&gt;</code> 或者 <code>-o=custom-columns-file=&lt;filename&gt;</code>.</p><h5><span id="示例">示例</span></h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods &lt;pod-name&gt; -o=custom-columns=NAME:.metadata.name,RSRC:.metadata.resourceVersion</span></span><br></pre></td></tr></table></figure><p>模版文件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods &lt;pod-name&gt; -o=custom-columns-file=template.txt</span></span><br></pre></td></tr></table></figure><p><code>template.txt</code> 文件内容:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NAME          RSRC</span><br><span class="line">metadata.name metadata.resourceVersion</span><br></pre></td></tr></table></figure><p>输出结果:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NAME           RSRC</span><br><span class="line">submit-queue   610995</span><br></pre></td></tr></table></figure><h4><span id="服务器端列">服务器端列</span></h4><p><code>kubectl</code>支持从服务器接收有关对象的特定列信息。<br>这意味着对于任何给定资源，服务器将返回与该资源相关的列和行，以供客户端打印。<br>通过让服务器封装打印细节，这允许在针对同一群集使用的客户端之间提供一致的人类可读输出。</p><p>默认情况下，在<code>kubectl</code> 1.11及更高版本中启用此功能。 要禁用它，请添加<br><code>--server-print=false</code> 标志为<code>kubectl get</code>命令。</p><h5><span id="示例">示例</span></h5><p>要打印有关pod状态的信息，请使用如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods &lt;pod-name&gt; --server-print=false</span><br></pre></td></tr></table></figure><p>如下输出</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NAME       READY     STATUS              RESTARTS   AGE</span><br><span class="line">pod-name   1/1       Running             0          1m</span><br></pre></td></tr></table></figure><h3><span id="排序列表对象">排序列表对象</span></h3><p>要将对象输出到终端窗口中的排序列表，可以将<code>--sort-by</code>标志添加到支持的<code>kubectl</code>命令。 通过使用<code>--sort-by</code>标志指定任何数字或字符串字段来对对象进行排序。</p><h4><span id="语法">语法</span></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl [command] [TYPE] [NAME] --sort-by=&lt;jsonpath_exp&gt;</span><br></pre></td></tr></table></figure><h5><span id="示例">示例</span></h5><p>要打印按名称排序的pod列表，请运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods --sort-by=.metadata.name</span></span><br></pre></td></tr></table></figure><h2><span id="示例常见操作">示例：常见操作</span></h2><p>使用以下示例集来帮助您熟悉运行常用的<code>kubectl</code>操作：</p><p><code>kubectl create</code> - 从文件或stdin创建资源。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// Create a service using the definition in example-service.yaml.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f example-service.yaml</span></span><br><span class="line"></span><br><span class="line">// Create a replication controller using the definition in example-controller.yaml.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f example-controller.yaml</span></span><br><span class="line"></span><br><span class="line">// Create the objects that are defined in any .yaml, .yml, or .json file within the &lt;directory&gt; directory.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f &lt;directory&gt;</span></span><br></pre></td></tr></table></figure><p><code>kubectl get</code> - 列出一个或多个资源。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// List all pods in plain-text output format.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods</span></span><br><span class="line"></span><br><span class="line">// List all pods in plain-text output format and includes additional information (such as node name).</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods -o wide</span></span><br><span class="line"></span><br><span class="line">// List the replication controller with the specified name in plain-text output format. Tip: You can shorten and replace the 'replicationcontroller' resource type with the alias 'rc'.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get replicationcontroller &lt;rc-name&gt;</span></span><br><span class="line"></span><br><span class="line">// List all replication controllers and services together in plain-text output format.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get rc,services</span></span><br><span class="line"></span><br><span class="line">// List all daemon sets, including uninitialized ones, in plain-text output format.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get ds --include-uninitialized</span></span><br><span class="line"></span><br><span class="line">// List all pods running on node server01</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods --field-selector=spec.nodeName=server01</span></span><br><span class="line"></span><br><span class="line">// List all pods in plain-text output format, delegating the details of printing to the server</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods --experimental-server-print</span></span><br></pre></td></tr></table></figure><p><code>kubectl describe</code> - 显示一个或多个资源的详细状态，包括默认情况下未初始化的资源。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// Display the details of the node with name &lt;node-name&gt;.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe nodes &lt;node-name&gt;</span></span><br><span class="line"></span><br><span class="line">// Display the details of the pod with name &lt;pod-name&gt;.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe pods/&lt;pod-name&gt;</span></span><br><span class="line"></span><br><span class="line">// Display the details of all the pods that are managed by the replication controller named &lt;rc-name&gt;.</span><br><span class="line">// Remember: Any pods that are created by the replication controller get prefixed with the name of the replication controller.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe pods &lt;rc-name&gt;</span></span><br><span class="line"></span><br><span class="line">// Describe all pods, not including uninitialized ones</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe pods --include-uninitialized=<span class="literal">false</span></span></span><br></pre></td></tr></table></figure><p><code>kubectl get</code> 命令通常用于检索一个或多个相同资源类型的资源。 它具有丰富的标志，允许使用<code>-o</code>或<code>--output</code>标志自定义输出格式，也可以指定<code>-w</code>或<code>--watch</code>标志以开始观察特定的更新。</p><p><code>kubectl describe</code> 命令更侧重于描述许多指定资源的相关方面。 它可以调用对API服务器的多个API调用来为用户构建视图。 例如，<code>kubectl describe node</code>命令不仅检索有关节点的信息，还检索摘要在其上运行的pod，为节点生成的事件等。</p><p><code>kubectl delete</code> - 从文件，标准输入或指定标签选择器，名称，资源选择器或资源中删除资源。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// Delete a pod using the type and name specified in the pod.yaml file.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete -f pod.yaml</span></span><br><span class="line"></span><br><span class="line">// Delete all the pods and services that have the label name=&lt;label-name&gt;.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete pods,services -l name=&lt;label-name&gt;</span></span><br><span class="line"></span><br><span class="line">// Delete all the pods and services that have the label name=&lt;label-name&gt;, including uninitialized ones.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete pods,services -l name=&lt;label-name&gt; --include-uninitialized</span></span><br><span class="line"></span><br><span class="line">// Delete all pods, including uninitialized ones.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete pods --all</span></span><br></pre></td></tr></table></figure><p><code>kubectl exec</code> - 对pod中的容器执行命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// Get output from running 'date' from pod &lt;pod-name&gt;. By default, output is from the first container.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> &lt;pod-name&gt; date</span></span><br><span class="line"></span><br><span class="line">// Get output from running 'date' in container &lt;container-name&gt; of pod &lt;pod-name&gt;.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> &lt;pod-name&gt; -c &lt;container-name&gt; date</span></span><br><span class="line"></span><br><span class="line">// Get an interactive TTY and run /bin/bash from pod &lt;pod-name&gt;. By default, output is from the first container.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> -ti &lt;pod-name&gt; /bin/bash</span></span><br></pre></td></tr></table></figure><p><code>kubectl logs</code> - 在容器中打印容器的日志。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Return a snapshot of the logs from pod &lt;pod-name&gt;.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl logs &lt;pod-name&gt;</span></span><br><span class="line"></span><br><span class="line">// Start streaming the logs from pod &lt;pod-name&gt;. This is similar to the 'tail -f' Linux command.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl logs -f &lt;pod-name&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;kubectl&lt;/code&gt; 是一个命令行界面，用于运行针对Kubernetes集群的命令。 本概述介绍kubectl语法，描述命令操作，并提供常见示例。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kube-router 用户指南</title>
    <link href="yunke.science/2018/06/07/kuberouter-guide/"/>
    <id>yunke.science/2018/06/07/kuberouter-guide/</id>
    <published>2018-06-07T07:47:37.000Z</published>
    <updated>2018-06-07T07:50:56.260Z</updated>
    
    <content type="html"><![CDATA[<p>Kube-router是Kubernetes网络的一站式解决方案，旨在提供简单操作和高性能。</p><a id="more"></a><p>@[TOC]</p><h2><span id="部署">部署</span></h2><p>根据您要使用的kube-router的功能，可能会有多个部署选项。 您可以使用标志 <code>--run-firewall, --run-router, --run-service-proxy</code> 有选择地启用kube-router所需的功能。</p><p>您也可以选择运行kube-router作为在每个群集节点上运行的代理。 也可以通过守护进程在每个节点上运行kube-router pod。</p><h2><span id="命令行选项">命令行选项</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">Usage of kube-router:</span><br><span class="line">      --advertise-cluster-ip             Add Cluster IP of the service to the RIB so that it gets advertises to the BGP peers.</span><br><span class="line">      --advertise-external-ip            Add External IP of service to the RIB so that it gets advertised to the BGP peers.</span><br><span class="line">      --advertise-loadbalancer-ip        Add LoadbBalancer IP of service status as set by the LB provider to the RIB so that it gets advertised to the BGP peers.</span><br><span class="line">      --advertise-pod-cidr               Add Node&apos;s POD cidr to the RIB so that it gets advertised to the BGP peers. (default true)</span><br><span class="line">      --bgp-graceful-restart             Enables the BGP Graceful Restart capability so that routes are preserved on unexpected restarts</span><br><span class="line">      --cleanup-config                   Cleanup iptables rules, ipvs, ipset configuration and exit.</span><br><span class="line">      --cluster-asn uint                 ASN number under which cluster nodes will run iBGP.</span><br><span class="line">      --cluster-cidr string              CIDR range of pods in the cluster. It is used to identify traffic originating from and destinated to pods.</span><br><span class="line">      --enable-cni                       Enable CNI plugin. Disable if you want to use kube-router features alongside another CNI plugin. (default true)</span><br><span class="line">      --enable-ibgp                      Enables peering with nodes with the same ASN, if disabled will only peer with external BGP peers (default true)</span><br><span class="line">      --enable-overlay                   When enable-overlay set to true, IP-in-IP tunneling is used for pod-to-pod networking across nodes in different subnets. When set to false no tunneling is used and routing infrastrcture is expected to route traffic for pod-to-pod networking across nodes in different subnets (default true)</span><br><span class="line">      --enable-pod-egress                SNAT traffic from Pods to destinations outside the cluster. (default true)</span><br><span class="line">      --enable-pprof                     Enables pprof for debugging performance and memory leak issues.</span><br><span class="line">      --hairpin-mode                     为每个服务端点添加iptable规则以支持 hairpin 流量。</span><br><span class="line">      --health-port uint16               Health check port, 0 = Disabled (default 20244)</span><br><span class="line">  -h, --help                             Print usage information.</span><br><span class="line">      --hostname-override string         Overrides the NodeName of the node. Set this if kube-router is unable to determine your NodeName automatically.</span><br><span class="line">      --iptables-sync-period duration    The delay between iptables rule synchronizations (e.g. &apos;5s&apos;, &apos;1m&apos;). Must be greater than 0. (default 5m0s)</span><br><span class="line">      --ipvs-sync-period duration        The delay between ipvs config synchronizations (e.g. &apos;5s&apos;, &apos;1m&apos;, &apos;2h22m&apos;). Must be greater than 0. (default 5m0s)</span><br><span class="line">      --kubeconfig string                Path to kubeconfig file with authorization information (the master location is set by the master flag).</span><br><span class="line">      --masquerade-all                   SNAT all traffic to cluster IP/node port.</span><br><span class="line">      --master string                    The address of the Kubernetes API server (overrides any value in kubeconfig).</span><br><span class="line">      --metrics-path string              Prometheus metrics path (default &quot;/metrics&quot;)</span><br><span class="line">      --metrics-port uint16              Prometheus metrics port, (Default 0, Disabled)</span><br><span class="line">      --nodeport-bindon-all-ip           For service of NodePort type create IPVS service that listens on all IP&apos;s of the node.</span><br><span class="line">      --nodes-full-mesh                  Each node in the cluster will setup BGP peering with rest of the nodes. (default true)</span><br><span class="line">      --peer-router-asns uints           ASN numbers of the BGP peer to which cluster nodes will advertise cluster ip and node&apos;s pod cidr. (default [])</span><br><span class="line">      --peer-router-ips ipSlice          The ip address of the external router to which all nodes will peer and advertise the cluster ip and pod cidr&apos;s. (default [])</span><br><span class="line">      --peer-router-multihop-ttl uint8   Enable eBGP multihop supports -- sets multihop-ttl. (Relevant only if ttl &gt;= 2)</span><br><span class="line">      --peer-router-passwords strings    Password for authenticating against the BGP peer defined with &quot;--peer-router-ips&quot;.</span><br><span class="line">      --routes-sync-period duration      The delay between route updates and advertisements (e.g. &apos;5s&apos;, &apos;1m&apos;, &apos;2h22m&apos;). Must be greater than 0. (default 5m0s)</span><br><span class="line">      --run-firewall                     Enables Network Policy -- sets up iptables to provide ingress firewall for pods. (default true)</span><br><span class="line">      --run-router                       Enables Pod Networking -- Advertises and learns the routes to Pods via iBGP. (default true)</span><br><span class="line">      --run-service-proxy                Enables Service Proxy -- sets up IPVS for Kubernetes Services. (default true)</span><br><span class="line">  -v, --v string                         log level for V logs (default &quot;0&quot;)</span><br><span class="line">  -V, --version                          Print version information.</span><br></pre></td></tr></table></figure><h2><span id="前提条件">前提条件</span></h2><ul><li><p>Kube-router需要访问kubernetes API服务器以获取有关Pod，服务，端点，网络策略等的信息。它需要的最少信息是关于在何处访问kubernetes - API服务器的详细信息。此信息可以通过<code>kube-router --master=http://192.168.1.99:8080/</code>或<code>kube-router --kubeconfig=&lt;path to kubeconfig file&gt;</code>传递。</p></li><li><p>如果您在该节点上运行kube-router作为代理，则必须在每个节点上安装ipset软件包（当以守护程序集运行时，容器映像使用ipset预先打包）</p></li><li><p>如果您选择使用kube-router作为pod-to-pod网络连接，则需要将Kubernetes控制器管理器配置为通过传递<code>--allocate-node-cidrs=true</code>标志并提供一个cluster-cidr来分配pod CIDR（即通过传递 - 例如，<code>--cluster-cidr=10.1.0.0/16</code>）</p></li><li><p>如果您选择将kube-router作为守护进程运行，则kube-apiserver和kubelet必须使用<code>--allow-privileged = true</code>选项运行</p></li><li><p>如果您选择使用kube-router作为<code>pod-to-pod</code>网络连接，则必须将Kubernetes集群配置为使用CNI网络插件。在每个节点上CNI conf文件预计会作为<code>/etc/cni/net.d/10-kuberouter.conf</code>存在<code>.bridge CNI</code>插件和<code>IPAM</code>的主机本地应该被使用。示例conf文件，可以下载<code>wget -O /etc/cni/net.d/10-kuberouter.conf https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/cni/10-kuberouter.conf</code> .</p></li></ul><h2><span id="以k8s-daemonset-运行">以k8s daemonset 运行</span></h2><p>这是部署kube-router的最快捷的方式。<br><code>kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kube-router-all-service-daemonset.yaml</code>  </p><p>以上将自动在每个节点上运行kube-router作为pod。 您可以根据需要更改守护进程定义中的参数以满足您的需求。 有些示例可以在<a href="https://github.com/cloudnativelabs/kube-router/tree/master/daemonset中找到，并使用不同的参数来选择kube-router应该运行的服务集。" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-router/tree/master/daemonset中找到，并使用不同的参数来选择kube-router应该运行的服务集。</a></p><h2><span id="以代理agent运行">以代理agent运行</span></h2><p>您可以选择运行kube-router作为每个节点上运行的代理。 例如，如果您只想让kube-router为pod提供入口防火墙，那么您可以启动kube-router as</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kube-router --master=http://192.168.1.99:8080/ --run-firewall=true --run-service-proxy=false --run-router=false</span><br></pre></td></tr></table></figure><h2><span id="清除配置">清除配置</span></h2><p>请删除kube-router daemonset，然后通过运行以下命令清除节点上的kube-router完成的所有配置（对ipvs，iptables，ipset，ip routes等）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --privileged --net=host cloudnativelabs/kube-router --cleanup-config</span><br></pre></td></tr></table></figure><h2><span id="尝试使用kube-router作为kube-proxy的替代品">尝试使用kube-router作为kube-proxy的替代品</span></h2><p>如果您有一个正在使用的kube-proxy，并且想尝试使用kube-router作为服务代理，那么您可以这样做</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kube-proxy --cleanup-iptables</span><br></pre></td></tr></table></figure><p>然后执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kube-router --master=http://192.168.1.99:8080/ --run-service-proxy=true --run-firewall=false --run-router=false</span><br></pre></td></tr></table></figure><p>并且如果您想要移回kube-proxy，则通过运行来清理由kube-router完成的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kube-router --cleanup-config</span><br></pre></td></tr></table></figure><p>并使用您拥有的配置运行kube-proxy。</p><h2><span id="hairpin-模式">Hairpin 模式</span></h2><p>kubeadm-kuberouter 默认不支持从service后面的Pod到其自己的ClusterIP的端口。<br>即，如果在一个pod中，想通过service访问pod中的服务，网络默认是不通。  </p><p>这里需要设置kube-router 的 <a href="https://github.com/cloudnativelabs/kube-router/blob/master/docs/user-guide.md#hairpin-mode" target="_blank" rel="noopener">hairpin-mode</a>。  </p><p>两步操作：</p><ol><li>对于每个节点上的所有veth接口，hairpin_mode sysctl选项必须设置为1。<br>这可以通过选项添加 <code>&quot;hairpinMode&quot;:true</code> 到CNI配置并重新引导所有群集节点（如果它们已经运行kubernetes）来完成。</li></ol><p>在安装kube-router之前，如果还没有执行<code>KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml</code> 操作如下：<br>下载这个文件到本地：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># wget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml</span><br><span class="line"># vi kubeadm-kuberouter.yaml</span><br><span class="line">在`ConfigMap` `kube-router-cfg` `cni-conf.json` 中添加 `&quot;hairpinMode&quot;:true,`, 如下图</span><br><span class="line">  cni-conf.json: |</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;:&quot;kubernetes&quot;,</span><br><span class="line">      &quot;type&quot;:&quot;bridge&quot;,</span><br><span class="line">      &quot;bridge&quot;:&quot;kube-bridge&quot;,</span><br><span class="line">      &quot;isDefaultGateway&quot;:true,</span><br><span class="line">      &quot;hairpinMode&quot;:true,</span><br><span class="line">      &quot;ipam&quot;: &#123;</span><br><span class="line">        &quot;type&quot;:&quot;host-local&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">添加完成后，引用这个文件</span><br><span class="line">KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f kubeadm-kuberouter.yaml</span><br></pre></td></tr></table></figure><ol start="2"><li>对于需要启用的服务，单独添加 <code>annotations:kube-router.io/service.hairpin: &quot;&quot;</code> 。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: ServiceName</span><br><span class="line">  annotations:</span><br><span class="line">    kube-router.io/service.hairpin: &quot;&quot;</span><br></pre></td></tr></table></figure><p>如果要默认对所有服务启用，需要修改<code>kubeadm-kuberouter.yaml</code> ，<code>kube-router</code> 添加启动参数 <code>--hairpin-mode=true</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">- name: kube-router</span><br><span class="line">  image: cloudnativelabs/kube-router</span><br><span class="line">  imagePullPolicy: Always</span><br><span class="line">  args:</span><br><span class="line">  - --run-router=true</span><br><span class="line">  - --run-firewall=true</span><br><span class="line">  - --run-service-proxy=false</span><br><span class="line">  - --run-service-proxy=false</span><br><span class="line">  - --hairpin-mode=true</span><br></pre></td></tr></table></figure><h2><span id="直接服务端返回">直接服务端返回</span></h2><p>请阅读以下博客，了解如何结合使用DSR和–advertise-external-ip构建高度可扩展和可用的入口。<a href="https://cloudnativelabs.github.io/post/2017-11-01-kube-high-available-ingress/" target="_blank" rel="noopener">https://cloudnativelabs.github.io/post/2017-11-01-kube-high-available-ingress/</a></p><p>您可以为每个服务启用DSR（直接服务器返回）功能。 启用的服务端点将通过传递服务代理直接响应客户端。 启用DSR时，Kube-router将使用LVS的隧道模式来实现此目的。</p><p>要启用DSR，您需要使用kube-router.io/service.dsr=tunnel注释来注释服务。 对于例如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl annotate service my-service &quot;kube-router.io/service.dsr=tunnel&quot;</span><br></pre></td></tr></table></figure><p>在目前的实施中，当在服务上应用注释时，DSR将仅适用于外部IP。</p><p>同样，当使用DSR时，当前实现不支持端口重映射。 所以你需要为服务使用相同的端口和目标端口</p><p>您需要在kube-router守护程序集清单中启用<code>hostIPC：true</code>和<code>hostPID：true</code>。 此外，主路径<code>/var/run/docker.sock</code>必须作为<code>kube-router</code>的卷挂载。</p><p>上述更改需要kube-router输入pod namespeace并在pod中创建ipip隧道并将外部IP分配给VIP。</p><p>对于例如清单，请查看启用DSR要求的<a href="https://github.com/cloudnativelabs/kube-router/blob/master/daemonset/kubeadm-kuberouter-all-features-dsr.yaml" target="_blank" rel="noopener">清单</a>。</p><h2><span id="负载平衡调度算法">负载平衡调度算法</span></h2><p>Kube-router使用LVS作为服务代理。 LVS支持丰富的调度算法。 您可以注释该服务以选择其中一个调度变量。 未注释服务时默认情况下选择循环调度程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">For least connection scheduling use:</span><br><span class="line">kubectl annotate service my-service &quot;kube-router.io/service.scheduler=lc&quot;</span><br><span class="line"></span><br><span class="line">For round-robin scheduling use:</span><br><span class="line">kubectl annotate service my-service &quot;kube-router.io/service.scheduler=rr&quot;</span><br><span class="line"></span><br><span class="line">For source hashing scheduling use:</span><br><span class="line">kubectl annotate service my-service &quot;kube-router.io/service.scheduler=sh&quot;</span><br><span class="line"></span><br><span class="line">For destination hashing scheduling use:</span><br><span class="line">kubectl annotate service my-service &quot;kube-router.io/service.scheduler=dh&quot;</span><br></pre></td></tr></table></figure><h2><span id="负载均衡器-ips">负载均衡器 IPS</span></h2><p>如果您还想通告负载均衡器设置的IP（<code>status.loadBalancer.ingress IPs</code>），例如 当与MetalLb一起使用时，添加<code>--advertise-loadbalancer-ip</code>标志（默认为false）。</p><p>要有选择地禁用每个服务的这种行为，可以使用kube-router.io/service.skiplbips注释作为例如：<code>$ kubectl annotate service my-external-service &quot;kube-router.io/service.skiplbips=true&quot;</code></p><p>具体而言，除非服务按照上述注释，否则<code>--advertise-loadbalancer-ip</code>标志将使服务的Ingress IP由LoadBalancer设置为：</p><ul><li>可以本地添加到节点的kube-dummy（如果网络接口）</li><li>被通告给BGP对等体</li></ul><p>FYI Above已成功通过ARP模式的MetalLB测试。</p><h2><span id="hostport-支持">HostPort 支持</span></h2><p>如果您想使用HostPort功能，清单中需要进行更改。</p><ul><li><p>默认情况下，kube-router假定CNI conf文件为<code>/etc/cni/net.d/10-kuberouter.conf</code>。 将环境变量<code>KUBE_ROUTER_CNI_CONF_FILE</code>添加到<code>kube-router</code>清单并将其设置为<code>/etc/cni/net.d/10-kuberouter.conflist</code></p></li><li><p>使用支持<code>portmap</code>的CNI config修改<code>kube-router-cfg</code> ConfigMap作为附加插件</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">     &quot;cniVersion&quot;:&quot;0.3.0&quot;,</span><br><span class="line">     &quot;name&quot;:&quot;mynet&quot;,</span><br><span class="line">     &quot;plugins&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">           &quot;name&quot;:&quot;kubernetes&quot;,</span><br><span class="line">           &quot;type&quot;:&quot;bridge&quot;,</span><br><span class="line">           &quot;bridge&quot;:&quot;kube-bridge&quot;,</span><br><span class="line">           &quot;isDefaultGateway&quot;:true,</span><br><span class="line">           &quot;ipam&quot;:&#123;</span><br><span class="line">              &quot;type&quot;:&quot;host-local&quot;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">           &quot;type&quot;:&quot;portmap&quot;,</span><br><span class="line">           &quot;capabilities&quot;:&#123;</span><br><span class="line">              &quot;snat&quot;:true,</span><br><span class="line">              &quot;portMappings&quot;:true</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line">     ]</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li>更新init容器命令以创建/etc/cni/net.d/10-kuberouter.conflist文件</li><li>重新启动运行容器</li></ul><p>对于例如清单，请查看清单，并对HostPort功能进行必要的更改。</p><h2><span id="bgp-configuration">BGP configuration</span></h2><p><a href="https://github.com/cloudnativelabs/kube-router/blob/master/docs/user-guide.md#hairpin-mode" target="_blank" rel="noopener">Configuring BGP Peers</a></p><h2><span id="metrics">Metrics</span></h2><p><a href="https://github.com/cloudnativelabs/kube-router/blob/master/docs/user-guide.md#hairpin-mode" target="_blank" rel="noopener">Configure metrics gathering</a></p><h2><span id="参考">参考</span></h2><p><a href="https://github.com/cloudnativelabs/kube-router/blob/master/docs/user-guide.md#hairpin-mode" target="_blank" rel="noopener">User Guide</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kube-router是Kubernetes网络的一站式解决方案，旨在提供简单操作和高性能。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes 网络策略简单配置</title>
    <link href="yunke.science/2018/05/18/k8sNetworkPolicy/"/>
    <id>yunke.science/2018/05/18/k8sNetworkPolicy/</id>
    <published>2018-05-18T11:21:26.000Z</published>
    <updated>2018-05-18T11:30:44.656Z</updated>
    
    <content type="html"><![CDATA[<p>网络策略是配置允许群组与其他网络终端进行通信的规范。</p><p><code>NetworkPolicy</code> 使用 <code>labels</code> 选择 <code>pod</code> 并定义规则，以指定允许所选<code>pod</code>允许哪些流量。</p><a id="more"></a><p>@[TOC]</p><p><strong>实现功能：</strong>   </p><p>隔离namespace，不同namespace pod禁止访问，仅允许访问相同 namespace 中的 pod 。</p><p><strong>配置步骤：</strong>  </p><p>创建一个dev的namespace，并添加一个labels namespace: dev</p><ul><li>创建一个新的namespace：dev</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1  </span><br><span class="line">kind: Namespace  </span><br><span class="line">metadata:  </span><br><span class="line">   name: dev  </span><br><span class="line">   labels:  </span><br><span class="line">     namespace: dev</span><br></pre></td></tr></table></figure><ul><li>在 default namespace 和 dev namespace 分别创建三个pod，查看pod ip</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@docker01 k8srpm]# kubectl -n dev get pod -o wide</span><br><span class="line">NAME                   READY     STATUS    RESTARTS   AGE       IP           NODE</span><br><span class="line">myip-59dfc4d87-6ndlr   1/1       Running   0          24s       10.42.1.28   docker03</span><br><span class="line">myip-59dfc4d87-ksnbd   1/1       Running   0          24s       10.42.2.6    docker02</span><br><span class="line">myip-59dfc4d87-rfphz   1/1       Running   0          24s       10.42.0.7    docker01</span><br><span class="line">[root@docker01 k8srpm]# kubectl -n default get pod -o wide   </span><br><span class="line">NAME                   READY     STATUS    RESTARTS   AGE       IP           NODE</span><br><span class="line">myip-59dfc4d87-8h2d6   1/1       Running   0          36d       10.42.1.22   docker03</span><br><span class="line">myip-59dfc4d87-dfplq   1/1       Running   0          36d       10.42.2.5    docker02</span><br><span class="line">myip-59dfc4d87-zqr2w   1/1       Running   0          36d       10.42.1.21   docker03</span><br></pre></td></tr></table></figure><ul><li>进行互ping测试，互相可以联通</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@docker01 k8srpm]# kubectl -n dev exec -it myip-59dfc4d87-6ndlr ping 10.42.1.22      </span><br><span class="line">PING 10.42.1.22 (10.42.1.22): 56 data bytes</span><br><span class="line">64 bytes from 10.42.1.22: seq=0 ttl=64 time=0.167 ms</span><br><span class="line">^C</span><br><span class="line">--- 10.42.1.22 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.167/0.167/0.167 ms</span><br><span class="line">[root@docker01 k8srpm]# kubectl -n default exec -it myip-59dfc4d87-8h2d6 ping 10.42.1.28</span><br><span class="line">PING 10.42.1.28 (10.42.1.28): 56 data bytes</span><br><span class="line">64 bytes from 10.42.1.28: seq=0 ttl=64 time=0.151 ms</span><br><span class="line">64 bytes from 10.42.1.28: seq=1 ttl=64 time=0.129 ms</span><br></pre></td></tr></table></figure><ul><li>在dev namespace添加 NetworkPolicy，先deny all，然后放开dev namespace中pod访问</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: dev-env-deny-all</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: dev-isolate-namespace</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - namespaceSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          namespace: dev</span><br></pre></td></tr></table></figure><ul><li>进行 ping 测试，可以看到，dev 可以ping通default ，default 无法ping 通 dev 。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@docker01 k8srpm]# kubectl -n dev exec -it myip-59dfc4d87-6ndlr ping 10.42.1.22 </span><br><span class="line">PING 10.42.1.22 (10.42.1.22): 56 data bytes</span><br><span class="line">64 bytes from 10.42.1.22: seq=0 ttl=64 time=0.201 ms</span><br><span class="line">64 bytes from 10.42.1.22: seq=1 ttl=64 time=0.111 ms</span><br><span class="line">^C</span><br><span class="line">--- 10.42.1.22 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.111/0.156/0.201 ms</span><br><span class="line">[root@docker01 k8srpm]# kubectl -n default exec -it myip-59dfc4d87-8h2d6 ping 10.42.1.28</span><br><span class="line">PING 10.42.1.28 (10.42.1.28): 56 data bytes</span><br><span class="line">--- 10.42.1.28 ping statistics ---</span><br><span class="line">4 packets transmitted, 0 packets received, 100% packet loss</span><br><span class="line">command terminated with exit code 1</span><br></pre></td></tr></table></figure><ul><li>验证成功。</li></ul><hr><p><strong>kubernetes网络策略配置说明：</strong>  </p><p>网络策略由网络插件实现，因此您必须使用支持NetworkPolicy的网络解决方案 - 只创建资源而没有控制器实施它，策略将无效。</p><p>pod 通过一个 NetworkPolicy 来隔离网络。 一旦 namespace 中的任何NetworkPolicy选择了特定的Pod，该Pod将拒绝任何NetworkPolicy所不允许的连接。 （命名空间中未被任何NetworkPolicy选择的其他pod将继续接受所有流量。）</p><p>V1.10 新增了 ipBlock 过滤，和 egress 出口流量过滤 规则。</p><p>V1.10 NetworkPolicy 示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: test-network-policy</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      role: db</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br><span class="line">  - Egress</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - ipBlock:</span><br><span class="line">        cidr: 172.17.0.0/16</span><br><span class="line">        except:</span><br><span class="line">        - 172.17.1.0/24</span><br><span class="line">    - namespaceSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          project: myproject</span><br><span class="line">    - podSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          role: frontend</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 6379</span><br><span class="line">  egress:</span><br><span class="line">  - to:</span><br><span class="line">    - ipBlock:</span><br><span class="line">        cidr: 10.0.0.0/24</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 5978</span><br></pre></td></tr></table></figure><p>参考：</p><ol><li><a href="https://k8smeetup.github.io/docs/concepts/services-networking/network-policies/" target="_blank" rel="noopener">https://k8smeetup.github.io/docs/concepts/services-networking/network-policies/</a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-traffic" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-traffic</a></li><li><a href="https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/" target="_blank" rel="noopener">https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;网络策略是配置允许群组与其他网络终端进行通信的规范。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;NetworkPolicy&lt;/code&gt; 使用 &lt;code&gt;labels&lt;/code&gt; 选择 &lt;code&gt;pod&lt;/code&gt; 并定义规则，以指定允许所选&lt;code&gt;pod&lt;/code&gt;允许哪些流量。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>微服务架构的优势</title>
    <link href="yunke.science/2018/05/17/microsrv-advant/"/>
    <id>yunke.science/2018/05/17/microsrv-advant/</id>
    <published>2018-05-17T04:06:06.000Z</published>
    <updated>2018-05-17T04:10:21.612Z</updated>
    
    <content type="html"><![CDATA[<p>微服务是当前互联网产品的一个技术架构趋势，近两年伴随着Docker容器计算的发展和普及,微服务架构在业界逐渐落地,那么驱动企业进行微服务化架构改造的根本推动力是什么?如何在企业内部实施微服务架构?实施微服务契构需要哪些技术框架和基础设施?这些都是企业技术人员需要了解并关心的问题,也是要具体讨论的问题。</p><a id="more"></a><p><strong>微服务作为对应单体应用出现的概念,其架构通常有哪些优势呢?</strong></p><ul><li><p><strong>项目工程简洁</strong>。一个复杂的产品功能集合,代码仓库规模往往随着业务复杂度的增加而线性增加。对于单体应用来说,代码的堆积意味着工程规模的迅速膨胀。而对于微服务来说,因为每个微服务承担的职责小而且单一,所以工程规模简洁可控。</p></li><li><p><strong>升级代价小</strong>。当一个产品所有的功能都集中在同一个应用时,对程序的升级会带来两方面的影响:一是项目工程过大造成的应用启动时间过久,我们见过有些产品一个应用实例的启动时间需要半个小时以上,这对业务来说显然不可行。二是每次hotfix都需要对整个应用重启,引起业务的不稳定,而微服务架构恰恰相反,每个服务独立升级,对业务整体的影响极小。</p></li><li><p><strong>扩展性好</strong>。对于互联网产品来说,产品迭代速度很重要。对于一个庞大的单体应用来说,牵一发而动全身,无论对产品功能的扩展性还是性能的扩展性来说,都是一个很大的挑战。通过微服务化,把业务中扩展性差的部分独立出去,不同的业务类型采用不同扩展方案,可以提升业务整体的可扩展性。</p></li><li><p><strong>稳定性好</strong>。单体应用的稳定性差主要体现在功能间的隔离性,对于单体应用而言,所有的功能都在同一个进程空间里,这意味着任何一个功能的bug可能会造成应用整个崩溃。微服务的好处是可以实现进程级别的隔离,单个服务异常很少会造成全局故障。</p></li><li><p><strong>人员变更影响小</strong>。项目中人员更迭并不少见,单体应用的交接要求被交接人员必须对整个项目非常熟悉,才有可能消化变更人员带来的负面影响,否则人员离职或转岗会影响项目的正常进度。实施微服务架构的团队往往同时也遵循“2 pizza”原则的组织架构,团队小而精,人员变更影响小。</p></li><li><p><strong>技术栈丰富</strong>。单体应用因为都跑在同一个进程里,所以项目的整体技术栈就被这个进程锁死了,比如说一个Java单体应用,无论业界的其他编程语言和开发框架如何发展,我们也无法利用起来,无法实现技术反哺业务。而微服务可以采用一些通用的服务间通信方式(HTTP等)去集成,使得服务的实现方式不局限于某个技术栈，适合用最合适的技术实现功能。</p></li><li><p><strong>开发效率高</strong>。主要体现在为服务架构下项目的学习曲线平滑，因为工程规模小，所以开发人员能很快做到对项目有一个大而全的认识。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;微服务是当前互联网产品的一个技术架构趋势，近两年伴随着Docker容器计算的发展和普及,微服务架构在业界逐渐落地,那么驱动企业进行微服务化架构改造的根本推动力是什么?如何在企业内部实施微服务架构?实施微服务契构需要哪些技术框架和基础设施?这些都是企业技术人员需要了解并关心的问题,也是要具体讨论的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="yunke.science/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="架构" scheme="yunke.science/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>分布式服务架构</title>
    <link href="yunke.science/2018/05/16/architec-distri/"/>
    <id>yunke.science/2018/05/16/architec-distri/</id>
    <published>2018-05-16T05:11:57.000Z</published>
    <updated>2018-05-16T07:08:37.056Z</updated>
    
    <content type="html"><![CDATA[<p>一般而言,企业经过初创期和成长期两个阶段的发展,就基本确定了业务的发展方向,接下来只要面对竞争对手的跟随和大量用户访问请求的问题。这些企业也会提供各种不同的子产品模块功能来满足业务的多样性发展,比如产品会设计不同的产品功能体系,运营人员会设计不同的运营活动,客服人员会接到不同的用户反馈等。</p><a id="more"></a><p>@[TOC]</p><p>这些需求叠加在一起,会导致整个业务越来越复杂,有些系统变得不再具有维护性,无法满足可用性的需求,只要一出现流量高峰,系统必定宕机,所以很多公司面临重构架构设计,甚至推翻重来,比如京东大部分的用户业务从.NET转到Java语言的重构,淘宝从PHP转到Java,升级2.0,再到3.0的架构转变,可以看到,这个时间点的转型或重构可能不会到生死攸关的地步,但其成本,是非常高的。因此,如何在一开始就能做好这种转变的预见性,对契构设计有相当大的挑战,本节先给出几点供读者来参考。</p><h2><span id="业务需求">业务需求</span></h2><p>通过云服务通常可以解决很多架构层面的问题,比如对象存储系统解决了文件分布式存储的问题, CDN解决了静态资源访问的性能问题,但实际上随着业务的不断发展,系统访问压力增大,还可能有很多的请求变慢或者超时,应用服务器或数据库服务的压力波动较大,只要不停地上线新业务,技术债就会越来越明显,业务的迭代也越来越跟不上产品需求。</p><p>为了解决这些问题,企业往往需要进行各种业务的拆分,把不同的功能模块拆分到不同的服务器上进行独立部署。比如用户模块、商品模块、购物车模块、订单模块和支付模块等,这些模块拆分并独立部署出来后,可以再进一步根据系统的瓶领进行细分。但是进行服务拆分之后,各模块之间的依赖又变得明显起来,比如数据库的建接效、数据的分<br>布式事务、数据库的性能开销等都是急切需要解决的问题。</p><p>同时,随着业务模块的拆分,除了上述的技术问题要解决外,还面临着工程实践的问题,比如在业务的不同分支中,需要保证开发人员、测试人员、运维人员快速地对开发环境、则试环境、预发布环境的搭建和发布。在高速发展的企业中迭代的频率非常高,以网易考拉平台为例,所有系统的日发布次数到达数千次,所以技术人员对效率的要求比较高。当扩大到一个公司多个产品线,整体的运行就要求像现代化工厂一样来运作,需要自动化<br>的平台去解决,纯手工根本无法满足企业的高效运转。</p><h2><span id="弹性扩容">弹性扩容</span></h2><p>随着需求和用户的不断增长,系统会出现波峰和波谷,为了更好地利用资源和成本预算,弹性扩容成了必要需求,在峰值的时候能够根据业务的压力自动扩容,分担流量,在压力低的时候目动缩容,藏少成本或提高资源的利用率,把缩容的资源做离线业务计算。</p><p>也许在过去是简单地通过垂直扩大规模能力来处理更多的需求,或者是购买更强的服务器,这在一定程度上是可行的,但过程很慢开且代价庞大,通过提前准备过多的资源,会导致只根据峰值使用量预测来规划能力值,比如根据服务器的最高计算能力购买硬件予以满足,这是不得已的做法。例如国外的黑色星期五、国内的双11等活动,当天请求非常高,需要足够的资源来满足业务请求,而平时这些服务器的使用率很低,所以,<strong>只有依赖云的弹性才能满足这种业务场景的需求</strong>。</p><h2><span id="服务化">服务化</span></h2><p>不管是互联网还是传统行业转型的企业,基本都是在原有基础业务上发展而来的,不可能把业务停止从零开始,因此,直接对原有系统进行微服务改造比较困难,风险也较大.这时基本上处于一种混合架构期,即新的业务会从头开发,逐步接入到老系统中,一步步替换老系统不满足的地方,通过不断地快速迭代来保证业务的可持续性,同时又保证新业务的快速需求。</p><p>著名的架构大师Martin Fowler从2013年正式提出微服务架构的综述文章。至今,都没有提供统一的最佳实践方案。现在微服务的架构实现方式也各种各样,通常根据应用的类型拆分成不同的微服务来实现,每个服务根据业务的特征采用不同的技术栈进行组合,把每个服务划分成可以独立部署的隔离进程来运行。目前,微服务的基本框架都类似,比如包括服务发现、降级、治理等方面。业务实现微服务的技术细节各不相同,没有统一的实现方案,比如服务发现有自建服务基础设施的,也有依赖第三方开源的,技术人员需要根据自己的场景来做选择。简化的架构模型如图所示。</p><img src="/2018/05/16/architec-distri/Distribute-Service-Architecture.png" title="Distribute-Service-Architecture"><hr><p>显然,这个架构模型只是整个业务服务架构的一部分,实际的系统可能要复杂几十倍。如果业务的迭代速度非常快,同时每个业务之间的依赖从设计、开发、测试、上线到运维都是一个非常庞大的复杂工程,因此,如何高效管理所依赖的服务和系统依赖,诊断并及时对业务反馈响应是对服务架构的考验。</p><p><strong>架构演化发展历程：</strong></p><ol><li><a href="/2018/05/16/architec-initial/">初创期架构</a></li><li><a href="/2018/05/16/architec-growth/">快速成长期架构</a></li><li><a href="/2018/05/16/architec-distri/">分布式服务架构</a></li></ol><p>参考：</p><ol><li>云原生应用架构实践 （网易云基础服务架构团队 著）</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一般而言,企业经过初创期和成长期两个阶段的发展,就基本确定了业务的发展方向,接下来只要面对竞争对手的跟随和大量用户访问请求的问题。这些企业也会提供各种不同的子产品模块功能来满足业务的多样性发展,比如产品会设计不同的产品功能体系,运营人员会设计不同的运营活动,客服人员会接到不同的用户反馈等。&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="yunke.science/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="架构" scheme="yunke.science/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>快速成长期架构</title>
    <link href="yunke.science/2018/05/16/architec-growth/"/>
    <id>yunke.science/2018/05/16/architec-growth/</id>
    <published>2018-05-16T05:08:29.000Z</published>
    <updated>2018-05-16T07:08:40.820Z</updated>
    
    <content type="html"><![CDATA[<p>初创公司随着业务的进一步发展,当DAU达到十万的时候,通常是最关键的时刻,既要保证业务的稳定运行,又要进行产品的快速迭代。到了这个阶段,由于业务模式得到了一定的验证和反馈,有可能会出现很多竞品或友商。一方面,随着风险资本的注入,会依赖更有质量的数据进行发展运营,另一方面,竞品的出现又导致了市场的加速前进.因此,能否在这个阶段保证业务与技术的和谐发展,是考验架构是否足够灵活的指标之一,本节主要说明几点。</p><a id="more"></a><p>@[TOC]</p><h2><span id="前端加速优化">前端加速优化</span></h2><p>首先基于浏览器端应用或者移动端应用,随着请求的不断增加,偶尔会看到Web服务出现性能瓶颈导致请求变慢或者失败,除去服务器本身的配置低外,更有可能由于架构设计或分离的原因,大量的Web并发请求被堵塞或者变慢,原因无非是服务器的CPU、磁盘、IO、带宽竞争激烈,导致相互影响,这时候我们就需要对架构进行前后端分解,合理配置或转发请求。</p><p>如果是前端的服务请求来不及处理或者有瓶颈,可以将图片、Js、 CSS、 HTML及应用服务相关的静态资源文件存储通过Nginx本地代理或者对象存储服务来进行物理加速,使用不同的域名来转发请求,并通过CDN将静态资源分布式缓存在各个节点实现“就近访问”,主动或被动刷新CDN的缓存来加速前端服务。</p><p>如果是后端的动态请求压力过大或者有热点服务,可以把无状态的后端的服务再进一步水平扩展满足业务分担,有状态需要判断是否能通过垂直扩容来服务,否则只能进行代码、架构设计或者业务规划的调整来优化。</p><p>一般而言,通过将动态请求、静态请求的访问分离(“动静分离”),能有效解决服务器在CPU、磁盘IO、带宽方面的访问压力。当然,这需要在架构设计时采用一些方法来进行调整。</p><h2><span id="水平扩展">水平扩展</span></h2><p>上面提到垂直扩容能解决部分的问题,但由于业务和流量的快速增长及垂直资源有限,不同的应用场景需要依赖不同的策略分流,比如长连接的应用会依赖于4层的网络连接,互联网应用通常采用7层的模式来完成,甚至在游戏场景中,依赖UDP进行通信。</p><p>为了更多地分担服务器的压力和保证业务的高可用,负载均衡技术通常是这个阶段解决问题的一个方法,通过增加多台后端服务器就可以实现分流的功能,分流设计也面临很多原则与技巧,比如分流的路径、权重等。负戴均衡承担的角色也决定了后端的应用架构,比如无状态化设计才能实现水平扩展,另外还要考虑业务是否有亲缘性,同时在后端服务出现异常的情况下, 自动进行健康检查,异常的服务能及时进行下线操作,快速失败。</p><h2><span id="数据库及缓存优化">数据库及缓存优化</span></h2><p>数据库和缓存配合使用是解决后端结构化数据与非结构化问题的有效手段,根据不同的场景,要明白哪种数据使用结构化数据合适,哪种数据使用非结构化数据更合适,以及哪种方式在保证性能较好的情况下成本又可以接受。</p><p>同时,如何在数据库和缓存之间进行过渡也是需要考虑的,比如数据在更新的时候,如何保证缓存的一致性,如何保证热点数据一直被访问,提高缓存的命中率等。另外,当大量用户访问不存在的数据时,也有可能导致后端的压力非常大,甚至有可能造成雪崩效应。</p><p>每种服务独立承担对应的功能,各司其职,并且根据应用的特性区别提供不同的服务能力,比如应用服务器提供用户的接入服务,数据库服务专门承担结构化数据的存储,缓存承担或非结构化数据(KV键值对)的存储等,如果要提供搜索的功能,还需将数据进行分词、索引、检索等,不同服务器根据业务的功用需求来提供对应的服务。</p><hr><p>在这个阶段,除了必须保证满足业务的功能型需求,还要更多考虑非功能性需求。比如,通过前端负载均衡提供业务分流的能力,根据用户的特征进行不同的流量转发;数据库提供主备的能力,两者之间通过数据同步进行数据备份,当主数据库发生故障后,应用可以自动切换到备份的服务器来为用户提供服务;在用户体验方面,可能会引入缓存、CDN等基础服务来提供性能加速。</p><p><strong>架构演化发展历程：</strong></p><ol><li><a href="/2018/05/16/architec-initial/">初创期架构</a></li><li><a href="/2018/05/16/architec-growth/">快速成长期架构</a></li><li><a href="/2018/05/16/architec-distri/">分布式服务架构</a></li></ol><p>参考：</p><ol><li>云原生应用架构实践 （网易云基础服务架构团队 著）</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;初创公司随着业务的进一步发展,当DAU达到十万的时候,通常是最关键的时刻,既要保证业务的稳定运行,又要进行产品的快速迭代。到了这个阶段,由于业务模式得到了一定的验证和反馈,有可能会出现很多竞品或友商。一方面,随着风险资本的注入,会依赖更有质量的数据进行发展运营,另一方面,竞品的出现又导致了市场的加速前进.因此,能否在这个阶段保证业务与技术的和谐发展,是考验架构是否足够灵活的指标之一,本节主要说明几点。&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="yunke.science/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="架构" scheme="yunke.science/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>初创期架构</title>
    <link href="yunke.science/2018/05/16/architec-initial/"/>
    <id>yunke.science/2018/05/16/architec-initial/</id>
    <published>2018-05-16T04:28:03.000Z</published>
    <updated>2018-05-16T07:08:45.615Z</updated>
    
    <content type="html"><![CDATA[<p>创业公司在开始新业务的时期,基本处在试错或原型验证阶段,这个阶段更多是关注业务的本身是否有前景或商业模式,而不会把非常多的精力放在技术的系统架构上,尤其是对于非技术型或不确定型的项目立项阶段。尽管很多技术人员也预料到前期需要很多时间去好好设计系统,才能保证支撑后续可能的业务快速发展,但往往由于时间成本或人力等原因而无法很好地执行。</p><a id="more"></a><p>@[TOC]</p><p>一般来讲,创业型的项目对时间的要求非常苛刻,需要在3到6个月时间内完成系统的上线,否则有可能由于业务无法快速上线验证,导致无法获取相关的原始数据进行下<br>个目标验证,更严重的有可能造成资金链的断裂。罗马不是一天建成的,因此这个阶段会使用相对简单的架构方式来进行设计,本节先从最主要的几点进行说明.</p><h2><span id="单体架构">单体架构</span></h2><p>对于创业型公司来说,由于人才、技术、资金等重要因素的影响,同时,技术人员为了配合产品的需求,会采用最简单的架构来完成最原始阶段开发,根据我们接触的不少用户反馈,有些企业考虑成本因素,甚至只使用一台服务器或者容器服务。另外,传统官网、论坛等应用,由于早期的设计采用了单体架构来实现,只需要一台服务器或容器来服务即可。对于其他的应用服务器、数据库、静态文件等资源,也是部署到同一台服务器或容器上来服务。最简单的架构模型如图所示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">PHP--&gt;Apache</span><br><span class="line">Apache--&gt;Mysql</span><br></pre></td></tr></table></figure><p>对于早期的单体应用,应用服务+数据库服务基本上就组成了最原始的架构模型,技术人更多会考虑技术的选型,包括编程语言、版本管理、数据库的类型等。比如PHP的开发者选择PHP-MySQL, Java的开发者采用Tomcat-MySOL等开发方式。</p><h2><span id="服务器分离">服务器分离</span></h2><p>根据线上运行经验,一般的业务的类型,如果每日的用户访问量在百万级别以内,只要进行简单的Web应用性能参数调优、数据库索引优化等,基本上就能保证服务的稳定运行,当然,随着访问量的不断增加,部署在同一台服务器上的应用及数据库服务,会造成服务器的CPU/内存/磁盘/带宽等系统资源竞争,从而相互影响,显然很容易出现性能瓶颈,如果这台服务器出现了宕机或无法恢复的错误,就有可能导致全站不可访问或者数据丢失等情况,后果非常严重,因此大部分产品会将Web应用服务器和数据库服务器进行物理分离,独立部署,相互热备提供服务,只需要增加很少的成本,就能解决对应性能和数据的可靠性等问题.</p><p>初期由于各种条件存在不能很好地进行新项目前景的预见,技术人员如果能在最小成本的情况下保证架构的合理性,还能很好地服务产品功能需求,甚至只要在部署架构上稍做调整,就可以防止出现灾难性的问题,这其中也包括很多技术架构上的考虑。</p><h2><span id="业务模型">业务模型</span></h2><p>一般而言,现阶段的业务比较简单,产品也比较单一,业务会随时根据其运营数据进行调整,因此,这时需要技术人能够较好地把不同的模块分离出来,对于偏业务相关的功能,需要有较好的心态接收随时变化的不确定性,对于后续可能复用或大量依赖的工程,需要进行较好的设计,否则可能在业务爆发时导致业务开发的进度越来越慢,甚至阻碍业务的发展,造成业务时常中断,即使有人力或时间来对系统进行重新设计,也会令技术人员产生抵触心理,同时也会引入较高的风险,因此,基于云原生应用的设计模式在最基础的阶段对架构也有很大的作用,包括考虑如何使用云的弹性,将不可变优势融合到系统的设计中,合理的业务模型分界也是确保后续能发展的重要步骤之一。</p><hr><p>总而言之,在早期项目原型验证或者快速试错阶段,采用单体架构具有很大的技术优势,产品的想法也在项目的初始阶段就能进行比较好的迭代开发,发布和部署也比较灵活。然而,随着业务的增长,如果架构还是一成不变的话,带来的技术风险就越来越高.<br>比如,代码行数的增加影响技术人员的学习成本、业务的变更速度、业务的可靠性、安全性及工程变大后的发布效率等,每次修改都必须反复测试,否则全站随时可能不可用,导致业务中断或者丢失市场的机会,因此,这部分的技术债务必须在业务快速发展的同时,进行技术架构的改造,使其能保证后期业务的支撑,所以,除了业务的发展判断外,对开发人员的技术能力储备和架构远见判断也将成为考虑的事情之一.</p><p><strong>架构演化发展历程：</strong></p><ol><li><a href="/2018/05/16/architec-initial/">初创期架构</a></li><li><a href="/2018/05/16/architec-growth/">快速成长期架构</a></li><li><a href="/2018/05/16/architec-distri/">分布式服务架构</a></li></ol><p>参考：</p><ol><li>云原生应用架构实践 （网易云基础服务架构团队 著）</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;创业公司在开始新业务的时期,基本处在试错或原型验证阶段,这个阶段更多是关注业务的本身是否有前景或商业模式,而不会把非常多的精力放在技术的系统架构上,尤其是对于非技术型或不确定型的项目立项阶段。尽管很多技术人员也预料到前期需要很多时间去好好设计系统,才能保证支撑后续可能的业务快速发展,但往往由于时间成本或人力等原因而无法很好地执行。&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="yunke.science/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="架构" scheme="yunke.science/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
</feed>
