<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>天青色等烟雨</title>
  
  <subtitle>文不在多、有换则新、人不在挤、有来就行</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="yunke.science/"/>
  <updated>2018-09-27T05:57:45.166Z</updated>
  <id>yunke.science/</id>
  
  <author>
    <name>Young</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubernetes 1.11.3 使用kubeadm创建高可用的ETCD群集</title>
    <link href="yunke.science/2018/09/27/k8s-1-11-3-etcd/"/>
    <id>yunke.science/2018/09/27/k8s-1-11-3-etcd/</id>
    <published>2018-09-27T05:54:51.000Z</published>
    <updated>2018-09-27T05:57:45.166Z</updated>
    
    <content type="html"><![CDATA[<p>Kubeadm默认在由master节点上的kubelet管理的静态pod中运行单个成员etcd集群。这不是高可用性设置，因为etcd集群只包含一个成员，并且无法维持任何成员变得不可用。</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#etcd-%E4%B8%8Emaster-%E4%B8%80%E8%B5%B7%E9%83%A8%E7%BD%B2">etcd 与master 一起部署</a><ul><li><a href="#%E7%AC%AC%E4%B8%80%E5%8F%B0master%E8%8A%82%E7%82%B9%E4%B8%BB%E6%9C%BA%E4%B8%8A%E6%89%A7%E8%A1%8C%E9%85%8D%E7%BD%AE">第一台master节点主机上执行配置</a></li><li><a href="#%E5%A4%8D%E5%88%B6%E7%AC%AC%E4%B8%80%E5%8F%B0%E7%94%9F%E6%88%90%E7%9A%84%E8%AF%81%E4%B9%A6%E5%8F%8A%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%88%B0%E5%85%B6%E4%BB%96master%E4%B8%BB%E6%9C%BA%E7%9B%B8%E5%90%8C%E4%BD%8D%E7%BD%AE">复制第一台生成的证书及配置文件到其他master主机相同位置</a></li><li><a href="#%E7%AC%AC%E4%BA%8C%E5%8F%B0master%E8%8A%82%E7%82%B9%E4%B8%BB%E6%9C%BA%E4%B8%8A%E6%89%A7%E8%A1%8C%E9%85%8D%E7%BD%AE">第二台master节点主机上执行配置</a></li><li><a href="#%E7%AC%AC%E4%B8%89%E5%8F%B0master%E8%8A%82%E7%82%B9%E4%B8%BB%E6%9C%BA%E4%B8%8A%E6%89%A7%E8%A1%8C%E9%85%8D%E7%BD%AE">第三台master节点主机上执行配置</a></li></ul></li><li><a href="#%E4%BD%BF%E7%94%A8-kubeadm-%E9%83%A8%E7%BD%B2%E5%8D%95%E7%8B%AC%E7%9A%84-etcd-%E9%9B%86%E7%BE%A4%E5%B9%B6%E4%BD%BF%E7%94%A8kubelet%E7%AE%A1%E7%90%86">使用 kubeadm 部署单独的 etcd 集群，并使用kubelet管理</a><ul><li><a href="#%E8%AE%BE%E7%BD%AE%E7%BE%A4%E9%9B%86">设置群集</a></li><li><a href="#1-%E5%B0%86kubelet%E9%85%8D%E7%BD%AE%E4%B8%BAetcd%E7%9A%84%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86%E5%99%A8">1. 将kubelet配置为etcd的服务管理器。</a></li><li><a href="#2-%E4%B8%BAkubeadm%E5%88%9B%E5%BB%BA%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">2. 为kubeadm创建配置文件。</a></li><li><a href="#3-%E7%94%9F%E6%88%90%E8%AF%81%E4%B9%A6%E9%A2%81%E5%8F%91%E6%9C%BA%E6%9E%84">3. 生成证书颁发机构</a></li><li><a href="#4-%E4%B8%BA%E6%AF%8F%E4%B8%AA%E6%88%90%E5%91%98%E5%88%9B%E5%BB%BA%E8%AF%81%E4%B9%A6">4. 为每个成员创建证书</a></li><li><a href="#5-%E5%A4%8D%E5%88%B6%E8%AF%81%E4%B9%A6%E5%92%8Ckubeadm%E9%85%8D%E7%BD%AE">5. 复制证书和kubeadm配置</a></li><li><a href="#6-%E7%A1%AE%E4%BF%9D%E5%AD%98%E5%9C%A8%E6%89%80%E6%9C%89%E9%A2%84%E6%9C%9F%E6%96%87%E4%BB%B6">6. 确保存在所有预期文件</a></li><li><a href="#7-%E5%88%9B%E5%BB%BA%E9%9D%99%E6%80%81pod%E6%B8%85%E5%8D%95">7. 创建静态pod清单</a></li><li><a href="#8-%E8%BF%87%E5%87%A0%E5%88%86%E9%92%9F%E6%9F%A5%E7%9C%8B-etcd-pod-%E7%9A%84%E7%8A%B6%E6%80%81">8. 过几分钟，查看 etcd pod 的状态</a></li><li><a href="#9-%E6%A3%80%E6%9F%A5%E7%BE%A4%E9%9B%86%E8%BF%90%E8%A1%8C%E7%8A%B6%E5%86%B5">9. 检查群集运行状况</a></li></ul></li><li><a href="#%E4%BD%BF%E7%94%A8-kubeadm-%E7%94%9F%E6%88%90etcd%E8%AF%81%E4%B9%A6%E5%B9%B6%E4%BD%BF%E7%94%A8systemctl%E7%AE%A1%E7%90%86">使用 <code>kubeadm</code> 生成<code>etcd</code>证书，，并使用<code>systemctl</code>管理</a><ul><li><a href="#1-%E4%BD%BF%E7%94%A8%E4%BB%A5%E4%B8%8B%E8%84%9A%E6%9C%AC%E4%B8%BA%E6%AF%8F%E4%B8%AA%E5%B0%86%E5%9C%A8%E5%85%B6%E4%B8%8A%E8%BF%90%E8%A1%8Cetcd%E6%88%90%E5%91%98%E7%9A%84%E4%B8%BB%E6%9C%BA%E7%94%9F%E6%88%90%E4%B8%80%E4%B8%AAetcdservice%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8%E6%96%87%E4%BB%B6">1. 使用以下脚本为每个将在其上运行etcd成员的主机生成一个<code>etcd.service</code>服务启动文件。</a></li><li><a href="#2-%E4%BD%BF%E7%94%A8-kubeadm-%E7%94%9F%E6%88%90etcd%E8%AF%81%E4%B9%A6">2.  使用 <code>kubeadm</code> 生成<code>etcd</code>证书</a><ul><li><a href="#%E5%88%9B%E5%BB%BA%E6%A0%B9%E8%AF%81%E4%B9%A6">创建根证书</a></li><li><a href="#%E5%88%9B%E5%BB%BA%E8%AF%81%E4%B9%A6">创建证书</a></li><li><a href="#%E5%A4%8D%E5%88%B6%E8%AF%81%E4%B9%A6%E5%88%B0%E5%85%B6%E4%BB%96%E4%B8%A4%E5%8F%B0%E4%B8%BB%E6%9C%BA%E5%AF%B9%E5%BA%94%E7%9B%AE%E5%BD%95">复制证书到其他两台主机对应目录</a></li></ul></li><li><a href="#3-%E5%90%AF%E5%8A%A8etcd%E6%9C%8D%E5%8A%A1%E9%AA%8C%E8%AF%81%E6%9C%8D%E5%8A%A1">3. 启动etcd服务,验证服务</a></li></ul></li><li><a href="#kubernetes-%E9%85%8D%E7%BD%AE%E5%A4%96%E9%83%A8-etcd">Kubernetes 配置外部 etcd</a></li></ul></p><blockquote><p>本节将介绍创建由三个成员组成的高可用性etcd集群的过程，该集群可在使用kubeadm设置kubernetes集群时用作外部etcd。</p></blockquote><blockquote><p><a href="https://kubernetes.io/docs/setup/independent/high-availability/" target="_blank" rel="noopener">Creating Highly Available Clusters with kubeadm</a></p></blockquote><p>使用kubeadm设置高可用性Kubernetes集群主要是创建高可用的ETCD群集，有两种不同方法：</p><ol><li>etcd 与 master节点一起部署。只需较少的主机，etcd与master部署在同一位置。</li><li>etcd 单独部署，使用外部独立的etcd集群。需要较多的主机，master与etcd分开部署。</li></ol><h2><span id="etcd-与master-一起部署"> etcd 与master 一起部署</span></h2><h3><span id="第一台master节点主机上执行配置"> 第一台master节点主机上执行配置</span></h3><ol><li>创建 <code>kubeadm-config.yaml</code> 模版文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.x</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https://127.0.0.1:2379,https://CP0_IP:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https://CP0_IP:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https://CP0_IP:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https://CP0_IP:2380&quot;</span><br><span class="line">      initial-cluster: &quot;CP0_HOSTNAME=https://CP0_IP:2380&quot;</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - CP0_HOSTNAME</span><br><span class="line">      - CP0_IP</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - CP0_HOSTNAME</span><br><span class="line">      - CP0_IP</span><br></pre></td></tr></table></figure><blockquote><p>以上模版文件仅显示配置了etcd配置，其他配置没有显示</p></blockquote><ol start="2"><li>替换 <code>kubernetesVersion: v1.11.x</code> 中的<code>x</code>为<a href="https://storage.googleapis.com/kubernetes-release/release/stable-1.11.txt" target="_blank" rel="noopener">最近可用的版本</a>，如<code>kubernetesVersion: v1.11.3</code></li><li>替换模版中的变量</li></ol><ul><li><code>CP0_HOSTNAME</code></li><li><code>CP0_IP</code></li></ul><ol start="4"><li>运行初始化</li></ol><blockquote><p>kubeadm 过程中可能会连接外网<code>gcr.io</code>查询当前稳定版本，由于无法连接<code>gcr.io</code>导致执行失败。<br>添加代理<code>export http_proxy=http://IP:PORT</code> ,执行完以后取消代理 <code>unset http_proxy</code></p></blockquote><p><code>kubeadm init --config kubeadm-config.yaml</code></p><h3><span id="复制第一台生成的证书及配置文件到其他master主机相同位置"> 复制第一台生成的证书及配置文件到其他master主机相同位置</span></h3><ul><li>/etc/kubernetes/pki/ca.crt</li><li>/etc/kubernetes/pki/ca.key</li><li>/etc/kubernetes/pki/sa.key</li><li>/etc/kubernetes/pki/sa.pub</li><li>/etc/kubernetes/pki/front-proxy-ca.crt</li><li>/etc/kubernetes/pki/front-proxy-ca.key</li><li>/etc/kubernetes/pki/etcd/ca.crt</li><li>/etc/kubernetes/pki/etcd/ca.key</li><li>/etc/kubernetes/admin.conf</li></ul><h3><span id="第二台master节点主机上执行配置"> 第二台master节点主机上执行配置</span></h3><ol><li>创建 <code>kubeadm-config.yaml</code> 模版文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.x</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https://127.0.0.1:2379,https://CP1_IP:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https://CP1_IP:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https://CP1_IP:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https://CP1_IP:2380&quot;</span><br><span class="line">      initial-cluster: &quot;CP0_HOSTNAME=https://CP0_IP:2380,CP1_HOSTNAME=https://CP1_IP:2380&quot;</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - CP1_HOSTNAME</span><br><span class="line">      - CP1_IP</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - CP1_HOSTNAME</span><br><span class="line">      - CP1_IP</span><br></pre></td></tr></table></figure><blockquote><p>以上模版文件仅显示配置了etcd配置，其他配置没有显示</p></blockquote><ol start="2"><li>替换 <code>kubernetesVersion: v1.11.x</code> 中的<code>x</code>为<a href="https://storage.googleapis.com/kubernetes-release/release/stable-1.11.txt" target="_blank" rel="noopener">最近可用的版本</a>，如<code>kubernetesVersion: v1.11.3</code></li><li>替换模版中的变量</li></ol><ul><li><code>CP0_HOSTNAME</code></li><li><code>CP0_IP</code></li><li><code>CP1_HOSTNAME</code></li><li><code>CP1_IP</code></li></ul><ol start="4"><li><p>确认复制第一台主机目录 <code>/etc/kubernetes/pki/</code> 及文件 <code>/etc/kubernetes/admin.conf</code> 到本主机相同位置</p></li><li><p>运行<code>kubeadm phase</code> 命令引导kubelet:</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase certs all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml</span><br><span class="line">systemctl start kubelet</span><br></pre></td></tr></table></figure><ol start="6"><li>运行命令，添加节点到<code>etcd</code>集群</li></ol><blockquote><p>按实际修改以下IP 和 HOSTNAME</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">export CP0_IP=10.0.0.7</span><br><span class="line">export CP0_HOSTNAME=cp0</span><br><span class="line">export CP1_IP=10.0.0.8</span><br><span class="line">export CP1_HOSTNAME=cp1</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/admin.conf </span><br><span class="line">kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP1_HOSTNAME&#125; https://$&#123;CP1_IP&#125;:2380</span><br><span class="line">kubeadm alpha phase etcd local --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure><blockquote><p>在将节点添加到正在运行的集群之后，以及在将新节点加入etcd集群之前，此命令会导致etcd集群在短时间内不可用</p></blockquote><ol start="7"><li>部署其他控制组件并将节点标记为master主节点：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure><h3><span id="第三台master节点主机上执行配置"> 第三台master节点主机上执行配置</span></h3><ol><li>创建 <code>kubeadm-config.yaml</code> 模版文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.x</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https://127.0.0.1:2379,https://CP2_IP:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https://CP2_IP:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https://CP2_IP:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https://CP2_IP:2380&quot;</span><br><span class="line">      initial-cluster: &quot;CP0_HOSTNAME=https://CP0_IP:2380,CP1_HOSTNAME=https://CP1_IP:2380,CP2_HOSTNAME=https://CP2_IP:2380&quot;</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - CP2_HOSTNAME</span><br><span class="line">      - CP2_IP</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - CP2_HOSTNAME</span><br><span class="line">      - CP2_IP</span><br></pre></td></tr></table></figure><blockquote><p>以上模版文件仅显示配置了etcd配置，其他配置没有显示</p></blockquote><ol start="2"><li>替换 <code>kubernetesVersion: v1.11.x</code> 中的<code>x</code>为<a href="https://storage.googleapis.com/kubernetes-release/release/stable-1.11.txt" target="_blank" rel="noopener">最近可用的版本</a>，如<code>kubernetesVersion: v1.11.3</code></li><li>替换模版中的变量</li></ol><ul><li><code>CP0_HOSTNAME</code></li><li><code>CP0_IP</code></li><li><code>CP1_HOSTNAME</code></li><li><code>CP1_IP</code></li><li><code>CP2_HOSTNAME</code></li><li><code>CP2_IP</code></li></ul><ol start="4"><li><p>确认复制第一台master主机目录 <code>/etc/kubernetes/pki/</code> 及文件 <code>/etc/kubernetes/admin.conf</code> 到本主机相同位置</p></li><li><p>运行<code>kubeadm phase</code> 命令引导kubelet:</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase certs all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml</span><br><span class="line">systemctl start kubelet</span><br></pre></td></tr></table></figure><ol start="6"><li>运行命令，添加节点到<code>etcd</code>集群</li></ol><blockquote><p>按实际修改以下IP 和 HOSTNAME</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">export CP0_IP=10.0.0.7</span><br><span class="line">export CP0_HOSTNAME=cp0</span><br><span class="line">export CP2_IP=10.0.0.8</span><br><span class="line">export CP2_HOSTNAME=cp1</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=/etc/kubernetes/admin.conf </span><br><span class="line">kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP2_HOSTNAME&#125; https://$&#123;CP2_IP&#125;:2380</span><br><span class="line">kubeadm alpha phase etcd local --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure><blockquote><p>在将节点添加到正在运行的集群之后，以及在将新节点加入etcd集群之前，此命令会导致etcd集群在短时间内不可用</p></blockquote><ol start="7"><li>部署其他控制组件并将节点标记为master主节点：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure><p>整个集群部署完成。</p><h2><span id="使用-kubeadm-部署单独的-etcd-集群并使用kubelet管理"> 使用 kubeadm 部署单独的 etcd 集群，并使用kubelet管理</span></h2><blockquote><p><a href="https://kubernetes.io/docs/setup/independent/setup-ha-etcd-with-kubeadm/" target="_blank" rel="noopener">Set up a high availability etcd cluster with kubeadm</a></p></blockquote><p>准备工作：<br>0. 准备三个单独的主机，不能部署kubernetes ，仅部署 <code>etcd</code></p><ol><li>三个主机可以通过端口2379和2380相互通信。本文档假定这些默认端口。但是，它们可以通过kubeadm配置文件进行配置。</li><li>每个主机必须安装<code>docker</code>，<code>kubelet</code>和<code>kubeadm</code>。</li><li>一些在主机之间复制文件的基础结构 例如ssh，scp 可以满足这个要求</li></ol><h3><span id="设置群集"> 设置群集</span></h3><p>一般方法是在一个节点上生成所有证书，并仅将必要的文件分发给其他节点。</p><h3><span id="1-将kubelet配置为etcd的服务管理器"> 1. 将kubelet配置为etcd的服务管理器。</span></h3><p>(每台主机上执行)<br>运行etcd比运行kubernetes更简单，因此您必须通过创建一个具有更高优先级的新文件来覆盖kubeadm提供的kubelet单元文件。</p><p><code>cgroup-driver=systemd</code> 要与 <code>docker info中的Cgroup Driver</code> 相匹配。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=&quot;KUBELET_CGROUP_ARGS=--cgroup-driver=systemd&quot;</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet $KUBELET_CGROUP_ARGS --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true</span><br><span class="line">Restart=always</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure><h3><span id="2-为kubeadm创建配置文件"> 2. 为kubeadm创建配置文件。</span></h3><p>(仅在HOST0主机上执行)<br>使用以下脚本为每个将在其上运行etcd成员的主机生成一个kubeadm配置文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># Update HOST0, HOST1, and HOST2 with the IPs or resolvable names of your hosts</span><br><span class="line">export HOST0=10.0.0.6</span><br><span class="line">export HOST1=10.0.0.7</span><br><span class="line">export HOST2=10.0.0.8</span><br><span class="line"></span><br><span class="line"># Create temp directories to store files that will end up on other hosts.</span><br><span class="line">mkdir -p /tmp/$&#123;HOST0&#125;/ /tmp/$&#123;HOST1&#125;/ /tmp/$&#123;HOST2&#125;/</span><br><span class="line"></span><br><span class="line">ETCDHOSTS=($&#123;HOST0&#125; $&#123;HOST1&#125; $&#123;HOST2&#125;)</span><br><span class="line">NAMES=(&quot;infra0&quot; &quot;infra1&quot; &quot;infra2&quot;)</span><br><span class="line"></span><br><span class="line">for i in &quot;$&#123;!ETCDHOSTS[@]&#125;&quot;; do</span><br><span class="line">HOST=$&#123;ETCDHOSTS[$i]&#125;</span><br><span class="line">NAME=$&#123;NAMES[$i]&#125;</span><br><span class="line">cat &lt;&lt; EOF &gt; /tmp/$&#123;HOST&#125;/kubeadmcfg.yaml</span><br><span class="line">apiVersion: &quot;kubeadm.k8s.io/v1alpha2&quot;</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">etcd:</span><br><span class="line">    local:</span><br><span class="line">        serverCertSANs:</span><br><span class="line">        - &quot;$&#123;HOST&#125;&quot;</span><br><span class="line">        peerCertSANs:</span><br><span class="line">        - &quot;$&#123;HOST&#125;&quot;</span><br><span class="line">        extraArgs:</span><br><span class="line">            initial-cluster: infra0=https://$&#123;ETCDHOSTS[0]&#125;:2380,infra1=https://$&#123;ETCDHOSTS[1]&#125;:2380,infra2=https://$&#123;ETCDHOSTS[2]&#125;:2380</span><br><span class="line">            initial-cluster-state: new</span><br><span class="line">            name: $&#123;NAME&#125;</span><br><span class="line">            listen-peer-urls: https://$&#123;HOST&#125;:2380</span><br><span class="line">            listen-client-urls: https://$&#123;HOST&#125;:2379</span><br><span class="line">            advertise-client-urls: https://$&#123;HOST&#125;:2379</span><br><span class="line">            initial-advertise-peer-urls: https://$&#123;HOST&#125;:2380</span><br><span class="line">EOF</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3><span id="3-生成证书颁发机构"> 3. 生成证书颁发机构</span></h3><p>如果您已有CA，那么唯一的操作是将CA crt和 key文件复制到<code>/etc/kubernetes/pki/etcd/ca.crt</code>和 <code>/etc/kubernetes/pki/etcd/ca.key</code>。复制这些文件后，继续执行下一步“为每个成员创建证书”。</p><p>如果您还没有CA，则在<code>$HOST0</code>（生成kubeadm的配置文件的位置）上运行此命令。</p><p><code>kubeadm alpha phase certs etcd-ca</code></p><p>这会创建两个文件</p><ul><li><code>/etc/kubernetes/pki/etcd/ca.crt</code></li><li><code>/etc/kubernetes/pki/etcd/ca.key</code></li></ul><h3><span id="4-为每个成员创建证书"> 4. 为每个成员创建证书</span></h3><p>(仅在HOST0主机上执行)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase certs etcd-server --config=/tmp/$&#123;HOST2&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-peer --config=/tmp/$&#123;HOST2&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/$&#123;HOST2&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/$&#123;HOST2&#125;/kubeadmcfg.yaml</span><br><span class="line">cp -R /etc/kubernetes/pki /tmp/$&#123;HOST2&#125;/</span><br><span class="line"># cleanup non-reusable certificates</span><br><span class="line">find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete</span><br><span class="line"></span><br><span class="line">kubeadm alpha phase certs etcd-server --config=/tmp/$&#123;HOST1&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-peer --config=/tmp/$&#123;HOST1&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/$&#123;HOST1&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/$&#123;HOST1&#125;/kubeadmcfg.yaml</span><br><span class="line">cp -R /etc/kubernetes/pki /tmp/$&#123;HOST1&#125;/</span><br><span class="line">find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete</span><br><span class="line"></span><br><span class="line">kubeadm alpha phase certs etcd-server --config=/tmp/$&#123;HOST0&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-peer --config=/tmp/$&#123;HOST0&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/$&#123;HOST0&#125;/kubeadmcfg.yaml</span><br><span class="line">kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/$&#123;HOST0&#125;/kubeadmcfg.yaml</span><br><span class="line"># No need to move the certs because they are for HOST0</span><br><span class="line"></span><br><span class="line"># clean up certs that should not be copied off this host</span><br><span class="line">find /tmp/$&#123;HOST2&#125; -name ca.key -type f -delete</span><br><span class="line">find /tmp/$&#123;HOST1&#125; -name ca.key -type f -delete</span><br></pre></td></tr></table></figure><h3><span id="5-复制证书和kubeadm配置"> 5. 复制证书和kubeadm配置</span></h3><p>已生成证书，现在必须将它们移动到各自的主机。<br>将 <code>kubeadmcfg.yaml</code> 和 <code>pki</code> 目录拷贝到对应的主机上<code>/etc/kubernetes/pki</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">USER=ubuntu</span><br><span class="line"> HOST=$&#123;HOST1&#125;</span><br><span class="line"> scp -r /tmp/$&#123;HOST&#125;/* $&#123;USER&#125;@$&#123;HOST&#125;:</span><br><span class="line"> ssh $&#123;USER&#125;@$&#123;HOST&#125;</span><br><span class="line"> USER@HOST $ sudo -Es</span><br><span class="line"> root@HOST $ chown -R root:root pki</span><br><span class="line"> root@HOST $ mv pki /etc/kubernetes/</span><br></pre></td></tr></table></figure><h3><span id="6-确保存在所有预期文件"> 6. 确保存在所有预期文件</span></h3><p>所需文件的完整列表<code>$HOST0</code>是：</p><h3><span id="7-创建静态pod清单"> 7. 创建静态pod清单</span></h3><p>现在证书和配置已到位，是时候创建清单了。在每个主机上运行kubeadm命令以生成etcd的静态清单。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@HOST0 $ kubeadm alpha phase etcd local --config=/tmp/$&#123;HOST0&#125;/kubeadmcfg.yaml</span><br><span class="line">root@HOST1 $ kubeadm alpha phase etcd local --config=/tmp/$&#123;HOST1&#125;/kubeadmcfg.yaml</span><br><span class="line">root@HOST2 $ kubeadm alpha phase etcd local --config=/tmp/$&#123;HOST2&#125;/kubeadmcfg.yaml</span><br></pre></td></tr></table></figure><p>查看是否生成文件 <code>/etc/kubernetes/manifests/etcd.yaml</code> .</p><h3><span id="8-过几分钟查看-etcd-pod-的状态"> 8. 过几分钟，查看 etcd pod 的状态</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># docker ps | grep etcd</span><br><span class="line">e29577f035f6        b8df3b177be2                               &quot;etcd --advertise-...&quot;   24 seconds ago      Up 23 seconds                                                                  k8s_etcd_etcd-test01_kube-system_61e08b291cb3ea4992e74c5a47f0b8e0_12</span><br><span class="line">c9312e7f67af        k8s.gcr.io/pause:3.1                       &quot;/pause&quot;                 24 minutes ago      Up 24 minutes                                                                  k8s_POD_etcd-test01_kube-system_61e08b291cb3ea4992e74c5a47f0b8e0_0</span><br></pre></td></tr></table></figure><p>有两个容器正在运行，如果 <code>pod k8s_etcd_etcd</code>启动失败，使用 <code>docker logs -f k8s_etcd_etcd..</code> 查看启动错误日志。</p><h3><span id="9-检查群集运行状况"> 9. 检查群集运行状况</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it \</span><br><span class="line">--net host \</span><br><span class="line">-v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.18 etcdctl \</span><br><span class="line">--cert-file /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key-file /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--ca-file /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--endpoints https://$&#123;HOST0&#125;:2379 cluster-health</span><br><span class="line"></span><br><span class="line">member 327e88294e54db55 is healthy: got healthy result from https://++:2379</span><br><span class="line">cluster is healthy</span><br></pre></td></tr></table></figure><h2><span id="使用-kubeadm-生成etcd证书并使用systemctl管理"> 使用 <code>kubeadm</code> 生成<code>etcd</code>证书，，并使用<code>systemctl</code>管理</span></h2><blockquote><p>使用 <code>systemctl</code>管理 <code>etcd</code> ，不占用 <code>kubelet</code>，可以在主机上同时作为kubernetes 节点。</p></blockquote><h3><span id="1-使用以下脚本为每个将在其上运行etcd成员的主机生成一个etcdservice服务启动文件"> 1. 使用以下脚本为每个将在其上运行etcd成员的主机生成一个<code>etcd.service</code>服务启动文件。</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">export HOST0=10.0.0.6</span><br><span class="line">export HOST1=10.0.0.7</span><br><span class="line">export HOST2=10.0.0.8</span><br><span class="line"></span><br><span class="line"># Create temp directories to store files that will end up on other hosts.</span><br><span class="line">mkdir -p /tmp/$&#123;HOST0&#125;/ /tmp/$&#123;HOST1&#125;/ /tmp/$&#123;HOST2&#125;/</span><br><span class="line"></span><br><span class="line">ETCDHOSTS=($&#123;HOST0&#125; $&#123;HOST1&#125; $&#123;HOST2&#125;)</span><br><span class="line">NAMES=(&quot;infra0&quot; &quot;infra1&quot; &quot;infra2&quot;)</span><br><span class="line"></span><br><span class="line">for i in &quot;$&#123;!ETCDHOSTS[@]&#125;&quot;; do</span><br><span class="line">HOST=$&#123;ETCDHOSTS[$i]&#125;</span><br><span class="line">NAME=$&#123;NAMES[$i]&#125;</span><br><span class="line">cat &lt;&lt; EOF &gt; /tmp/$&#123;HOST&#125;/etcd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line">Documentation=https://github.com/coreos</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">WorkingDirectory=/var/lib/etcd/</span><br><span class="line">ExecStart=/usr/local/bin/etcd \\</span><br><span class="line">  --name=$&#123;NAME&#125; \\</span><br><span class="line">  --cert-file=/etc/kubernetes/pki/etcd/server.crt \\</span><br><span class="line">  --key-file=/etc/kubernetes/pki/etcd/server.key \\</span><br><span class="line">  --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt \\</span><br><span class="line">  --peer-key-file=/etc/kubernetes/pki/etcd/peer.key \\</span><br><span class="line">  --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt \\</span><br><span class="line">  --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt \\</span><br><span class="line">  --initial-advertise-peer-urls=https://$&#123;HOST&#125;:2380 \\</span><br><span class="line">  --listen-peer-urls=https://$&#123;HOST&#125;:2380 \\</span><br><span class="line">  --listen-client-urls=https://$&#123;HOST&#125;:2379,http://127.0.0.1:2379 \\</span><br><span class="line">  --advertise-client-urls=https://$&#123;HOST&#125;:2379 \\</span><br><span class="line">  --initial-cluster=infra0=https://$&#123;ETCDHOSTS[0]&#125;:2380,infra1=https://$&#123;ETCDHOSTS[1]&#125;:2380,infra2=https://$&#123;ETCDHOSTS[2]&#125;:2380 \\</span><br><span class="line">  --initial-cluster-state=new \\</span><br><span class="line">  --client-cert-auth=true \\</span><br><span class="line">  --peer-client-cert-auth=true  \\</span><br><span class="line">  --snapshot-count=10000   \\</span><br><span class="line">  --data-dir=/var/lib/etcd</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line">done</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">把三个 etcd.service 文件复制到对应主机的 /etc/systemd/system/ 目录下</span><br></pre></td></tr></table></figure><h3><span id="2-使用-kubeadm-生成etcd证书"> 2.  使用 <code>kubeadm</code> 生成<code>etcd</code>证书</span></h3><blockquote><p>仅生成一套证书，供etcd集群节点公用、</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">export HOST0=10.0.0.6</span><br><span class="line">export HOST1=10.0.0.7</span><br><span class="line">export HOST2=10.0.0.8</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; /tmp/kubeadm-etcd-cfg.yaml</span><br><span class="line">apiVersion: &quot;kubeadm.k8s.io/v1alpha2&quot;</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">etcd:</span><br><span class="line">    local:</span><br><span class="line">        serverCertSANs:</span><br><span class="line">        - &quot;127.0.0.1&quot;</span><br><span class="line">        - &quot;$&#123;HOST0&#125;&quot;</span><br><span class="line">        - &quot;$&#123;HOST1&#125;&quot;</span><br><span class="line">        - &quot;$&#123;HOST2&#125;&quot;</span><br><span class="line">        peerCertSANs:</span><br><span class="line">        - &quot;127.0.0.1&quot;</span><br><span class="line">        - &quot;$&#123;HOST0&#125;&quot;</span><br><span class="line">        - &quot;$&#123;HOST1&#125;&quot;</span><br><span class="line">        - &quot;$&#123;HOST2&#125;&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4><span id="创建根证书"> 创建根证书</span></h4><p><code>kubeadm alpha phase certs etcd-ca</code></p><p>这会创建两个文件</p><ol><li>/etc/kubernetes/pki/etcd/ca.crt</li><li>/etc/kubernetes/pki/etcd/ca.key</li></ol><h4><span id="创建证书"> 创建证书</span></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm alpha phase certs etcd-server --config=/tmp/kubeadm-etcd-cfg.yaml</span><br><span class="line">kubeadm alpha phase certs etcd-peer --config=/tmp/kubeadm-etcd-cfg.yaml</span><br><span class="line">kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/kubeadm-etcd-cfg.yaml</span><br></pre></td></tr></table></figure><h4><span id="复制证书到其他两台主机对应目录"> 复制证书到其他两台主机对应目录</span></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pki/</span><br><span class="line">├── apiserver-etcd-client.crt</span><br><span class="line">├── apiserver-etcd-client.key</span><br><span class="line">└── etcd</span><br><span class="line">    ├── ca.crt</span><br><span class="line">    ├── ca.key</span><br><span class="line">    ├── peer.crt</span><br><span class="line">    ├── peer.key</span><br><span class="line">    ├── server.crt</span><br><span class="line">    └── server.key</span><br><span class="line"></span><br><span class="line">1 directory, 8 files</span><br></pre></td></tr></table></figure><h3><span id="3-启动etcd服务验证服务"> 3. 启动etcd服务,验证服务</span></h3><p>启动 etcd 服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable etcd</span><br><span class="line">systemctl start etcd</span><br><span class="line">systemctl status etcd</span><br></pre></td></tr></table></figure><p>验证服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">etcdctl \</span><br><span class="line">  --endpoints=https://$&#123;NODE_IP&#125;:2379  \</span><br><span class="line">  --ca-file=/etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">  --cert-file=/etc/kubernetes/pki/etcd/server.crt \</span><br><span class="line">  --key-file=/etc/kubernetes/pki/etcd/server.key \</span><br><span class="line">  cluster-health</span><br></pre></td></tr></table></figure><h2><span id="kubernetes-配置外部-etcd"> Kubernetes 配置外部 etcd</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">etcd:</span><br><span class="line">  external:</span><br><span class="line">    endpoints:</span><br><span class="line">    - https://etcd_HOST0:2379</span><br><span class="line">    - https://etcd_HOST1:2379</span><br><span class="line">    - https://etcd_HOST2:2379</span><br><span class="line">    caFile: /etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">    certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt</span><br><span class="line">    keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubeadm默认在由master节点上的kubelet管理的静态pod中运行单个成员etcd集群。这不是高可用性设置，因为etcd集群只包含一个成员，并且无法维持任何成员变得不可用。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 1.11.3 安装</title>
    <link href="yunke.science/2018/09/27/k8s-1-11-install/"/>
    <id>yunke.science/2018/09/27/k8s-1-11-install/</id>
    <published>2018-09-27T05:50:42.000Z</published>
    <updated>2018-09-27T05:54:15.024Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes 1.11.3 发布，容器编排工具</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#%E5%88%9D%E5%A7%8B%E5%8C%96">初始化</a><ul><li><a href="#%E8%AE%BE%E7%BD%AE%E4%B8%BB%E6%9C%BA%E5%90%8D">设置主机名</a></li><li><a href="#%E9%85%8D%E7%BD%AE-dns">配置 dns</a></li><li><a href="#%E5%81%9C%E6%AD%A2%E9%98%B2%E7%81%AB%E5%A2%99">停止防火墙</a></li><li><a href="#%E5%85%B3%E9%97%ADswap">关闭Swap</a></li><li><a href="#selinux">Selinux</a></li><li><a href="#%E9%85%8D%E7%BD%AE-dns-2">配置 dns</a></li><li><a href="#%E4%BF%AE%E6%94%B9%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0">修改内核参数</a></li></ul></li><li><a href="#k8s-rpm-install">k8s rpm install</a></li><li><a href="#%E5%85%B3%E4%BA%8E-kubelet-cgroup-driver">关于 kubelet cgroup driver</a></li><li><a href="#%E5%87%86%E5%A4%87%E9%95%9C%E5%83%8F">准备镜像</a></li><li><a href="#kube-proxy-%E4%BD%BF%E7%94%A8-ipvs-%E6%A8%A1%E5%BC%8F">kube-proxy 使用 ipvs 模式</a></li><li><a href="#kubeadm-init-%E5%88%9D%E5%A7%8B%E5%8C%96">Kubeadm Init 初始化</a></li><li><a href="#%E5%8D%B8%E8%BD%BD-kubeadm-%E5%AE%89%E8%A3%85">卸载 kubeadm 安装</a></li><li><a href="#%E9%AA%8C%E8%AF%81%E9%9B%86%E7%BE%A4">验证集群</a></li><li><a href="#%E5%AE%89%E8%A3%85calico%E7%BD%91%E7%BB%9C">安装calico网络</a></li><li><a href="#%E5%8E%BB%E9%99%A4%E5%AF%B9master%E7%9A%84%E8%B0%83%E5%BA%A6%E9%9A%94%E7%A6%BB">去除对master的调度隔离</a></li><li><a href="#%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1%E6%B5%8B%E8%AF%95">部署服务测试</a></li><li><a href="#hairpin-mode">Hairpin Mode</a></li><li><a href="#%E6%B7%BB%E5%8A%A0%E8%8A%82%E7%82%B9">添加节点</a></li></ul></p><h2><span id="初始化"> 初始化</span></h2><h3><span id="设置主机名"> 设置主机名</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl  set-hostname docker01</span><br></pre></td></tr></table></figure><h3><span id="配置-dns"> 配置 dns</span></h3><blockquote><p>echo nameserver 114.114.114.114&gt;&gt;/etc/resolv.conf</p></blockquote><h3><span id="停止防火墙"> 停止防火墙</span></h3><p>systemctl disable firewalld &amp;&amp; systemctl stop firewalld &amp;&amp; systemctl status firewalld</p><h3><span id="关闭swap"> 关闭Swap</span></h3><blockquote><p>swapoff -a<br>sed  ‘s/.<em>swap.</em>/#&amp;/’ /etc/fstab</p></blockquote><h3><span id="selinux"> Selinux</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ -f /etc/selinux/config ] &amp;&amp; sed -i &apos;s/^SELINUX=.*/SELINUX=disabled/g&apos; /etc/selinux/config</span><br><span class="line">[ -f /etc/sysconfig/selinux ] &amp;&amp; sed -i &apos;s/^SELINUX=.*/SELINUX=disabled/g&apos; /etc/sysconfig/selinux</span><br><span class="line">[ -x /usr/sbin/setenforce ] &amp;&amp; /usr/sbin/setenforce 0</span><br></pre></td></tr></table></figure><h3><span id="配置-dns"> 配置 dns</span></h3><blockquote><p>echo nameserver 114.114.114.114&gt;&gt;/etc/resolv.conf</p></blockquote><h3><span id="修改内核参数"> 修改内核参数</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">vm.swappiness=0</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sysctl -p /etc/sysctl.d/k8s.conf</span><br><span class="line"></span><br><span class="line">echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-iptables</span><br><span class="line">echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-ip6tables</span><br></pre></td></tr></table></figure><p><code>swapoff -a</code></p><h2><span id="k8s-rpm-install"> k8s rpm install</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg</span><br><span class="line">       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">指定版本下载安装：</span><br><span class="line">离线下载rpm包</span><br><span class="line">mkdir k8srpm &amp;&amp; cd k8srpm</span><br><span class="line">yum install -y yum-utils</span><br><span class="line">yumdownloader kubectl-1.11.3 kubelet-1.11.3 kubeadm-1.11.3  kubernetes-cni</span><br><span class="line"></span><br><span class="line">yum install  ./*.rpm -y</span><br><span class="line"></span><br><span class="line">systemctl enable kubelet.service</span><br></pre></td></tr></table></figure><h2><span id="关于-kubelet-cgroup-driver"> 关于 kubelet cgroup driver</span></h2><p>当前版本，使用Docker时，kubeadm会自动检测<code>kubelet</code>的cgroup驱动程序，并在运行时将其设置在<code>/var/lib/kubelet/kubeadm-flags.env</code>文件中。</p><p>因此，无需修改 kubelet cgroup driver</p><h2><span id="准备镜像"> 准备镜像</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">docker pull k8s.gcr.io/kube-apiserver-amd64:v1.11.3</span><br><span class="line">docker pull k8s.gcr.io/kube-controller-manager-amd64:v1.11.3</span><br><span class="line">docker pull k8s.gcr.io/kube-proxy-amd64:v1.11.3</span><br><span class="line">docker pull k8s.gcr.io/kube-scheduler-amd64:v1.11.3</span><br><span class="line">docker pull k8s.gcr.io/pause:3.1</span><br><span class="line">docker pull k8s.gcr.io/coredns:1.1.3</span><br><span class="line">docker pull quay.io/calico/node:v3.1.3</span><br><span class="line">docker pull quay.io/calico/cni:v3.1.3</span><br><span class="line">docker pull k8s.gcr.io/etcd-amd64:3.2.18</span><br></pre></td></tr></table></figure><h2><span id="kube-proxy-使用-ipvs-模式"> kube-proxy 使用 ipvs 模式</span></h2><ol><li>对于Kubernetes v1.10及更高版本，功能门默认 <code>SupportIPVSProxyMode</code>设置为<code>true</code>。但是，在v1.10之前版本用您需要 <code>--feature-gates=SupportIPVSProxyMode=true</code> 。</li><li>指定 <code>proxy-mode=ipvs</code></li><li>安装需要的内核模块和软件包</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">安装</span><br><span class="line">yum install ipvsadm -y</span><br><span class="line">加载</span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br><span class="line">检查</span><br><span class="line">cut -f1 -d &quot; &quot;  /proc/modules | grep -e ip_vs -e nf_conntrack_ipv4</span><br><span class="line">输出</span><br><span class="line">ip_vs_sh</span><br><span class="line">ip_vs_wrr</span><br><span class="line">ip_vs_rr</span><br><span class="line">nf_conntrack_ipv4</span><br><span class="line">ip_vs</span><br></pre></td></tr></table></figure><blockquote><p><a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/proxy/ipvs" target="_blank" rel="noopener">详细IPVS</a></p></blockquote><blockquote><p><a href="https://blog.csdn.net/cloudvtech/article/details/79942121" target="_blank" rel="noopener">kubernetes系列之六：安装kubernets v1.10.0和ipvs mode kube-proxy</a></p></blockquote><h2><span id="kubeadm-init-初始化"> Kubeadm Init 初始化</span></h2><p><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file" target="_blank" rel="noopener">将kubeadm init与配置文件一起使用</a></p><p><code>kubeadm init</code>使用配置文件而不是命令行标志进行配置，而某些更高级的功能可能仅作为配置文件选项提供。该文件在–config选项中传递。</p><p>在<code>Kubernetes 1.11</code>及更高版本中，可以使用<mark>kubeadm config print-default</mark>命令打印出默认配置 。这是建议您在旧迁移v1alpha1配置v1alpha2使用kubeadm配置迁移命令，因为v1alpha1会在Kubernetes 1.12被删除。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># cat kubeadm-init.conf</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">imageRepository: k8s.gcr.io</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">api:</span><br><span class="line">  advertiseAddress: 192.168.1.241</span><br><span class="line">  bindPort: 6443</span><br><span class="line">  controlPlaneEndpoint: &quot;&quot;</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: &quot;ipvs&quot;</span><br><span class="line">kubeletConfiguration:</span><br><span class="line">  baseConfig:</span><br><span class="line">    clusterDNS:</span><br><span class="line">    - 10.96.0.10</span><br><span class="line">    clusterDomain: cluster.local</span><br><span class="line">kubernetesVersion: v1.11.3</span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  podSubnet: &quot;10.42.0.0/16&quot;</span><br><span class="line">  serviceSubnet: 10.96.0.0/12</span><br></pre></td></tr></table></figure><blockquote><p>这里没有配置etcd ，<code>kubeadm init</code> 自动安装etcd。<br>Kubeadm默认在mater节点上的kubelet管理的静态pod中运行单节点的etcd集群。这不是高可用性设置，因为etcd集群只包含一个成员，并且无法维持任何成员变得不可用。</p></blockquote><p>以上主要修改2点：</p><ol><li>kubeProxy 启用 ipvs</li><li>指定 pod 子网段为 <code>10.42.0.0/16</code></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"># kubeadm init --config kubeadm-init.conf  </span><br><span class="line">[init] using Kubernetes version: v1.11.3</span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">I0920 15:57:53.117081   30624 kernel_validator.go:81] Validating kernel version</span><br><span class="line">I0920 15:57:53.117279   30624 kernel_validator.go:96] Validating kernel config</span><br><span class="line">[preflight/images] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight/images] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class="line">[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[preflight] Activating the kubelet service</span><br><span class="line">[certificates] Generated ca certificate and key.</span><br><span class="line">[certificates] Generated apiserver certificate and key.</span><br><span class="line">[certificates] apiserver serving cert is signed for DNS names [localhost.localdomain kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.241]</span><br><span class="line">[certificates] Generated apiserver-kubelet-client certificate and key.</span><br><span class="line">[certificates] Generated sa key and public key.</span><br><span class="line">[certificates] Generated front-proxy-ca certificate and key.</span><br><span class="line">[certificates] Generated front-proxy-client certificate and key.</span><br><span class="line">[certificates] Generated etcd/ca certificate and key.</span><br><span class="line">[certificates] Generated etcd/server certificate and key.</span><br><span class="line">[certificates] etcd/server serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [127.0.0.1 ::1]</span><br><span class="line">[certificates] Generated etcd/peer certificate and key.</span><br><span class="line">[certificates] etcd/peer serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [192.168.1.241 127.0.0.1 ::1]</span><br><span class="line">[certificates] Generated etcd/healthcheck-client certificate and key.</span><br><span class="line">[certificates] Generated apiserver-etcd-client certificate and key.</span><br><span class="line">[certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot;</span><br><span class="line">[controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;</span><br><span class="line">[controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span><br><span class="line">[controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span><br><span class="line">[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span><br><span class="line">[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot; </span><br><span class="line">[init] this might take a minute or longer if the control plane images have to be pulled</span><br><span class="line">[apiclient] All control plane components are healthy after 49.001944 seconds</span><br><span class="line">[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.11&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[markmaster] Marking the node localhost.localdomain as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class="line">[markmaster] Marking the node localhost.localdomain as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;localhost.localdomain&quot; as an annotation</span><br><span class="line">[bootstraptoken] using token: wadx1n.9gfusllys13qk1yj</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of machines by running the following on each node</span><br><span class="line">as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 192.168.1.241:6443 --token se4mgd.ukmg2hxxg42gt65t --discovery-token-ca-cert-hash sha256:0a737675e1e37aa4025077b27ced8053fe84c363df11c506bfb512b88408697e</span><br></pre></td></tr></table></figure><p>启动完毕</p><h2><span id="卸载-kubeadm-安装"> 卸载 kubeadm 安装</span></h2><p>要撤消kubeadm所执行的操作，您应首先排空节点，并确保节点在关闭之前为空。</p><p>使用适当的凭据与主人交谈，运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets</span><br><span class="line">kubectl delete node &lt;node name&gt;</span><br></pre></td></tr></table></figure><p>然后，在要删除的节点上，重置所有kubeadm安装状态：</p><p><code>kubeadm reset</code></p><p>如果您希望重新开始，只需运行<code>kubeadm init</code>或<code>kubeadm join</code>使用适当的参数。</p><h2><span id="验证集群"> 验证集群</span></h2><ol><li>按照提示，复制配置文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line"># kubectl  -n kube-system get po -o wide</span><br><span class="line">NAME                             READY     STATUS    RESTARTS   AGE       IP              NODE      NOMINATED NODE</span><br><span class="line">coredns-78fcdf6894-2p6gp         0/1       Pending   0          2m        &lt;none&gt;          &lt;none&gt;    &lt;none&gt;</span><br><span class="line">coredns-78fcdf6894-wnz4p         0/1       Pending   0          2m        &lt;none&gt;          &lt;none&gt;    &lt;none&gt;</span><br><span class="line">etcd-test01                      1/1       Running   0          1m        192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-apiserver-test01            1/1       Running   0          1m        192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-controller-manager-test01   1/1       Running   0          1m        192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-proxy-8wjp8                 1/1       Running   0          2m        192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-scheduler-test01            1/1       Running   0          1m        192.168.1.241   test01    &lt;none&gt;</span><br></pre></td></tr></table></figure><p>由于还没有部署网络，<code>coredns</code> 处于 <code>Pending</code> 状态。</p><ol start="2"><li>验证 cgroup-driver 与 docker cgroup-driver 是否匹配：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /var/lib/kubelet/kubeadm-flags.env</span><br><span class="line">KUBELET_KUBEADM_ARGS=--cgroup-driver=systemd --cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/cni/net.d --network-plugin=cni</span><br></pre></td></tr></table></figure><ol start="3"><li>验证 kube-proxy IPVS 模式</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># ipvsadm -L -n</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  10.96.0.1:443 rr</span><br><span class="line">  -&gt; 192.168.1.241:6443           Masq    1      0          0         </span><br><span class="line">TCP  10.96.0.10:53 rr</span><br><span class="line">UDP  10.96.0.10:53 rr</span><br></pre></td></tr></table></figure><h2><span id="安装calico网络"> 安装calico网络</span></h2><p><a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network" target="_blank" rel="noopener">安装pod网络附加组件</a></p><p>有关使用Calico的更多信息，请参阅Kubernetes上的 Calico 快速入门，为策略和网络安装Calico以及其他相关资源。</p><p>calico 默认pod网络为<code>192.168.0.0/16</code>, 上面kubeadm初始化时指定了 <code>networking.podSubnet: &quot;10.42.0.0/16&quot;</code> 。<br>这里需要修改<code>calico.yaml</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">加载rbac策略</span><br><span class="line">kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">修改 CALICO_IPV4POOL_CIDR</span><br><span class="line">wget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml</span><br><span class="line">sed -i &apos;s/192.168.0.0/10.42.0.0/g&apos; calico.yaml</span><br><span class="line">grep 0.0 calico.yaml </span><br><span class="line"></span><br><span class="line">kubectl apply -f calico.yaml</span><br></pre></td></tr></table></figure><p>查看容器状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># kubectl  -n kube-system get po -o wide</span><br><span class="line">NAME                             READY     STATUS    RESTARTS   AGE       IP              NODE      NOMINATED NODE</span><br><span class="line">calico-node-sfxvc                2/2       Running   0          32s       192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">coredns-78fcdf6894-2p6gp         1/1       Running   0          28m       10.42.0.2       test01    &lt;none&gt;</span><br><span class="line">coredns-78fcdf6894-wnz4p         1/1       Running   0          28m       10.42.0.3       test01    &lt;none&gt;</span><br><span class="line">etcd-test01                      1/1       Running   0          27m       192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-apiserver-test01            1/1       Running   0          27m       192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-controller-manager-test01   1/1       Running   0          27m       192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-proxy-8wjp8                 1/1       Running   0          28m       192.168.1.241   test01    &lt;none&gt;</span><br><span class="line">kube-scheduler-test01            1/1       Running   0          27m       192.168.1.241   test01    &lt;none&gt;</span><br></pre></td></tr></table></figure><p>增加了一个 <code>calico</code> 容器，<code>coredns</code> 启动成功。</p><p>查看 ipvs 列表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># ipvsadm -L -n</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  10.96.0.1:443 rr</span><br><span class="line">  -&gt; 192.168.1.241:6443           Masq    1      4          0         </span><br><span class="line">TCP  10.96.0.10:53 rr</span><br><span class="line">  -&gt; 10.42.0.2:53                 Masq    1      0          0         </span><br><span class="line">  -&gt; 10.42.0.3:53                 Masq    1      0          0         </span><br><span class="line">TCP  10.99.127.226:5473 rr</span><br><span class="line">UDP  10.96.0.10:53 rr</span><br><span class="line">  -&gt; 10.42.0.2:53                 Masq    1      0          0         </span><br><span class="line">  -&gt; 10.42.0.3:53                 Masq    1      0          0</span><br></pre></td></tr></table></figure><h2><span id="去除对master的调度隔离"> 去除对master的调度隔离</span></h2><p>默认情况下，出于安全原因，您的群集不会在<code>master</code>服务器上安排容器。如果您希望能够在<code>master</code>服务器上安排<code>pod</code>，例如，对于用于开发的单机<code>Kubernetes</code>集群，请运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubectl taint nodes --all node-role.kubernetes.io/master-</span><br><span class="line">node/test01 untainted</span><br></pre></td></tr></table></figure><p>这将从<code>node-role.kubernetes.io/master</code>包含主节点的任何节点中删除<code>taint</code>，这意味着调度程序将能够在任何地方安排<code>pod</code>。</p><h2><span id="部署服务测试"> 部署服务测试</span></h2><blockquote><p>cloudnativelabs/whats-my-ip 是一个每次请求，返回当前主机的主机名和IP的服务</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run myip --image=cloudnativelabs/whats-my-ip --replicas=3 --port=8080</span><br></pre></td></tr></table></figure><p>创建一个服务，类型为NodePort</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment myip --port=8080 --target-port=8080 --type=NodePort</span><br></pre></td></tr></table></figure><p>启动一个busybox镜像容器 用于测试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run tools --image=byrnedo/alpine-curl --replicas=1 --command -- sleep 99999999</span><br></pre></td></tr></table></figure><p>查看服务状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># kubectl  get po -o wide</span><br><span class="line">NAME                     READY     STATUS    RESTARTS   AGE       IP           NODE      NOMINATED NODE</span><br><span class="line">myip-5fc5cf6476-cfwjn    1/1       Running   0          15m       10.42.0.7    test01    &lt;none&gt;</span><br><span class="line">myip-5fc5cf6476-g55f7    1/1       Running   0          15m       10.42.0.8    test01    &lt;none&gt;</span><br><span class="line">myip-5fc5cf6476-wvxvt    1/1       Running   0          15m       10.42.0.6    test01    &lt;none&gt;</span><br><span class="line">tools-7c7dcbc894-74m6z   1/1       Running   0          1m        10.42.0.10   test01    &lt;none&gt;</span><br><span class="line"></span><br><span class="line">[root@test01 ~]# kubectl  get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          17h</span><br><span class="line">myip         NodePort    10.101.143.54   &lt;none&gt;        8080:30319/TCP   3m</span><br></pre></td></tr></table></figure><p>测试myip服务状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># kubectl  exec -it tools-7c7dcbc894-74m6z  /bin/sh</span><br><span class="line">测试 dns 是否正常解析</span><br><span class="line">/ # nslookup myip</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      myip</span><br><span class="line">Address 1: 10.101.143.54 myip.default.svc.cluster.local</span><br><span class="line">访问服务测试，连续访问十次</span><br><span class="line">/ # for i in $(seq 1 10);do curl myip:8080 ;done</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-cfwjn IP:10.42.0.7</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-wvxvt IP:10.42.0.6</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-g55f7 IP:10.42.0.8</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-cfwjn IP:10.42.0.7</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-wvxvt IP:10.42.0.6</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-g55f7 IP:10.42.0.8</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-cfwjn IP:10.42.0.7</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-wvxvt IP:10.42.0.6</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-g55f7 IP:10.42.0.8</span><br><span class="line">HOSTNAME:myip-5fc5cf6476-cfwjn IP:10.42.0.7</span><br></pre></td></tr></table></figure><h2><span id="hairpin-mode"> Hairpin Mode</span></h2><p>许多网络附加组件尚未启用<code>Hairpin</code>模式 ，<code>Hairpin</code>模式允许pod通过其服务IP访问自己。这是与CNI相关的问题 。请联系网络附加提供商以获取他们对发夹模式支持的最新状态。</p><p>calico 网络经过以上测试，pod可以通过其服务IP访问自己，无需配置 <code>Hairpin Mode</code> .</p><h2><span id="添加节点"> 添加节点</span></h2><p>节点是运行工作负载（容器和容器等）的位置。要向群集添加新节点，先执行以上的准备工作</p><p>然后执行 <code>master</code> 节点 <code>kubeadm init</code> 输出的最后一句, 即可加入集群：</p><p><code>kubeadm join 192.168.1.241:6443 --token se4mgd.ukmg2hxxg42gt65t --discovery-token-ca-cert-hash sha256:0a737675e1e37aa4025077b27ced8053fe84c363df11c506bfb512b88408697e</code></p><p><strong>如果不知道以上命令，怎么查询 token 和 discovery-token-ca-cert-has 呢？</strong></p><ol><li>如果没有<code>token</code>，执行以下查询获得</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># kubeadm token list</span><br><span class="line">TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION   EXTRA GROUPS</span><br><span class="line">se4mgd.ukmg2hxxg42gt65t   5h        2018-09-21T17:32:06+08:00   authentication,signing   &lt;none&gt;        system:bootstrappers:kubeadm:default-node-token</span><br></pre></td></tr></table></figure><ol start="2"><li>默认情况下，令牌在24小时后过期。如果在当前令牌过期后将节点加入群集，则可以通过在主节点上运行以下命令来创建新令牌：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubeadm token create</span><br><span class="line">ih6qhw.tbkp26l64xivcca7</span><br></pre></td></tr></table></figure><ol start="3"><li>如果没有<code>discovery-token-ca-cert-hash</code>,执行以下查询获得。<br><code>--discovery-token-ca-cert-hash</code> 的值可以配合多个<code>token</code>以重复使用。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | \</span><br><span class="line">&gt;    openssl dgst -sha256 -hex | sed &apos;s/^.* //&apos;</span><br><span class="line">0a737675e1e37aa4025077b27ced8053fe84c363df11c506bfb512b88408697e</span><br></pre></td></tr></table></figure><ol start="4"><li>通过创建新的token时，打印 join 命令：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubeadm token create --print-join-command</span><br><span class="line">kubeadm join 192.168.1.241:6443 --token 771jdx.k0jtjvzgfiu8k4uv --discovery-token-ca-cert-hash sha256:0a737675e1e37aa4025077b27ced8053fe84c363df11c506bfb512b88408697e</span><br></pre></td></tr></table></figure><p>几秒钟后，您应该注意到kubectl get nodes在master服务器上运行时输出中的此节点。</p><p><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/" target="_blank" rel="noopener">kubeadm join 详细</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes 1.11.3 发布，容器编排工具&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>CKA常见问题解答</title>
    <link href="yunke.science/2018/09/19/cka-faq/"/>
    <id>yunke.science/2018/09/19/cka-faq/</id>
    <published>2018-09-19T07:19:24.000Z</published>
    <updated>2018-09-19T07:55:56.695Z</updated>
    
    <content type="html"><![CDATA[<p>认证的Kubernetes管理员（CKA）和认证的Kubernetes应用程序开发人员（CKAD）常见问题解答</p><p>考试费用是多少？<br>考试费用为300美元。</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#%E8%AE%A4%E8%AF%81%E7%9A%84kubernetes%E7%AE%A1%E7%90%86%E5%91%98cka%E5%92%8C%E8%AE%A4%E8%AF%81%E7%9A%84kubernetes%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91%E4%BA%BA%E5%91%98ckad%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E7%AD%94">认证的Kubernetes管理员（CKA）和认证的Kubernetes应用程序开发人员（CKAD）常见问题解答</a><ul><li><a href="#%E8%80%83%E8%AF%95%E8%B4%B9%E7%94%A8%E6%98%AF%E5%A4%9A%E5%B0%91">考试费用是多少？</a></li><li><a href="#%E8%80%83%E8%AF%95%E9%9C%80%E8%A6%81%E5%A4%9A%E9%95%BF%E6%97%B6%E9%97%B4">考试需要多长时间？</a></li><li><a href="#%E8%80%83%E8%AF%95%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C">考试如何进行？</a></li><li><a href="#%E5%8F%82%E5%8A%A0%E8%80%83%E8%AF%95%E7%9A%84%E7%B3%BB%E7%BB%9F%E8%A6%81%E6%B1%82%E6%98%AF%E4%BB%80%E4%B9%88">参加考试的系统要求是什么？</a></li><li><a href="#%E6%88%91%E5%9C%A8%E8%80%83%E8%AF%95%E6%9C%9F%E9%97%B4%E5%8F%AF%E4%BB%A5%E8%AE%BF%E9%97%AE%E5%93%AA%E4%BA%9B%E8%B5%84%E6%BA%90">我在考试期间可以访问哪些资源？</a></li><li><a href="#%E6%88%91%E5%8F%AF%E4%BB%A5%E5%9C%A8%E8%80%83%E8%AF%95%E6%9C%9F%E9%97%B4%E5%81%9A%E7%AC%94%E8%AE%B0%E5%90%97">我可以在考试期间做笔记吗？</a></li><li><a href="#%E5%8F%82%E5%8A%A0%E8%80%83%E8%AF%95%E7%9A%84id%E8%A6%81%E6%B1%82%E6%98%AF%E4%BB%80%E4%B9%88">参加考试的ID要求是什么？</a></li><li><a href="#%E6%88%91%E7%9A%84%E8%80%83%E8%AF%95%E6%88%90%E7%BB%A9%E5%A6%82%E4%BD%95">我的考试成绩如何？</a></li><li><a href="#%E6%8F%90%E4%BE%9B%E4%BB%80%E4%B9%88%E8%AF%AD%E8%A8%80%E7%9A%84%E8%80%83%E8%AF%95">提供什么语言的考试？</a></li><li><a href="#%E6%88%91%E8%83%BD%E9%87%8D%E8%80%83%E5%90%97">我能重考吗？</a></li><li><a href="#%E6%88%91%E7%9A%84%E8%AE%A4%E8%AF%81%E6%9C%89%E6%95%88%E6%9C%9F%E5%A4%9A%E9%95%BF">我的认证有效期多长？</a></li><li><a href="#%E5%A6%82%E4%BD%95%E7%BB%AD%E8%AE%A2%E8%AE%A4%E8%AF%81">如何续订认证？</a></li><li><a href="#%E6%88%91%E5%A6%82%E4%BD%95%E4%BB%8E%E8%AE%A4%E8%AF%81%E4%B8%AD%E5%8F%97%E7%9B%8A">我如何从认证中受益？</a></li><li><a href="#%E6%88%91%E7%9A%84%E7%BB%84%E7%BB%87%E5%A6%82%E4%BD%95%E4%BB%8E%E4%B8%BA%E5%91%98%E5%B7%A5%E6%8F%90%E4%BE%9Bcka%E8%AE%A4%E8%AF%81%E4%B8%AD%E5%8F%97%E7%9B%8A">我的组织如何从为员工提供CKA认证中受益？</a></li><li><a href="#%E4%BB%80%E4%B9%88%E6%98%AFkubernetes%E8%AE%A4%E8%AF%81%E6%9C%8D%E5%8A%A1%E6%8F%90%E4%BE%9B%E5%95%86kcsp%E8%AE%A1%E5%88%92">什么是Kubernetes认证服务提供商（KCSP）计划？</a></li><li><a href="#%E6%98%AF%E5%90%A6%E6%9C%89%E5%9F%B9%E8%AE%AD%E5%87%86%E5%A4%87%E8%AE%A4%E8%AF%81%E8%80%83%E8%AF%95">是否有培训准备认证考试？</a></li></ul></li></ul></p><h2><span id="认证的kubernetes管理员cka和认证的kubernetes应用程序开发人员ckad常见问题解答"> 认证的Kubernetes管理员（CKA）和认证的Kubernetes应用程序开发人员（CKAD）常见问题解答</span></h2><h3><span id="考试费用是多少"> 考试费用是多少？</span></h3><p>考试费用为300美元。</p><h3><span id="考试需要多长时间"> 考试需要多长时间？</span></h3><p>根据候选人的经验水平，CKA考试大约需要3个小时才能完成，CKAD大约需要2个小时才能完成。</p><p>考生最多需要3小时（CKA）/ 2小时（CKAD）才能完成考试。</p><h3><span id="考试如何进行"> 考试如何进行？</span></h3><p>在考试期间通过流式音频，视频和屏幕共享源远程监控认证考试。屏幕共享需要允许监管人员查看候选人的桌面（包括所有监视器）。如果随后需要审核，则音频，视频和屏幕共享源将被存储一段有限的时间。</p><h3><span id="参加考试的系统要求是什么"> 参加考试的系统要求是什么？</span></h3><p>考试通过网络摄像头，音频和远程屏幕观看在线提供并由监管人员密切监控。考生必须提供自己的前端硬件才能参加考试，包括一台电脑：</p><ul><li>Chrome或Chromium浏览器</li><li>可靠的互联网接入</li><li>摄像头</li><li>麦克风</li></ul><p>除了所需的硬件之外，放置硬件（即台式机或笔记本电脑）的工作站必须露出干净的表面，顶部或下方没有障碍物。考生应确保他们的网络摄像头能够被移动，以防监考人员要求候选人平移以检查周围环境可能违反考试政策的情况。</p><p>考生应运行考试委托合作伙伴提供的兼容性检查工具，以验证其硬件是否符合最低要求。选择“Linux Foundation”作为考试赞助商，选择“CKA”或“CKAD”作为考试。（目前，仅支持Chrome和Chromium浏览器。）</p><h3><span id="我在考试期间可以访问哪些资源"> 我在考试期间可以访问哪些资源？</span></h3><p>候选人可以使用他们的<code>Chrome</code>或<code>Chromium</code>浏览器打开一个额外的标签，以便访问<a href="https://kubernetes.io/docs/" target="_blank" rel="noopener">https://kubernetes.io/docs/</a>及其子域名或<a href="https://kubernetes.io/blog/" target="_blank" rel="noopener">https://kubernetes.io/blog/</a>上的资源。不能打开其他选项卡，也不能导航到其他站点。<mark>上面允许的网站可能包含指向外部网站的链接，候选人有责任不点击任何导致他们导航到不允许的域的链接。</mark></p><h3><span id="我可以在考试期间做笔记吗"> 我可以在考试期间做笔记吗？</span></h3><p>是的，但仅使用可在考试控制台顶部菜单栏中访问的记事本功能。请注意，在考试结束后，此处输入的注释将不会被保留或访问。如果您需要帮助操作记事本，请询问您的监考人员。</p><h3><span id="参加考试的id要求是什么"> 参加考试的ID要求是什么？</span></h3><p>考生必须在考试开始前提供政府颁发的带照片的身份证明。任何包含候选人照片和拉丁字母中候选人姓名的未过期政府身份证可用于身份验证。政府签发的带照片的可接受形式的示例包括但不限于：</p><ul><li>护照</li><li>政府颁发的驾驶执照/许可证</li><li>国民身份证</li><li>州或省颁发的身份证</li></ul><p>请查看候选手册以获取更多信息。如果您对照片ID是否可以接受有疑问，请联系：certificationsupport@cncf.io。</p><p>要注册考试，您需要一个Linux Foundation ID。如果您还没有Linux Foundation ID，请在以下位置创建一个：<a href="https%EF%BC%9A//identity.linuxfoundation.org/">https：//identity.linuxfoundation.org/</a>。完成后，您可以在出现提示时使用新的Linux Foundation ID凭据登录。</p><h3><span id="我的考试成绩如何"> 我的考试成绩如何？</span></h3><p>考试通常在完成后24小时内自动评分。结果将在考试完成后36小时内通过电子邮件发送。结果也将在My Portal上提供。</p><p>考试成绩分级。在考试中执行目标的方法可能不止一种，除非另有说明，否则候选人可以选择任何可用的路径来执行目标，只要它产生正确的结果即可。</p><h3><span id="提供什么语言的考试"> 提供什么语言的考试？</span></h3><p>CKA和CKAD考试目前以英语授课。</p><h3><span id="我能重考吗"> 我能重考吗？</span></h3><p>对于直接从云计算本地计算基金会购买的考试，如果未达到及格分数并且候选人未被视为没有资格获得认证或重新考试，则每次考试购买将获得一（1）次免费重考。除非在考试中另有说明，否则<mark>免费重考必须在原始考试购买之日起12个月内完成</mark>。在免费重考已经用尽或者免费重考的截止日期已经过去之后，候选人可以注册并支付再次参加考试，而不会给予此类额外重考。</p><p>对于通过授权培训合作伙伴（ATP）进行的购买，请联系ATP以获取免费重考的资格。</p><h3><span id="我的认证有效期多长"> 我的认证有效期多长？</span></h3><p>Cloud Native Computing Foundation认证有效期为2年，候选人可通过填写下面的续订要求选项保持认证有效。续订要求必须在认证到期之前完成。</p><h3><span id="如何续订认证"> 如何续订认证？</span></h3><p>考生可以选择重考并通过相同的考试，以保证其证书有效。该认证将从重考和通过考试之日起2年内有效。</p><h3><span id="我如何从认证中受益"> 我如何从认证中受益？</span></h3><p>认证Kubernetes管理员（CKA）认证旨在确保认证持有者具备履行Kubernetes管理员职责的技能，知识和能力。CKA认证允许经过认证的管理员在就业市场中快速建立自己的信誉和价值，并允许公司更快地雇用高质量的团队来支持他们的发展。</p><p>认证Kubernetes应用程序开发人员（CKAD）认证旨在确保CKAD具备履行Kubernetes应用程序开发人员职责的技能，知识和能力。经过认证的<code>Kubernetes Application Developer</code>可以定义应用程序资源并使用核心原语来构建，监视和排除Kubernetes中可伸缩应用程序和工具的故障。</p><h3><span id="我的组织如何从为员工提供cka认证中受益"> 我的组织如何从为员工提供CKA认证中受益？</span></h3><p>拥有三名或更多CKA认证管理员的组织，Kubernetes社区中可演示的活动（包括积极贡献）和支持企业最终用户的商业模式都有资格成为Kubernetes认证服务提供商（KCSP）。</p><h3><span id="什么是kubernetes认证服务提供商kcsp计划"> 什么是Kubernetes认证服务提供商（KCSP）计划？</span></h3><p>KCSP计划是一个经过资格审核的预审服务提供商，他们在帮助企业成功采用Kubernetes方面拥有丰富的经验。您可以在此处了解有关该计划以及如何申请的更多信息。</p><h3><span id="是否有培训准备认证考试"> 是否有培训准备认证考试？</span></h3><p>是! 对于CKA，Linux基金会提供免费的<a href="https://training.linuxfoundation.org/linux-courses/system-administration-training/introduction-to-kubernetes" target="_blank" rel="noopener">Kubernetes入门课程</a>，该课程介绍了Kubernetes的许多关键概念。接下来的课程，<a href="https://training.linuxfoundation.org/linux-courses/system-administration-training/kubernetes-fundamentals" target="_blank" rel="noopener">Kubernetes基础知识（LFS258）</a>，以此介绍性材料为基础，直接达到Kubernetes认证管理员考试的要求。对于CKAD，<code>LFD259 - Kubernetes for Developers</code>即将推出。</p><blockquote><p>文章翻译自 <a href="https://www.cncf.io/certification/cka/faq/" target="_blank" rel="noopener">https://www.cncf.io/certification/cka/faq/</a></p></blockquote><p>2018.09.19</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;认证的Kubernetes管理员（CKA）和认证的Kubernetes应用程序开发人员（CKAD）常见问题解答&lt;/p&gt;
&lt;p&gt;考试费用是多少？&lt;br&gt;
考试费用为300美元。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>redis 安全</title>
    <link href="yunke.science/2018/08/28/redis-security/"/>
    <id>yunke.science/2018/08/28/redis-security/</id>
    <published>2018-08-28T03:08:58.000Z</published>
    <updated>2018-08-28T03:12:23.903Z</updated>
    
    <content type="html"><![CDATA[<p>本文档从Redis的角度介绍了安全性主题：Redis提供的访问控制，代码安全问题，可以通过选择恶意输入和其他类似主题从外部触发的攻击。</p><p>对于与安全相关的联系人，请在GitHub上打开一个问题，或者当您认为保持通信安全性非常重要时，请使用本文档末尾的GPG密钥。</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#redis%E9%80%9A%E7%94%A8%E5%AE%89%E5%85%A8%E6%A8%A1%E5%9E%8B">Redis通用安全模型</a></li><li><a href="#%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8">网络安全</a></li><li><a href="#%E4%BF%9D%E6%8A%A4%E6%A8%A1%E5%BC%8F">保护模式</a></li><li><a href="#%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81%E5%8A%9F%E8%83%BD">身份验证功能</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E5%AF%86%E6%94%AF%E6%8C%81">数据加密支持</a></li><li><a href="#%E7%A6%81%E7%94%A8%E7%89%B9%E5%AE%9A%E5%91%BD%E4%BB%A4">禁用特定命令</a></li><li><a href="#%E7%94%B1%E5%A4%96%E9%83%A8%E5%AE%A2%E6%88%B7%E7%B2%BE%E5%BF%83%E9%80%89%E6%8B%A9%E7%9A%84%E8%BE%93%E5%85%A5%E8%A7%A6%E5%8F%91%E7%9A%84%E6%94%BB%E5%87%BB">由外部客户精心选择的输入触发的攻击</a></li><li><a href="#%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E4%B9%89%E5%92%8Cnosql%E6%B3%A8%E5%85%A5">字符串转义和NoSQL注入</a></li><li><a href="#%E4%BB%A3%E7%A0%81%E5%AE%89%E5%85%A8%E6%80%A7">代码安全性</a></li><li><a href="#gpg%E5%AF%86%E9%92%A5">GPG密钥</a></li></ul></p><h2><span id="redis通用安全模型"> Redis通用安全模型</span></h2><p>Redis旨在由受信任环境中的受信任客户端访问。 这意味着通常不会将Redis实例直接暴露给Internet，或者是不受信任的客户端可以直接访问Redis TCP端口或UNIX套接字的环境。</p><p>例如，在使用Redis作为数据库，缓存或消息传递系统实现的Web应用程序的公共上下文中，应用程序前端（Web端）内的客户端将查询Redis以生成页面或执行请求的操作或由Web应用程序用户触发。</p><p>在这种情况下，Web应用程序连接Redis和不受信任的客户端（访问Web应用程序的用户浏览器）之间的访问。</p><p>这是一个具体示例，但是，通常，对Redis的不受信任访问应始终由实现ACL的层，验证用户输入以及决定对Redis实例执行哪些操作来调解。</p><p>一般而言，Redis未针对最大安全性进行优化，而是为了获得最佳性能和简单性。</p><h2><span id="网络安全"> 网络安全</span></h2><p>除了网络中受信任的客户端之外，每个人都应该拒绝访问Redis端口，因此运行Redis的服务器只能由使用Redis实现应用程序的计算机直接访问。</p><p>在直接暴露于互联网的单台计算机的常见情况下，例如虚拟化的Linux实例（Linode，EC2，…），应该对Redis端口进行防火墙以防止来自外部的访问。 客户端仍然可以使用 <code>localhost</code> 访问Redis。</p><p>请注意，可以通过在redis.conf文件中添加如下行来将Redis绑定到单个接口：</p><p><code>bind 127.0.0.1</code></p><p>由于Redis的性质，未能从外部保护Redis端口会产生很大的安全影响。 例如，外部攻击者可以使用单个<code>FLUSHALL</code>命令删除整个数据集。</p><h2><span id="保护模式"> 保护模式</span></h2><p>遗憾的是，许多用户无法保护Redis实例不被外部网络访问。 许多实例只是在公共IP上暴露在互联网上。 由于这个原因，<strong>从版本3.2.0开始，当使用默认配置（绑定所有接口）并且没有任何密码来执行Redis以访问它时，它进入称为保护模式的特殊模式 。</strong> 在此模式下，Redis仅回复来自环回接口的查询，并回复与其他地址连接的其他客户端的错误，解释正在发生的事情以及如何正确配置Redis。</p><p>我们希望保护模式能够严重降低因未经正确管理而执行的未受保护的Redis实例导致的安全问题，但系统管理员仍然可以忽略Redis提供的错误，只需禁用保护模式或手动绑定所有接口。</p><h2><span id="身份验证功能"> 身份验证功能</span></h2><p>虽然Redis没有尝试实现访问控制，但它提供了一个很小的身份验证层，可选择打开编辑<code>redis.conf</code>文件。</p><p>启用授权层后，Redis将拒绝未经身份验证的客户端的任何查询。 客户端可以通过发送<code>AUTH</code>命令后跟密码来验证自身。</p><p>密码由系统管理员以明文形式在redis.conf文件中设置。 它应该足够长以防止暴力攻击有两个原因：</p><ul><li>Redis在提供查询方面非常快。 每秒许多密码可以由外部客户端测试。</li><li>Redis密码存储在redis.conf文件内部和客户端配置中，因此不需要系统管理员记住，因此可能会很长。</li></ul><p>认证层的目标是可选地提供冗余层。 如果防火墙或任何其他用于保护Redis免受外部攻击者攻击的系统失败，外部客户端仍然无法在不知道验证密码的情况下访问Redis实例。</p><p>与其他所有Redis命令一样，AUTH命令以未加密的方式发送，因此它不能防止有足够访问权限的攻击者执行窃听。</p><h2><span id="数据加密支持"> 数据加密支持</span></h2><p>Redis不支持加密。 为了实现受信任方可以通过Internet或其他不受信任的网络访问Redis实例的设置，应该实施额外的保护层，例如SSL代理。 我们建议使用<a href="https://github.com/Tarsnap/spiped" target="_blank" rel="noopener">spiped</a> 。</p><h2><span id="禁用特定命令"> 禁用特定命令</span></h2><p><strong>可以在Redis中禁用命令或将它们重命名为不可授权的名称，以便普通客户端仅限于指定的命令集。</strong></p><p>例如，虚拟化服务器提供商可以提供托管的Redis实例服务。 在这种情况下，普通用户可能无法调用<code>Redis CONFIG</code>命令来更改实例的配置，但提供和删除实例的系统应该能够这样做。</p><p>在这种情况下，可以命令重命名或完全隐藏命令。 此功能作为可在<code>redis.conf</code>配置文件中使用的语句提供。 例如：</p><p><code>rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52</code></p><p>在上面的示例中， CONFIG命令已重命名为不可引用的名称。 也可以通过将其重命名为空字符串来完全禁用它（或任何其他命令），如下例所示：</p><p><code>rename-command CONFIG &quot;&quot;</code></p><h2><span id="由外部客户精心选择的输入触发的攻击"> 由外部客户精心选择的输入触发的攻击</span></h2><p>即使没有外部访问实例，攻击者也可以从外部触发一类攻击。 这种攻击的一个例子是能够将数据插入Redis，从而触发Redis内部实现的数据结构的病态（最坏情况）算法复杂性。</p><p>例如，攻击者可以通过Web表单提供一组字符串，这些字符串已知为同一个桶散列到哈希表中，以便将O（1）预期时间（平均时间）转换为O（N ）最糟糕的情况是，消耗的CPU比预期的多，最终导致拒绝服务。</p><p>为防止此特定攻击，Redis对散列函数使用每执行伪随机种子。</p><p>Redis使用qsort算法实现SORT命令。 目前，该算法不是随机的，因此可以通过仔细选择正确的输入集来触发二次最坏情况行为。</p><h2><span id="字符串转义和nosql注入"> 字符串转义和NoSQL注入</span></h2><p>Redis协议没有字符串转义的概念，因此在正常情况下使用普通客户端库无法注入。 该协议使用前缀长度字符串，完全是二进制安全的。</p><p>由EVAL和EVALSHA命令执行的Lua脚本遵循相同的规则，因此这些命令也是安全的。</p><p>虽然这将是一个非常奇怪的用例，但应用程序应该避免使用从不受信任的源获取的字符串来组成Lua脚本的主体。</p><h2><span id="代码安全性"> 代码安全性</span></h2><p>在传统的Redis设置中，允许客户端完全访问命令集，但访问实例永远不会导致能够控制运行Redis的系统。</p><p>在内部，Redis使用所有众所周知的实践来编写安全代码，防止缓冲区溢出，格式化错误和其他内存损坏问题。 但是，使用CONFIG命令控制服务器配置的能力使客户端能够更改程序的工作目录和转储文件的名称。 这允许客户端在随机路径上编写RDB Redis文件，这是一个安全问题 ，可能很容易导致破坏系统和/或运行不受信任的代码，因为Redis正在运行相同的用户。</p><p>Redis不需要root权限即可运行。 <strong>建议将其作为无特权的redis用户运行</strong>，仅用于此目的。 Redis作者目前正在研究添加新配置参数以防止<code>CONFIG SET / GET</code>目录和其他类似运行时配置指令的可能性。 这将阻止客户端强制服务器在任意位置写入Redis转储文件。</p><h2><span id="gpg密钥"> GPG密钥</span></h2><blockquote><p>翻译自： <a href="https://redis.io/topics/security" target="_blank" rel="noopener">https://redis.io/topics/security</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文档从Redis的角度介绍了安全性主题：Redis提供的访问控制，代码安全问题，可以通过选择恶意输入和其他类似主题从外部触发的攻击。&lt;/p&gt;
&lt;p&gt;对于与安全相关的联系人，请在GitHub上打开一个问题，或者当您认为保持通信安全性非常重要时，请使用本文档末尾的GPG密钥。&lt;/p&gt;
    
    </summary>
    
      <category term="redis" scheme="yunke.science/categories/redis/"/>
    
    
      <category term="redis" scheme="yunke.science/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>使用NGINX 1.13.9引入HTTP2服务器推送</title>
    <link href="yunke.science/2018/08/27/http2-server-push/"/>
    <id>yunke.science/2018/08/27/http2-server-push/</id>
    <published>2018-08-27T08:03:16.000Z</published>
    <updated>2018-08-27T08:25:26.683Z</updated>
    
    <content type="html"><![CDATA[<p>2018年2月20日发布的NGINX 1.13.9包括对HTTP/2 服务器推送的支持。<br>在 HTTP/2 规范中定义的服务器推送允许服务器提前将资源推送到远程客户端，预期客户端可能很快就会请求这些资源。 通过这样做，您可以在一个RTT或更多RTT页面加载操作中减少RTT的数量（往返时间 - 请求和响应所需的时间），从而为用户提供更快的响应。</p><p>服务器推送可用于为客户端填充style sheets，图片和呈现网页所需的其他资源。 你只需要注意只推送所需的资源，不要推送客户端可能已经缓存的资源。</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#%E9%85%8D%E7%BD%AE-http2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A8%E9%80%81">配置 <code>HTTP/2</code> 服务器推送</a></li><li><a href="#%E9%AA%8C%E8%AF%81-http2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A8%E9%80%81">验证 <code>HTTP/2</code> 服务器推送</a><ul><li><a href="#%E4%BD%BF%E7%94%A8%E6%B5%8F%E8%A7%88%E5%99%A8%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7">使用浏览器开发工具</a></li><li><a href="#%E4%BD%BF%E7%94%A8%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%AA%8C%E8%AF%81nghttp">使用命令行客户端验证(nghttp)</a></li></ul></li><li><a href="#%E8%87%AA%E5%8A%A8%E5%90%91%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%8E%A8%E9%80%81%E8%B5%84%E6%BA%90">自动向客户端推送资源</a></li><li><a href="#%E6%9C%89%E9%80%89%E6%8B%A9%E5%9C%B0%E5%B0%86%E8%B5%84%E6%BA%90%E6%8E%A8%E9%80%81%E7%BB%99%E5%AE%A2%E6%88%B7">有选择地将资源推送给客户</a></li><li><a href="#%E6%B5%8B%E8%AF%95http2%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8E%A8%E9%80%81%E7%9A%84%E6%95%88%E6%9E%9C">测试HTTP/2服务器推送的效果</a><ul><li><a href="#%E7%BB%93%E8%AE%BA">结论</a></li></ul></li><li><a href="#%E9%99%84%E5%BD%95http2%E6%8E%A8%E9%80%81%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C">附录：<code>HTTP/2</code>推送如何工作？</a></li></ul></p><h2><span id="配置-http2-服务器推送"> 配置 <code>HTTP/2</code> 服务器推送</span></h2><p>要将资源与页面加载一起推送，请使用http2_push指令，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    # Ensure that HTTP/2 is enabled for the server        </span><br><span class="line">    listen 443 ssl http2;</span><br><span class="line"></span><br><span class="line">    ssl_certificate ssl/certificate.pem;</span><br><span class="line">    ssl_certificate_key ssl/key.pem;</span><br><span class="line"></span><br><span class="line">    root /var/www/html;</span><br><span class="line"></span><br><span class="line">    # whenever a client requests demo.html, also push</span><br><span class="line">    # /style.css, /image1.jpg and /image2.jpg</span><br><span class="line">    location = /demo.html &#123;</span><br><span class="line">        http2_push /style.css;</span><br><span class="line">        http2_push /image1.jpg;</span><br><span class="line">        http2_push /image2.jpg;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="验证-http2-服务器推送"> 验证 <code>HTTP/2</code> 服务器推送</span></h2><h3><span id="使用浏览器开发工具"> 使用浏览器开发工具</span></h3><p>以Google Chrome浏览器为例， 在图中，Chrome开发者工具的“ 网络”选项卡上的“ 启动器”列表示，作为/demo.html请求的一部分，已将多个资源推送到客户端。</p><p><img src="https://cdn-1.wp.nginx.com/wp-content/uploads/2018/02/http2-server-push-chrome-dev-tools.png" alt="image"></p><h3><span id="使用命令行客户端验证nghttp"> 使用命令行客户端验证(nghttp)</span></h3><p>除了Web浏览器工具之外，您还可以使用nghttp2.org项目中的nghttp命令行客户端来验证服务器推送是否有效。 您可以从 <a href="https://github.com/nghttp2/nghttp2" target="_blank" rel="noopener">GitHub</a> 下载nghttp命令行客户端，或者在可用的情况下安装相应的操作系统软件包。 对于Ubuntu，请使用nghttp2-client软件包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ nghttp -ans https://example.com/demo.html</span><br><span class="line">id  responseEnd requestStart  process code size request path</span><br><span class="line"> 13    +84.25ms       +136us  84.11ms  200  492 /demo.html</span><br><span class="line">  2    +84.33ms *   +84.09ms    246us  200  266 /style.css</span><br><span class="line">  4   +261.94ms *   +84.12ms 177.83ms  200  40K /image2.jpg</span><br><span class="line">  6   +685.95ms *   +84.12ms 601.82ms  200 173K /image1.jpg</span><br></pre></td></tr></table></figure><h2><span id="自动向客户端推送资源"> 自动向客户端推送资源</span></h2><p>在许多情况下，列出您希望在 <code>NGINX</code> 配置文件中推送的资源是不方便的 - 甚至是不可能的。 出于这个原因，NGINX还支持拦截Link预加载 <code>header</code> 的约定，然后推送这些 <code>header</code> 中标识的资源。 要启用预加载，请在配置中包含 <code>http2_push_preload</code> 指令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    # Ensure that HTTP/2 is enabled for the server        </span><br><span class="line">    listen 443 ssl http2;</span><br><span class="line"></span><br><span class="line">    ssl_certificate ssl/certificate.pem;</span><br><span class="line">    ssl_certificate_key ssl/key.pem;</span><br><span class="line"></span><br><span class="line">    root /var/www/html;</span><br><span class="line"></span><br><span class="line">    # Intercept Link header and initiate requested Pushes</span><br><span class="line">    location = /myapp &#123;</span><br><span class="line">        proxy_pass http://upstream;</span><br><span class="line">        http2_push_preload on;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>例如，当NGINX作为proxy（用于HTTP，FastCGI或其他流量类型）运行时，上游服务器可以将这样的Link头添加到其响应中：</p><p><code>Link: &lt;/style.css&gt;; as=style; rel=preload</code></p><p>NGINX拦截此标头并开始服务器推送<code>/style.css</code> 。 Link头中的路径必须是绝对路径 - 不支持<code>./style.css</code>之类的相对路径。 该路径可以选择包括查询字符串。</p><p>要推送多个对象，您可以提供多个Link标题，或者更好的是，将所有对象包含在以<mark>逗号</mark>分隔的列表中：</p><p><code>Link: &lt;/style.css&gt;; as=style; rel=preload, &lt;/favicon.ico&gt;; as=image; rel=preload</code></p><p>如果您不希望NGINX推送预加载的资源，请将<code>nopush</code>参数添加到标头中：</p><p><code># Resource is not pushed Link: &lt;/nginx.png&gt;; as=image; rel=preload; nopush</code></p><p>启用<code>http2_push_preload</code> ，您还可以通过在NGINX配置中设置响应头来启动预加载服务器推送：</p><p><code>add_header Link &quot;&lt;/style.css&gt;; as=style; rel=preload&quot;;</code></p><h2><span id="有选择地将资源推送给客户"> 有选择地将资源推送给客户</span></h2><p><code>HTTP/2</code>规范没有解决确定是否推送资源的挑战。 显然，如果您知道他们可能需要资源并且他们不太可能已经缓存了资源，那么最好将资源推送给客户端。</p><p>一种可能的方法是仅在首次访问网站时将资源推送到客户端。 例如，您可以测试是否存在会话cookie，并有条件地设置Link头，因此仅在会话cookie不存在时才预加载资源。</p><p>假设客户端表现良好并在后续请求中包含cookie，使用以下配置，NGINX仅在每个浏览器会话中将资源推送到客户端一次：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 443 ssl http2 default_server;</span><br><span class="line"></span><br><span class="line">    ssl_certificate ssl/certificate.pem;</span><br><span class="line">    ssl_certificate_key ssl/key.pem;</span><br><span class="line"></span><br><span class="line">    root /var/www/html;</span><br><span class="line">    http2_push_preload on;</span><br><span class="line"></span><br><span class="line">    location = /demo.html &#123;</span><br><span class="line">        add_header Set-Cookie &quot;session=1&quot;;</span><br><span class="line">        add_header Link $resources;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">map $http_cookie $resources &#123;</span><br><span class="line">    &quot;~*session=1&quot; &quot;&quot;;</span><br><span class="line">    default &quot;&lt;/style.css&gt;; as=style; rel=preload, &lt;/image1.jpg&gt;; as=image; rel=preload, </span><br><span class="line">             &lt;/image2.jpg&gt;; as=style; rel=preload&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="测试http2服务器推送的效果"> 测试HTTP/2服务器推送的效果</span></h2><p>为了衡量服务器推送的效果，我们创建了一个简单的测试页面/demo.html ，它引用了一个单独的样式表/style.css 。 样式表进一步引用了两个图片。 我们使用三种不同的配置测试了页面加载时间：</p><ul><li>顺序GET （无优化） - 浏览器在发现需要时加载资源</li><li>预加载提示 - 预加载提示（ <code>Link headers</code>）包含在第一个响应中，告诉浏览器加载依赖项</li><li>服务器推送 （仅限<code>HTTP/2</code>） - 依赖性被抢先推送到浏览器</li></ul><p><img src="https://cdn-1.wp.nginx.com/wp-content/uploads/2018/02/http2-server-push-test-configurations.png" alt="image"></p><p>我们使用HTTP，HTTPS或<code>HTTP/2</code>对每个配置进行了多次测试运行。 前两个配置适用于所有三个协议，服务器仅适用于 <code>HTTP/2</code>。</p><p>使用Chrome开发人员工具测量行为。 评估和平均每种配置的最常见行为，并将时间与链路的RTT（使用ping测量）相关联，以说明每种方法的机械效果。</p><p><img src="https://cdn-1.wp.nginx.com/wp-content/uploads/2018/02/http2-server-push-testing-results.png" alt="image"></p><h3><span id="结论"> 结论</span></h3><p>此测试非常简单，以突出预加载提示和服务器推送的机制。 在简单的情况下，服务器推送比预加载提示多1-RTT改进，与未优化的顺序GET请求和相关资源的发现相比，具有更大的改进。</p><p>更现实的用例具有更多变量：多个依赖资源，多个源，甚至通过推送已经缓存或不立即需要的资源来浪费带宽的可能性。 浏览器不一致也会影响性能。 您的结果肯定会因这个简单的测试而异。</p><p>例如，Chrome团队已发布了有关何时部署服务器推送的一些详细建议，并已在更复杂的站点上进行测量，以比较无优化，预加载提示和服务器推送<code>HTTP/2</code>的影响。 对于考虑在生产中部署<code>HTTP/2</code>服务器推送的任何人来说，他们的<code>HTTP/2</code>推送报告的规则值得阅读。</p><p>实用的结论是，如果您可以提前确定需要哪些资源，那么让上游服务器发送预加载提示确实有好处。 推动这些资源的额外好处很小但可以衡量，但可能会导致带宽浪费和所需资源的延迟。 您应该仔细测试和监视任何服务器推送配置。</p><h2><span id="附录http2推送如何工作"> 附录：<code>HTTP/2</code>推送如何工作？</span></h2><p><code>HTTP/2</code>服务器推送通常用于在客户端请求资源时抢先发送相关资源。 例如，如果客户端请求网页，则服务器可以将依赖的样式表，字体和图像推送到客户端。</p><p>当客户端建立<code>HTTP/2</code>连接时，服务器可以选择通过连接发起一个或多个服务器推送响应。 这些推送发送客户端未明确请求的资源。</p><p>客户端可以拒绝推送（通过发送RST_STREAM帧）或接受它。 客户端将推送的内容存储在与<code>HTTP/2</code>连接相关联的本地“推送缓存”中。</p><p>稍后，当客户端使用已建立的<code>HTTP/2</code>连接请求资源时，它会检查连接的推送缓存，以查找对请求的已完成或正在传输的响应。 它优先使用缓存资源来为资源发出新的<code>HTTP/2</code>请求。</p><p>任何推送的资源都保留在每个连接的推送缓存中，直到（a）使用它或（b）<code>HTTP/2</code>连接关闭：</p><p>如果使用资源，则客户端将获取副本，并删除推送缓存中的条目。 如果资源是可缓存的，则客户端可以将其副本缓存在其HTTP页面缓存中。<br>如果<code>HTTP/2</code>连接因任何原因而关闭，则会删除其本地推送缓存。<br>这有几个含义：</p><p>即使推送的内容更新鲜，浏览器中HTTP页面缓存中的内容也优先于推送缓存中的内容使用。<br><code>HTTP/2</code>连接可以在不同的页面加载之间共享。 在不同页面加载中请求时，可以使用由于一个页面加载而推送的资源。<br>具有凭据的请求使用与没有凭据的请求不同的<code>HTTP/2</code>连接; 例如，如果浏览器对资源发出非凭证请求，则可能找不到使用跨源请求（凭证）推送的资源。<br>您可以在Jake Archibald的<code>HTTP/2</code>推送中查看更详细的问题列表，这比我认为的博客文章更难 。</p><p><code>HTTP/2</code>服务器推送是一项有趣的功能。 确保彻底测试您的<code>HTTP/2</code>服务器推送配置，并准备回退到预加载提示，以防这提供更可预测的，缓存感知行为。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2018年2月20日发布的NGINX 1.13.9包括对HTTP/2 服务器推送的支持。&lt;br&gt;
在 HTTP/2 规范中定义的服务器推送允许服务器提前将资源推送到远程客户端，预期客户端可能很快就会请求这些资源。 通过这样做，您可以在一个RTT或更多RTT页面加载操作中减少RTT的数量（往返时间 - 请求和响应所需的时间），从而为用户提供更快的响应。&lt;/p&gt;
&lt;p&gt;服务器推送可用于为客户端填充style sheets，图片和呈现网页所需的其他资源。 你只需要注意只推送所需的资源，不要推送客户端可能已经缓存的资源。&lt;/p&gt;
    
    </summary>
    
      <category term="nginx" scheme="yunke.science/categories/nginx/"/>
    
    
      <category term="nginx" scheme="yunke.science/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title>kubectl概述</title>
    <link href="yunke.science/2018/07/10/kubectl-overview/"/>
    <id>yunke.science/2018/07/10/kubectl-overview/</id>
    <published>2018-07-10T07:14:04.000Z</published>
    <updated>2018-07-10T07:15:39.316Z</updated>
    
    <content type="html"><![CDATA[<p><code>kubectl</code> 是一个命令行界面，用于运行针对Kubernetes集群的命令。 本概述介绍kubectl语法，描述命令操作，并提供常见示例。</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#%E8%AF%AD%E6%B3%95%E8%AF%B4%E6%98%8E">语法说明</a></li><li><a href="#%E6%93%8D%E4%BD%9C">操作</a></li><li><a href="#%E8%B5%84%E6%BA%90%E7%B1%BB%E5%9E%8B">资源类型</a></li><li><a href="#%E8%BE%93%E5%87%BA%E9%80%89%E9%A1%B9">输出选项</a><ul><li><a href="#formatting-output">Formatting output</a><ul><li><a href="#%E8%AF%AD%E6%B3%95">语法</a><ul><li><a href="#%E7%A4%BA%E4%BE%8B">示例</a></li></ul></li><li><a href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%97">自定义列</a><ul><li><a href="#%E7%A4%BA%E4%BE%8B-2">示例</a></li></ul></li><li><a href="#%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E5%88%97">服务器端列</a><ul><li><a href="#%E7%A4%BA%E4%BE%8B-3">示例</a></li></ul></li></ul></li><li><a href="#%E6%8E%92%E5%BA%8F%E5%88%97%E8%A1%A8%E5%AF%B9%E8%B1%A1">排序列表对象</a><ul><li><a href="#%E8%AF%AD%E6%B3%95-2">语法</a><ul><li><a href="#%E7%A4%BA%E4%BE%8B-4">示例</a></li></ul></li></ul></li></ul></li><li><a href="#%E7%A4%BA%E4%BE%8B%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C">示例：常见操作</a></li></ul></p><h2><span id="语法说明"> 语法说明</span></h2><p>使用以下语法从终端窗口运行 <code>kubectl</code> 命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl [command] [TYPE] [NAME] [flags]</span><br></pre></td></tr></table></figure><p>其中<code>command</code>，<code>TYPE</code>，<code>NAME</code>和<code>flags</code>是：</p><p>*<code>command</code>：指定要对一个或多个资源执行的操作，例如<code>create</code>，<code>get</code>，<code>describe</code>，<code>delete</code>。</p><p>*<code>TYPE</code>：指定[资源类型]（#resource-types）。 资源类型不区分大小写，您可以指定单数，复数或缩写形式。 例如，以下命令产生相同的输出：</p><pre><code>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod pod1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods pod1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po pod1</span></span><br></pre></td></tr></table></figure></code></pre><ul><li><p><code>NAME</code>：指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息，例如<code>$ kubectl get pods</code>。</p><p>在多个资源上执行操作时，您可以按类型和名称指定每个资源，或指定一个或多个文件：</p><ul><li><p>按类型和名称指定资源:</p><ul><li><p>如果资源类型相同，则对资源进行分组：<code>TYPE1 name1 name2 name&lt;#&gt;</code>。<br><br>示例：<code>$ kubectl get pod example-pod1 example-pod2</code></p></li><li><p>分别指定多种资源类型：<code>TYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE&lt;#&gt;/name&lt;#&gt;</code>。<br><br>示例：<code>$ kubectl get pod/example-pod1 replicationcontroller/example-rc1</code></p></li></ul></li><li><p>使用一个或多个文件指定资源：<code>-f file1 -f file2 -f file&lt;#&gt;</code></p><ul><li>[使用YAML而不是JSON]（/ docs / concepts / configuration / overview /＃general-config-tips），因为YAML往往更加用户友好，特别是对于配置文件。<br>示例：<code>$ kubectl get pod -f ./pod.yaml</code></li></ul></li></ul></li><li><p><code>flags</code>：指定可选标志。 例如，您可以使用<code>-s</code>或<code>--server</code>标志来指定Kubernetes API服务器的地址和端口。<br><br><strong>重要</strong>：您从命令行指定的标志会覆盖默认值和任何相应的环境变量。</p></li></ul><p>如果您需要帮助，只需从终端窗口运行<code>kubectl help</code>即可。</p><h2><span id="操作"> 操作</span></h2><p>下表包含所有 <code>kubectl</code> 操作的简短描述和一般语法：</p><table><thead><tr><th>操作</th><th>语法</th><th>描述</th></tr></thead><tbody><tr><td><code>annotate</code></td><td><code>kubectl annotate (-f FILENAME \| TYPE NAME \| TYPE/NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--overwrite] [--all] [--resource-version=version] [flags]</code></td><td>添加或更新一个或多个资源的注释。</td></tr><tr><td><code>api-versions</code></td><td><code>kubectl api-versions [flags]</code></td><td>列出可用的API版本。</td></tr><tr><td><code>apply</code></td><td><code>kubectl apply -f FILENAME [flags]</code></td><td>将配置更改应用于文件或stdin中的资源。</td></tr><tr><td><code>attach</code></td><td><code>kubectl attach POD -c CONTAINER [-i] [-t] [flags]</code></td><td>附加到正在运行的容器，以查看输出流或与容器（stdin）进行交互。</td></tr><tr><td><code>autoscale</code></td><td><code>kubectl autoscale (-f FILENAME \| TYPE NAME \| TYPE/NAME) [--min=MINPODS] --max=MAXPODS [--cpu-percent=CPU] [flags]</code></td><td>自动缩放由复制控制器管理的pod集。</td></tr><tr><td><code>cluster-info</code></td><td><code>kubectl cluster-info [flags]</code></td><td>显示有关群集中主服务器和服务的端点信息。</td></tr><tr><td><code>config</code></td><td><code>kubectl config SUBCOMMAND [flags]</code></td><td>修改kubeconfig文件。 有关详细信息，请参阅各个子命令。</td></tr><tr><td><code>create</code></td><td><code>kubectl create -f FILENAME [flags]</code></td><td>从文件或终端输入创建资源</td></tr><tr><td><code>delete</code></td><td><code>kubectl delete (-f FILENAME \| TYPE [NAME \| /NAME \| -l label \| --all]) [flags]</code></td><td>从文件，标准输入或指定标签选择器，名称，资源选择器或资源中删除资源。</td></tr><tr><td><code>describe</code></td><td><code>kubectl describe (-f FILENAME \| TYPE [NAME_PREFIX \| /NAME \| -l label]) [flags]</code></td><td>显示资源详细信息</td></tr><tr><td><code>edit</code></td><td><code>kubectl edit (-f FILENAME \| TYPE NAME \| TYPE/NAME) [flags]</code></td><td>更新一个或多个资源，使用默认的编辑器</td></tr><tr><td><code>exec</code></td><td><code>kubectl exec POD [-c CONTAINER] [-i] [-t] [flags] [-- COMMAND [args...]]</code></td><td>在pod 容器中执行命令</td></tr><tr><td><code>explain</code></td><td><code>kubectl explain [--include-extended-apis=true] [--recursive=false] [flags]</code></td><td>获取各种资源的文档。 例如pod，节点，服务等。</td></tr><tr><td><code>expose</code></td><td><code>kubectl expose (-f FILENAME \| TYPE NAME \| TYPE/NAME) [--port=port] [--protocol=TCP\|UDP] [--target-port=number-or-name] [--name=name] [----external-ip=external-ip-of-service] [--type=type] [flags]</code></td><td>将复制控制器，服务或pod公开为新的Kubernetes服务。</td></tr><tr><td><code>get</code></td><td><code>kubectl get (-f FILENAME \| TYPE [NAME \| /NAME \| -l label]) [--watch] [--sort-by=FIELD] [[-o \| --output]=OUTPUT_FORMAT] [flags]</code></td><td>列出一个或多个资源</td></tr><tr><td><code>label</code></td><td><code>kubectl label (-f FILENAME \| TYPE NAME \| TYPE/NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--overwrite] [--all] [--resource-version=version] [flags]</code></td><td>增加或者更新资源标签</td></tr><tr><td><code>logs</code></td><td><code>kubectl logs POD [-c CONTAINER] [--follow] [flags]</code></td><td>打印pod 容器的日志</td></tr><tr><td><code>patch</code></td><td><code>kubectl patch (-f FILENAME \| TYPE NAME \| TYPE/NAME) --patch PATCH [flags]</code></td><td>使用策略合并修补程序更新资源的一个或多个字段。</td></tr><tr><td><code>port-forward</code></td><td><code>kubectl port-forward POD [LOCAL_PORT:]REMOTE_PORT [...[LOCAL_PORT_N:]REMOTE_PORT_N] [flags]</code></td><td>转发本地端口到一个pod</td></tr><tr><td><code>proxy</code></td><td><code>kubectl proxy [--port=PORT] [--www=static-dir] [--www-prefix=prefix] [--api-prefix=prefix] [flags]</code></td><td>运行代理到Kubernetes API服务器。</td></tr><tr><td><code>replace</code></td><td><code>kubectl replace -f FILENAME</code></td><td>从文件或标准输入中替换资源。</td></tr><tr><td><code>rolling-update</code></td><td><code>kubectl rolling-update OLD_CONTROLLER_NAME ([NEW_CONTROLLER_NAME] --image=NEW_CONTAINER_IMAGE \| -f NEW_CONTROLLER_SPEC) [flags]</code></td><td>通过逐步替换指定的复制控制器及其pod来执行滚动更新。</td></tr><tr><td><code>run</code></td><td><code>kubectl run NAME --image=image [--env=&quot;key=value&quot;] [--port=port] [--replicas=replicas] [--dry-run=bool] [--overrides=inline-json] [flags]</code></td><td>在群集上运行指定的映像。</td></tr><tr><td><code>scale</code></td><td><code>kubectl scale (-f FILENAME \| TYPE NAME \| TYPE/NAME) --replicas=COUNT [--resource-version=version] [--current-replicas=count] [flags]</code></td><td>更新指定的复制控制器的大小。</td></tr><tr><td><code>stop</code></td><td><code>kubectl stop</code></td><td>废弃的命令: 查看 <code>kubectl delete</code>.</td></tr><tr><td><code>version</code></td><td><code>kubectl version [--client] [flags]</code></td><td>更新指定的复制控制器的大小。…</td></tr></tbody></table><p>切记：有关命令操作的更多信息，请参阅[kubectl]（/ docs / user-guide / kubectl /）参考文档。</p><h2><span id="资源类型"> 资源类型</span></h2><p>下表包含所有受支持的资源类型及其缩写别名的列表：</p><table><thead><tr><th>Resource type</th><th>Abbreviated alias</th></tr></thead><tbody><tr><td><code>apiservices</code></td><td></td></tr><tr><td><code>certificatesigningrequests</code></td><td><code>csr</code></td></tr><tr><td><code>clusters</code></td><td></td></tr><tr><td><code>clusterrolebindings</code></td><td></td></tr><tr><td><code>clusterroles</code></td><td></td></tr><tr><td><code>componentstatuses</code></td><td><code>cs</code></td></tr><tr><td><code>configmaps</code></td><td><code>cm</code></td></tr><tr><td><code>controllerrevisions</code></td><td></td></tr><tr><td><code>cronjobs</code></td><td></td></tr><tr><td><code>customresourcedefinition</code></td><td><code>crd</code></td></tr><tr><td><code>daemonsets</code></td><td><code>ds</code></td></tr><tr><td><code>deployments</code></td><td><code>deploy</code></td></tr><tr><td><code>endpoints</code></td><td><code>ep</code></td></tr><tr><td><code>events</code></td><td><code>ev</code></td></tr><tr><td><code>horizontalpodautoscalers</code></td><td><code>hpa</code></td></tr><tr><td><code>ingresses</code></td><td><code>ing</code></td></tr><tr><td><code>jobs</code></td><td></td></tr><tr><td><code>limitranges</code></td><td><code>limits</code></td></tr><tr><td><code>namespaces</code></td><td><code>ns</code></td></tr><tr><td><code>networkpolicies</code></td><td><code>netpol</code></td></tr><tr><td><code>nodes</code></td><td><code>no</code></td></tr><tr><td><code>persistentvolumeclaims</code></td><td><code>pvc</code></td></tr><tr><td><code>persistentvolumes</code></td><td><code>pv</code></td></tr><tr><td><code>poddisruptionbudget</code></td><td><code>pdb</code></td></tr><tr><td><code>podpreset</code></td><td></td></tr><tr><td><code>pods</code></td><td><code>po</code></td></tr><tr><td><code>podsecuritypolicies</code></td><td><code>psp</code></td></tr><tr><td><code>podtemplates</code></td><td></td></tr><tr><td><code>replicasets</code></td><td><code>rs</code></td></tr><tr><td><code>replicationcontrollers</code></td><td><code>rc</code></td></tr><tr><td><code>resourcequotas</code></td><td><code>quota</code></td></tr><tr><td><code>rolebindings</code></td><td></td></tr><tr><td><code>roles</code></td><td></td></tr><tr><td><code>secrets</code></td><td></td></tr><tr><td><code>serviceaccounts</code></td><td><code>sa</code></td></tr><tr><td><code>services</code></td><td><code>svc</code></td></tr><tr><td><code>statefulsets</code></td><td></td></tr><tr><td><code>storageclasses</code></td><td></td></tr></tbody></table><h2><span id="输出选项"> 输出选项</span></h2><p>有关如何格式化或排序某些命令的输出的信息，请使用以下部分。 有关哪些命令支持各种输出选项的详细信息，请参阅<code>[kubectl](/docs/user-guide/kubectl/)</code>参考文档。</p><h3><span id="formatting-output"> Formatting output</span></h3><p>所有<code>kubectl</code>命令的默认输出格式是人类可读的纯文本格式。 要以特定格式将详细信息输出到终端窗口，可以将<code>-o</code>或<code>-output</code>标志添加到支持的<code>kubectl</code>命令。</p><h4><span id="语法"> 语法</span></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl [command] [TYPE] [NAME] -o=&lt;output_format&gt;</span><br></pre></td></tr></table></figure><p>根据<code>kubectl</code>操作，支持以下输出格式：</p><table><thead><tr><th>输出格式</th><th>描述</th></tr></thead><tbody><tr><td><code>-o=custom-columns=&lt;spec&gt;</code></td><td>使用逗号分隔的[自定义列]列表（#custom-columns）打印表。</td></tr><tr><td><code>-o=custom-columns-file=&lt;filename&gt;</code></td><td>使用<code>&lt;filename&gt;</code>文件中的[custom columns]（#custom-columns）模板打印表。</td></tr><tr><td><code>-o=json</code></td><td>输出JSON格式的API对象。</td></tr><tr><td><code>-o=jsonpath=&lt;template&gt;</code></td><td>打印[jsonpath]（/docs/reference/kubectl/jsonpath/）表达式中定义的字段。</td></tr><tr><td><code>-o=jsonpath-file=&lt;filename&gt;</code></td><td>Print the fields defined by the <a href="/docs/reference/kubectl/jsonpath/">jsonpath</a> expression in the <code>&lt;filename&gt;</code> file.</td></tr><tr><td><code>-o=name</code></td><td>只输出name列</td></tr><tr><td><code>-o=wide</code></td><td>以纯文本格式输出，包含任何其他信息。 对于pod，包含节点名称。</td></tr><tr><td><code>-o=yaml</code></td><td>yaml格式输出</td></tr></tbody></table><h5><span id="示例"> 示例</span></h5><p>输出yaml格式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod web-pod-13je7 -o=yaml</span></span><br></pre></td></tr></table></figure><h4><span id="自定义列"> 自定义列</span></h4><p>要定义自定义列并仅将所需的详细信息输出到表中，可以使用<code>custom-columns</code>选项。 您可以选择内联定义自定义列或使用模板文件： <code>-o=custom-columns=&lt;spec&gt;</code> 或者 <code>-o=custom-columns-file=&lt;filename&gt;</code>.</p><h5><span id="示例"> 示例</span></h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods &lt;pod-name&gt; -o=custom-columns=NAME:.metadata.name,RSRC:.metadata.resourceVersion</span></span><br></pre></td></tr></table></figure><p>模版文件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods &lt;pod-name&gt; -o=custom-columns-file=template.txt</span></span><br></pre></td></tr></table></figure><p><code>template.txt</code> 文件内容:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NAME          RSRC</span><br><span class="line">metadata.name metadata.resourceVersion</span><br></pre></td></tr></table></figure><p>输出结果:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NAME           RSRC</span><br><span class="line">submit-queue   610995</span><br></pre></td></tr></table></figure><h4><span id="服务器端列"> 服务器端列</span></h4><p><code>kubectl</code>支持从服务器接收有关对象的特定列信息。<br>这意味着对于任何给定资源，服务器将返回与该资源相关的列和行，以供客户端打印。<br>通过让服务器封装打印细节，这允许在针对同一群集使用的客户端之间提供一致的人类可读输出。</p><p>默认情况下，在<code>kubectl</code> 1.11及更高版本中启用此功能。 要禁用它，请添加<br><code>--server-print=false</code> 标志为<code>kubectl get</code>命令。</p><h5><span id="示例"> 示例</span></h5><p>要打印有关pod状态的信息，请使用如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods &lt;pod-name&gt; --server-print=false</span><br></pre></td></tr></table></figure><p>如下输出</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NAME       READY     STATUS              RESTARTS   AGE</span><br><span class="line">pod-name   1/1       Running             0          1m</span><br></pre></td></tr></table></figure><h3><span id="排序列表对象"> 排序列表对象</span></h3><p>要将对象输出到终端窗口中的排序列表，可以将<code>--sort-by</code>标志添加到支持的<code>kubectl</code>命令。 通过使用<code>--sort-by</code>标志指定任何数字或字符串字段来对对象进行排序。</p><h4><span id="语法"> 语法</span></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl [command] [TYPE] [NAME] --sort-by=&lt;jsonpath_exp&gt;</span><br></pre></td></tr></table></figure><h5><span id="示例"> 示例</span></h5><p>要打印按名称排序的pod列表，请运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods --sort-by=.metadata.name</span></span><br></pre></td></tr></table></figure><h2><span id="示例常见操作"> 示例：常见操作</span></h2><p>使用以下示例集来帮助您熟悉运行常用的<code>kubectl</code>操作：</p><p><code>kubectl create</code> - 从文件或stdin创建资源。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// Create a service using the definition in example-service.yaml.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f example-service.yaml</span></span><br><span class="line"></span><br><span class="line">// Create a replication controller using the definition in example-controller.yaml.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f example-controller.yaml</span></span><br><span class="line"></span><br><span class="line">// Create the objects that are defined in any .yaml, .yml, or .json file within the &lt;directory&gt; directory.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f &lt;directory&gt;</span></span><br></pre></td></tr></table></figure><p><code>kubectl get</code> - 列出一个或多个资源。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// List all pods in plain-text output format.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods</span></span><br><span class="line"></span><br><span class="line">// List all pods in plain-text output format and includes additional information (such as node name).</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods -o wide</span></span><br><span class="line"></span><br><span class="line">// List the replication controller with the specified name in plain-text output format. Tip: You can shorten and replace the 'replicationcontroller' resource type with the alias 'rc'.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get replicationcontroller &lt;rc-name&gt;</span></span><br><span class="line"></span><br><span class="line">// List all replication controllers and services together in plain-text output format.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get rc,services</span></span><br><span class="line"></span><br><span class="line">// List all daemon sets, including uninitialized ones, in plain-text output format.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get ds --include-uninitialized</span></span><br><span class="line"></span><br><span class="line">// List all pods running on node server01</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods --field-selector=spec.nodeName=server01</span></span><br><span class="line"></span><br><span class="line">// List all pods in plain-text output format, delegating the details of printing to the server</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods --experimental-server-print</span></span><br></pre></td></tr></table></figure><p><code>kubectl describe</code> - 显示一个或多个资源的详细状态，包括默认情况下未初始化的资源。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// Display the details of the node with name &lt;node-name&gt;.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe nodes &lt;node-name&gt;</span></span><br><span class="line"></span><br><span class="line">// Display the details of the pod with name &lt;pod-name&gt;.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe pods/&lt;pod-name&gt;</span></span><br><span class="line"></span><br><span class="line">// Display the details of all the pods that are managed by the replication controller named &lt;rc-name&gt;.</span><br><span class="line">// Remember: Any pods that are created by the replication controller get prefixed with the name of the replication controller.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe pods &lt;rc-name&gt;</span></span><br><span class="line"></span><br><span class="line">// Describe all pods, not including uninitialized ones</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe pods --include-uninitialized=<span class="literal">false</span></span></span><br></pre></td></tr></table></figure><p><code>kubectl get</code> 命令通常用于检索一个或多个相同资源类型的资源。 它具有丰富的标志，允许使用<code>-o</code>或<code>--output</code>标志自定义输出格式，也可以指定<code>-w</code>或<code>--watch</code>标志以开始观察特定的更新。</p><p><code>kubectl describe</code> 命令更侧重于描述许多指定资源的相关方面。 它可以调用对API服务器的多个API调用来为用户构建视图。 例如，<code>kubectl describe node</code>命令不仅检索有关节点的信息，还检索摘要在其上运行的pod，为节点生成的事件等。</p><p><code>kubectl delete</code> - 从文件，标准输入或指定标签选择器，名称，资源选择器或资源中删除资源。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// Delete a pod using the type and name specified in the pod.yaml file.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete -f pod.yaml</span></span><br><span class="line"></span><br><span class="line">// Delete all the pods and services that have the label name=&lt;label-name&gt;.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete pods,services -l name=&lt;label-name&gt;</span></span><br><span class="line"></span><br><span class="line">// Delete all the pods and services that have the label name=&lt;label-name&gt;, including uninitialized ones.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete pods,services -l name=&lt;label-name&gt; --include-uninitialized</span></span><br><span class="line"></span><br><span class="line">// Delete all pods, including uninitialized ones.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete pods --all</span></span><br></pre></td></tr></table></figure><p><code>kubectl exec</code> - 对pod中的容器执行命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// Get output from running 'date' from pod &lt;pod-name&gt;. By default, output is from the first container.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> &lt;pod-name&gt; date</span></span><br><span class="line"></span><br><span class="line">// Get output from running 'date' in container &lt;container-name&gt; of pod &lt;pod-name&gt;.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> &lt;pod-name&gt; -c &lt;container-name&gt; date</span></span><br><span class="line"></span><br><span class="line">// Get an interactive TTY and run /bin/bash from pod &lt;pod-name&gt;. By default, output is from the first container.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> -ti &lt;pod-name&gt; /bin/bash</span></span><br></pre></td></tr></table></figure><p><code>kubectl logs</code> - 在容器中打印容器的日志。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Return a snapshot of the logs from pod &lt;pod-name&gt;.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl logs &lt;pod-name&gt;</span></span><br><span class="line"></span><br><span class="line">// Start streaming the logs from pod &lt;pod-name&gt;. This is similar to the 'tail -f' Linux command.</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl logs -f &lt;pod-name&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;kubectl&lt;/code&gt; 是一个命令行界面，用于运行针对Kubernetes集群的命令。 本概述介绍kubectl语法，描述命令操作，并提供常见示例。&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="yunke.science/categories/kubernetes/"/>
    
    
      <category term="kubernetes" scheme="yunke.science/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kube-router 用户指南</title>
    <link href="yunke.science/2018/06/07/kuberouter-guide/"/>
    <id>yunke.science/2018/06/07/kuberouter-guide/</id>
    <published>2018-06-07T07:47:37.000Z</published>
    <updated>2018-06-07T07:50:56.260Z</updated>
    
    <content type="html"><![CDATA[<p>Kube-router是Kubernetes网络的一站式解决方案，旨在提供简单操作和高性能。</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#%E9%83%A8%E7%BD%B2">部署</a></li><li><a href="#%E5%91%BD%E4%BB%A4%E8%A1%8C%E9%80%89%E9%A1%B9">命令行选项</a></li><li><a href="#%E5%89%8D%E6%8F%90%E6%9D%A1%E4%BB%B6">前提条件</a></li><li><a href="#%E4%BB%A5k8s-daemonset-%E8%BF%90%E8%A1%8C">以k8s daemonset 运行</a></li><li><a href="#%E4%BB%A5%E4%BB%A3%E7%90%86agent%E8%BF%90%E8%A1%8C">以代理agent运行</a></li><li><a href="#%E6%B8%85%E9%99%A4%E9%85%8D%E7%BD%AE">清除配置</a></li><li><a href="#%E5%B0%9D%E8%AF%95%E4%BD%BF%E7%94%A8kube-router%E4%BD%9C%E4%B8%BAkube-proxy%E7%9A%84%E6%9B%BF%E4%BB%A3%E5%93%81">尝试使用kube-router作为kube-proxy的替代品</a></li><li><a href="#hairpin-%E6%A8%A1%E5%BC%8F">Hairpin 模式</a></li><li><a href="#%E7%9B%B4%E6%8E%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF%E8%BF%94%E5%9B%9E">直接服务端返回</a></li><li><a href="#%E8%B4%9F%E8%BD%BD%E5%B9%B3%E8%A1%A1%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95">负载平衡调度算法</a></li><li><a href="#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8-ips">负载均衡器 IPS</a></li><li><a href="#hostport-%E6%94%AF%E6%8C%81">HostPort 支持</a></li><li><a href="#bgp-configuration">BGP configuration</a></li><li><a href="#metrics">Metrics</a></li><li><a href="#%E5%8F%82%E8%80%83">参考</a></li></ul></p><h2><span id="部署"> 部署</span></h2><p>根据您要使用的kube-router的功能，可能会有多个部署选项。 您可以使用标志 <code>--run-firewall, --run-router, --run-service-proxy</code> 有选择地启用kube-router所需的功能。</p><p>您也可以选择运行kube-router作为在每个群集节点上运行的代理。 也可以通过守护进程在每个节点上运行kube-router pod。</p><h2><span id="命令行选项"> 命令行选项</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">Usage of kube-router:</span><br><span class="line">      --advertise-cluster-ip             Add Cluster IP of the service to the RIB so that it gets advertises to the BGP peers.</span><br><span class="line">      --advertise-external-ip            Add External IP of service to the RIB so that it gets advertised to the BGP peers.</span><br><span class="line">      --advertise-loadbalancer-ip        Add LoadbBalancer IP of service status as set by the LB provider to the RIB so that it gets advertised to the BGP peers.</span><br><span class="line">      --advertise-pod-cidr               Add Node&apos;s POD cidr to the RIB so that it gets advertised to the BGP peers. (default true)</span><br><span class="line">      --bgp-graceful-restart             Enables the BGP Graceful Restart capability so that routes are preserved on unexpected restarts</span><br><span class="line">      --cleanup-config                   Cleanup iptables rules, ipvs, ipset configuration and exit.</span><br><span class="line">      --cluster-asn uint                 ASN number under which cluster nodes will run iBGP.</span><br><span class="line">      --cluster-cidr string              CIDR range of pods in the cluster. It is used to identify traffic originating from and destinated to pods.</span><br><span class="line">      --enable-cni                       Enable CNI plugin. Disable if you want to use kube-router features alongside another CNI plugin. (default true)</span><br><span class="line">      --enable-ibgp                      Enables peering with nodes with the same ASN, if disabled will only peer with external BGP peers (default true)</span><br><span class="line">      --enable-overlay                   When enable-overlay set to true, IP-in-IP tunneling is used for pod-to-pod networking across nodes in different subnets. When set to false no tunneling is used and routing infrastrcture is expected to route traffic for pod-to-pod networking across nodes in different subnets (default true)</span><br><span class="line">      --enable-pod-egress                SNAT traffic from Pods to destinations outside the cluster. (default true)</span><br><span class="line">      --enable-pprof                     Enables pprof for debugging performance and memory leak issues.</span><br><span class="line">      --hairpin-mode                     为每个服务端点添加iptable规则以支持 hairpin 流量。</span><br><span class="line">      --health-port uint16               Health check port, 0 = Disabled (default 20244)</span><br><span class="line">  -h, --help                             Print usage information.</span><br><span class="line">      --hostname-override string         Overrides the NodeName of the node. Set this if kube-router is unable to determine your NodeName automatically.</span><br><span class="line">      --iptables-sync-period duration    The delay between iptables rule synchronizations (e.g. &apos;5s&apos;, &apos;1m&apos;). Must be greater than 0. (default 5m0s)</span><br><span class="line">      --ipvs-sync-period duration        The delay between ipvs config synchronizations (e.g. &apos;5s&apos;, &apos;1m&apos;, &apos;2h22m&apos;). Must be greater than 0. (default 5m0s)</span><br><span class="line">      --kubeconfig string                Path to kubeconfig file with authorization information (the master location is set by the master flag).</span><br><span class="line">      --masquerade-all                   SNAT all traffic to cluster IP/node port.</span><br><span class="line">      --master string                    The address of the Kubernetes API server (overrides any value in kubeconfig).</span><br><span class="line">      --metrics-path string              Prometheus metrics path (default &quot;/metrics&quot;)</span><br><span class="line">      --metrics-port uint16              Prometheus metrics port, (Default 0, Disabled)</span><br><span class="line">      --nodeport-bindon-all-ip           For service of NodePort type create IPVS service that listens on all IP&apos;s of the node.</span><br><span class="line">      --nodes-full-mesh                  Each node in the cluster will setup BGP peering with rest of the nodes. (default true)</span><br><span class="line">      --peer-router-asns uints           ASN numbers of the BGP peer to which cluster nodes will advertise cluster ip and node&apos;s pod cidr. (default [])</span><br><span class="line">      --peer-router-ips ipSlice          The ip address of the external router to which all nodes will peer and advertise the cluster ip and pod cidr&apos;s. (default [])</span><br><span class="line">      --peer-router-multihop-ttl uint8   Enable eBGP multihop supports -- sets multihop-ttl. (Relevant only if ttl &gt;= 2)</span><br><span class="line">      --peer-router-passwords strings    Password for authenticating against the BGP peer defined with &quot;--peer-router-ips&quot;.</span><br><span class="line">      --routes-sync-period duration      The delay between route updates and advertisements (e.g. &apos;5s&apos;, &apos;1m&apos;, &apos;2h22m&apos;). Must be greater than 0. (default 5m0s)</span><br><span class="line">      --run-firewall                     Enables Network Policy -- sets up iptables to provide ingress firewall for pods. (default true)</span><br><span class="line">      --run-router                       Enables Pod Networking -- Advertises and learns the routes to Pods via iBGP. (default true)</span><br><span class="line">      --run-service-proxy                Enables Service Proxy -- sets up IPVS for Kubernetes Services. (default true)</span><br><span class="line">  -v, --v string                         log level for V logs (default &quot;0&quot;)</span><br><span class="line">  -V, --version                          Print version information.</span><br></pre></td></tr></table></figure><h2><span id="前提条件"> 前提条件</span></h2><ul><li><p>Kube-router需要访问kubernetes API服务器以获取有关Pod，服务，端点，网络策略等的信息。它需要的最少信息是关于在何处访问kubernetes - API服务器的详细信息。此信息可以通过<code>kube-router --master=http://192.168.1.99:8080/</code>或<code>kube-router --kubeconfig=&lt;path to kubeconfig file&gt;</code>传递。</p></li><li><p>如果您在该节点上运行kube-router作为代理，则必须在每个节点上安装ipset软件包（当以守护程序集运行时，容器映像使用ipset预先打包）</p></li><li><p>如果您选择使用kube-router作为pod-to-pod网络连接，则需要将Kubernetes控制器管理器配置为通过传递<code>--allocate-node-cidrs=true</code>标志并提供一个cluster-cidr来分配pod CIDR（即通过传递 - 例如，<code>--cluster-cidr=10.1.0.0/16</code>）</p></li><li><p>如果您选择将kube-router作为守护进程运行，则kube-apiserver和kubelet必须使用<code>--allow-privileged = true</code>选项运行</p></li><li><p>如果您选择使用kube-router作为<code>pod-to-pod</code>网络连接，则必须将Kubernetes集群配置为使用CNI网络插件。在每个节点上CNI conf文件预计会作为<code>/etc/cni/net.d/10-kuberouter.conf</code>存在<code>.bridge CNI</code>插件和<code>IPAM</code>的主机本地应该被使用。示例conf文件，可以下载<code>wget -O /etc/cni/net.d/10-kuberouter.conf https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/cni/10-kuberouter.conf</code> .</p></li></ul><h2><span id="以k8s-daemonset-运行"> 以k8s daemonset 运行</span></h2><p>这是部署kube-router的最快捷的方式。<br><code>kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kube-router-all-service-daemonset.yaml</code></p><p>以上将自动在每个节点上运行kube-router作为pod。 您可以根据需要更改守护进程定义中的参数以满足您的需求。 有些示例可以在https://github.com/cloudnativelabs/kube-router/tree/master/daemonset中找到，并使用不同的参数来选择kube-router应该运行的服务集。</p><h2><span id="以代理agent运行"> 以代理agent运行</span></h2><p>您可以选择运行kube-router作为每个节点上运行的代理。 例如，如果您只想让kube-router为pod提供入口防火墙，那么您可以启动kube-router as</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kube-router --master=http://192.168.1.99:8080/ --run-firewall=true --run-service-proxy=false --run-router=false</span><br></pre></td></tr></table></figure><h2><span id="清除配置"> 清除配置</span></h2><p>请删除kube-router daemonset，然后通过运行以下命令清除节点上的kube-router完成的所有配置（对ipvs，iptables，ipset，ip routes等）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --privileged --net=host cloudnativelabs/kube-router --cleanup-config</span><br></pre></td></tr></table></figure><h2><span id="尝试使用kube-router作为kube-proxy的替代品"> 尝试使用kube-router作为kube-proxy的替代品</span></h2><p>如果您有一个正在使用的kube-proxy，并且想尝试使用kube-router作为服务代理，那么您可以这样做</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kube-proxy --cleanup-iptables</span><br></pre></td></tr></table></figure><p>然后执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kube-router --master=http://192.168.1.99:8080/ --run-service-proxy=true --run-firewall=false --run-router=false</span><br></pre></td></tr></table></figure><p>并且如果您想要移回kube-proxy，则通过运行来清理由kube-router完成的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kube-router --cleanup-config</span><br></pre></td></tr></table></figure><p>并使用您拥有的配置运行kube-proxy。</p><h2><span id="hairpin-模式"> Hairpin 模式</span></h2><p>kubeadm-kuberouter 默认不支持从service后面的Pod到其自己的ClusterIP的端口。<br>即，如果在一个pod中，想通过service访问pod中的服务，网络默认是不通。</p><p>这里需要设置kube-router 的 <a href="https://github.com/cloudnativelabs/kube-router/blob/master/docs/user-guide.md#hairpin-mode" target="_blank" rel="noopener">hairpin-mode</a>。</p><p>两步操作：</p><ol><li>对于每个节点上的所有veth接口，hairpin_mode sysctl选项必须设置为1。<br>这可以通过选项添加 <code>&quot;hairpinMode&quot;:true</code> 到CNI配置并重新引导所有群集节点（如果它们已经运行kubernetes）来完成。</li></ol><p>在安装kube-router之前，如果还没有执行<code>KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml</code> 操作如下：<br>下载这个文件到本地：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># wget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml</span><br><span class="line"># vi kubeadm-kuberouter.yaml</span><br><span class="line">在`ConfigMap` `kube-router-cfg` `cni-conf.json` 中添加 `&quot;hairpinMode&quot;:true,`, 如下图</span><br><span class="line">  cni-conf.json: |</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;:&quot;kubernetes&quot;,</span><br><span class="line">      &quot;type&quot;:&quot;bridge&quot;,</span><br><span class="line">      &quot;bridge&quot;:&quot;kube-bridge&quot;,</span><br><span class="line">      &quot;isDefaultGateway&quot;:true,</span><br><span class="line">      &quot;hairpinMode&quot;:true,</span><br><span class="line">      &quot;ipam&quot;: &#123;</span><br><span class="line">        &quot;type&quot;:&quot;host-local&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">添加完成后，引用这个文件</span><br><span class="line">KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f kubeadm-kuberouter.yaml</span><br></pre></td></tr></table></figure><ol start="2"><li>对于需要启用的服务，单独添加 <code>annotations:kube-router.io/service.hairpin: &quot;&quot;</code> 。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: ServiceName</span><br><span class="line">  annotations:</span><br><span class="line">    kube-router.io/service.hairpin: &quot;&quot;</span><br></pre></td></tr></table></figure><p>如果要默认对所有服务启用，需要修改<code>kubeadm-kuberouter.yaml</code> ，<code>kube-router</code> 添加启动参数 <code>--hairpin-mode=true</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">- name: kube-router</span><br><span class="line">  image: cloudnativelabs/kube-router</span><br><span class="line">  imagePullPolicy: Always</span><br><span class="line">  args:</span><br><span class="line">  - --run-router=true</span><br><span class="line">  - --run-firewall=true</span><br><span class="line">  - --run-service-proxy=false</span><br><span class="line">  - --run-service-proxy=false</span><br><span class="line">  - --hairpin-mode=true</span><br></pre></td></tr></table></figure><h2><span id="直接服务端返回"> 直接服务端返回</span></h2><p>请阅读以下博客，了解如何结合使用DSR和–advertise-external-ip构建高度可扩展和可用的入口。<a href="https://cloudnativelabs.github.io/post/2017-11-01-kube-high-available-ingress/" target="_blank" rel="noopener">https://cloudnativelabs.github.io/post/2017-11-01-kube-high-available-ingress/</a></p><p>您可以为每个服务启用DSR（直接服务器返回）功能。 启用的服务端点将通过传递服务代理直接响应客户端。 启用DSR时，Kube-router将使用LVS的隧道模式来实现此目的。</p><p>要启用DSR，<a href="http://xn--kube-router-vd2ru89oi85cn15coe2b.io/service.dsr=tunnel%E6%B3%A8%E9%87%8A%E6%9D%A5%E6%B3%A8%E9%87%8A%E6%9C%8D%E5%8A%A1%E3%80%82" target="_blank" rel="noopener">您需要使用kube-router.io/service.dsr=tunnel注释来注释服务。</a> 对于例如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl annotate service my-service &quot;kube-router.io/service.dsr=tunnel&quot;</span><br></pre></td></tr></table></figure><p>在目前的实施中，当在服务上应用注释时，DSR将仅适用于外部IP。</p><p>同样，当使用DSR时，当前实现不支持端口重映射。 所以你需要为服务使用相同的端口和目标端口</p><p>您需要在kube-router守护程序集清单中启用<code>hostIPC：true</code>和<code>hostPID：true</code>。 此外，主路径<code>/var/run/docker.sock</code>必须作为<code>kube-router</code>的卷挂载。</p><p>上述更改需要kube-router输入pod namespeace并在pod中创建ipip隧道并将外部IP分配给VIP。</p><p>对于例如清单，请查看启用DSR要求的<a href="https://github.com/cloudnativelabs/kube-router/blob/master/daemonset/kubeadm-kuberouter-all-features-dsr.yaml" target="_blank" rel="noopener">清单</a>。</p><h2><span id="负载平衡调度算法"> 负载平衡调度算法</span></h2><p>Kube-router使用LVS作为服务代理。 LVS支持丰富的调度算法。 您可以注释该服务以选择其中一个调度变量。 未注释服务时默认情况下选择循环调度程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">For least connection scheduling use:</span><br><span class="line">kubectl annotate service my-service &quot;kube-router.io/service.scheduler=lc&quot;</span><br><span class="line"></span><br><span class="line">For round-robin scheduling use:</span><br><span class="line">kubectl annotate service my-service &quot;kube-router.io/service.scheduler=rr&quot;</span><br><span class="line"></span><br><span class="line">For source hashing scheduling use:</span><br><span class="line">kubectl annotate service my-service &quot;kube-router.io/service.scheduler=sh&quot;</span><br><span class="line"></span><br><span class="line">For destination hashing scheduling use:</span><br><span class="line">kubectl annotate service my-service &quot;kube-router.io/service.scheduler=dh&quot;</span><br></pre></td></tr></table></figure><h2><span id="负载均衡器-ips"> 负载均衡器 IPS</span></h2><p>如果您还想通告负载均衡器设置的IP（<code>status.loadBalancer.ingress IPs</code>），例如 当与MetalLb一起使用时，添加<code>--advertise-loadbalancer-ip</code>标志（默认为false）。</p><p>要有选择地禁用每个服务的这种行为，<a href="http://xn--kube-router-2u0rs6dm82acs1j.io/service.skiplbips%E6%B3%A8%E9%87%8A%E4%BD%9C%E4%B8%BA%E4%BE%8B%E5%A6%82%EF%BC%9A" target="_blank" rel="noopener">可以使用kube-router.io/service.skiplbips注释作为例如：</a><code>$ kubectl annotate service my-external-service &quot;kube-router.io/service.skiplbips=true&quot;</code></p><p>具体而言，除非服务按照上述注释，否则<code>--advertise-loadbalancer-ip</code>标志将使服务的Ingress IP由LoadBalancer设置为：</p><ul><li>可以本地添加到节点的kube-dummy（如果网络接口）</li><li>被通告给BGP对等体</li></ul><p>FYI Above已成功通过ARP模式的MetalLB测试。</p><h2><span id="hostport-支持"> HostPort 支持</span></h2><p>如果您想使用HostPort功能，清单中需要进行更改。</p><ul><li><p>默认情况下，kube-router假定CNI conf文件为<code>/etc/cni/net.d/10-kuberouter.conf</code>。 将环境变量<code>KUBE_ROUTER_CNI_CONF_FILE</code>添加到<code>kube-router</code>清单并将其设置为<code>/etc/cni/net.d/10-kuberouter.conflist</code></p></li><li><p>使用支持<code>portmap</code>的CNI config修改<code>kube-router-cfg</code> ConfigMap作为附加插件</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">     &quot;cniVersion&quot;:&quot;0.3.0&quot;,</span><br><span class="line">     &quot;name&quot;:&quot;mynet&quot;,</span><br><span class="line">     &quot;plugins&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">           &quot;name&quot;:&quot;kubernetes&quot;,</span><br><span class="line">           &quot;type&quot;:&quot;bridge&quot;,</span><br><span class="line">           &quot;bridge&quot;:&quot;kube-bridge&quot;,</span><br><span class="line">           &quot;isDefaultGateway&quot;:true,</span><br><span class="line">           &quot;ipam&quot;:&#123;</span><br><span class="line">              &quot;type&quot;:&quot;host-local&quot;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">           &quot;type&quot;:&quot;portmap&quot;,</span><br><span class="line">           &quot;capabilities&quot;:&#123;</span><br><span class="line">              &quot;snat&quot;:true,</span><br><span class="line">              &quot;portMappings&quot;:true</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line">     ]</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li>更新init容器命令以创建/etc/cni/net.d/10-kuberouter.conflist文件</li><li>重新启动运行容器</li></ul><p>对于例如清单，请查看清单，并对HostPort功能进行必要的更改。</p><h2><span id="bgp-configuration"> BGP configuration</span></h2><p><a href="https://github.com/cloudnativelabs/kube-router/blob/master/docs/user-guide.md#hairpin-mode" target="_blank" rel="noopener">Configuring BGP Peers</a></p><h2><span id="metrics"> Metrics</span></h2><p><a href="https://github.com/cloudnativelabs/kube-router/blob/master/docs/user-guide.md#hairpin-mode" target="_blank" rel="noopener">Configure metrics gathering</a></p><h2><span id="参考"> 参考</span></h2><p><a href="https://github.com/cloudnativelabs/kube-router/blob/master/docs/user-guide.md#hairpin-mode" target="_blank" rel="noopener">User Guide</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kube-router是Kubernetes网络的一站式解决方案，旨在提供简单操作和高性能。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes 网络策略简单配置</title>
    <link href="yunke.science/2018/05/18/k8sNetworkPolicy/"/>
    <id>yunke.science/2018/05/18/k8sNetworkPolicy/</id>
    <published>2018-05-18T11:21:26.000Z</published>
    <updated>2018-05-18T11:30:44.656Z</updated>
    
    <content type="html"><![CDATA[<p>网络策略是配置允许群组与其他网络终端进行通信的规范。</p><p><code>NetworkPolicy</code> 使用 <code>labels</code> 选择 <code>pod</code> 并定义规则，以指定允许所选<code>pod</code>允许哪些流量。</p><a id="more"></a><p></p><p><strong>实现功能：</strong></p><p>隔离namespace，不同namespace pod禁止访问，仅允许访问相同 namespace 中的 pod 。</p><p><strong>配置步骤：</strong></p><p>创建一个dev的namespace，并添加一个labels namespace: dev</p><ul><li>创建一个新的namespace：dev</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1  </span><br><span class="line">kind: Namespace  </span><br><span class="line">metadata:  </span><br><span class="line">   name: dev  </span><br><span class="line">   labels:  </span><br><span class="line">     namespace: dev</span><br></pre></td></tr></table></figure><ul><li>在 default namespace 和 dev namespace 分别创建三个pod，查看pod ip</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@docker01 k8srpm]# kubectl -n dev get pod -o wide</span><br><span class="line">NAME                   READY     STATUS    RESTARTS   AGE       IP           NODE</span><br><span class="line">myip-59dfc4d87-6ndlr   1/1       Running   0          24s       10.42.1.28   docker03</span><br><span class="line">myip-59dfc4d87-ksnbd   1/1       Running   0          24s       10.42.2.6    docker02</span><br><span class="line">myip-59dfc4d87-rfphz   1/1       Running   0          24s       10.42.0.7    docker01</span><br><span class="line">[root@docker01 k8srpm]# kubectl -n default get pod -o wide   </span><br><span class="line">NAME                   READY     STATUS    RESTARTS   AGE       IP           NODE</span><br><span class="line">myip-59dfc4d87-8h2d6   1/1       Running   0          36d       10.42.1.22   docker03</span><br><span class="line">myip-59dfc4d87-dfplq   1/1       Running   0          36d       10.42.2.5    docker02</span><br><span class="line">myip-59dfc4d87-zqr2w   1/1       Running   0          36d       10.42.1.21   docker03</span><br></pre></td></tr></table></figure><ul><li>进行互ping测试，互相可以联通</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@docker01 k8srpm]# kubectl -n dev exec -it myip-59dfc4d87-6ndlr ping 10.42.1.22      </span><br><span class="line">PING 10.42.1.22 (10.42.1.22): 56 data bytes</span><br><span class="line">64 bytes from 10.42.1.22: seq=0 ttl=64 time=0.167 ms</span><br><span class="line">^C</span><br><span class="line">--- 10.42.1.22 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.167/0.167/0.167 ms</span><br><span class="line">[root@docker01 k8srpm]# kubectl -n default exec -it myip-59dfc4d87-8h2d6 ping 10.42.1.28</span><br><span class="line">PING 10.42.1.28 (10.42.1.28): 56 data bytes</span><br><span class="line">64 bytes from 10.42.1.28: seq=0 ttl=64 time=0.151 ms</span><br><span class="line">64 bytes from 10.42.1.28: seq=1 ttl=64 time=0.129 ms</span><br></pre></td></tr></table></figure><ul><li>在dev namespace添加 NetworkPolicy，先deny all，然后放开dev namespace中pod访问</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: dev-env-deny-all</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: dev-isolate-namespace</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - namespaceSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          namespace: dev</span><br></pre></td></tr></table></figure><ul><li>进行 ping 测试，可以看到，dev 可以ping通default ，default 无法ping 通 dev 。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@docker01 k8srpm]# kubectl -n dev exec -it myip-59dfc4d87-6ndlr ping 10.42.1.22 </span><br><span class="line">PING 10.42.1.22 (10.42.1.22): 56 data bytes</span><br><span class="line">64 bytes from 10.42.1.22: seq=0 ttl=64 time=0.201 ms</span><br><span class="line">64 bytes from 10.42.1.22: seq=1 ttl=64 time=0.111 ms</span><br><span class="line">^C</span><br><span class="line">--- 10.42.1.22 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.111/0.156/0.201 ms</span><br><span class="line">[root@docker01 k8srpm]# kubectl -n default exec -it myip-59dfc4d87-8h2d6 ping 10.42.1.28</span><br><span class="line">PING 10.42.1.28 (10.42.1.28): 56 data bytes</span><br><span class="line">--- 10.42.1.28 ping statistics ---</span><br><span class="line">4 packets transmitted, 0 packets received, 100% packet loss</span><br><span class="line">command terminated with exit code 1</span><br></pre></td></tr></table></figure><ul><li>验证成功。</li></ul><hr><p><strong>kubernetes网络策略配置说明：</strong></p><p>网络策略由网络插件实现，因此您必须使用支持NetworkPolicy的网络解决方案 - 只创建资源而没有控制器实施它，策略将无效。</p><p>pod 通过一个 NetworkPolicy 来隔离网络。 一旦 namespace 中的任何NetworkPolicy选择了特定的Pod，该Pod将拒绝任何NetworkPolicy所不允许的连接。 （命名空间中未被任何NetworkPolicy选择的其他pod将继续接受所有流量。）</p><p>V1.10 新增了 ipBlock 过滤，和 egress 出口流量过滤 规则。</p><p>V1.10 NetworkPolicy 示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: test-network-policy</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      role: db</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br><span class="line">  - Egress</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - ipBlock:</span><br><span class="line">        cidr: 172.17.0.0/16</span><br><span class="line">        except:</span><br><span class="line">        - 172.17.1.0/24</span><br><span class="line">    - namespaceSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          project: myproject</span><br><span class="line">    - podSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          role: frontend</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 6379</span><br><span class="line">  egress:</span><br><span class="line">  - to:</span><br><span class="line">    - ipBlock:</span><br><span class="line">        cidr: 10.0.0.0/24</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 5978</span><br></pre></td></tr></table></figure><p>参考：</p><ol><li><a href="https://k8smeetup.github.io/docs/concepts/services-networking/network-policies/" target="_blank" rel="noopener">https://k8smeetup.github.io/docs/concepts/services-networking/network-policies/</a></li><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-traffic" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-traffic</a></li><li><a href="https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/" target="_blank" rel="noopener">https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;网络策略是配置允许群组与其他网络终端进行通信的规范。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;NetworkPolicy&lt;/code&gt; 使用 &lt;code&gt;labels&lt;/code&gt; 选择 &lt;code&gt;pod&lt;/code&gt; 并定义规则，以指定允许所选&lt;code&gt;pod&lt;/code&gt;允许哪些流量。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>微服务架构的优势</title>
    <link href="yunke.science/2018/05/17/microsrv-advant/"/>
    <id>yunke.science/2018/05/17/microsrv-advant/</id>
    <published>2018-05-17T04:06:06.000Z</published>
    <updated>2018-05-17T04:10:21.612Z</updated>
    
    <content type="html"><![CDATA[<p>微服务是当前互联网产品的一个技术架构趋势，近两年伴随着Docker容器计算的发展和普及,微服务架构在业界逐渐落地,那么驱动企业进行微服务化架构改造的根本推动力是什么?如何在企业内部实施微服务架构?实施微服务契构需要哪些技术框架和基础设施?这些都是企业技术人员需要了解并关心的问题,也是要具体讨论的问题。</p><a id="more"></a><p><strong>微服务作为对应单体应用出现的概念,其架构通常有哪些优势呢?</strong></p><ul><li><p><strong>项目工程简洁</strong>。一个复杂的产品功能集合,代码仓库规模往往随着业务复杂度的增加而线性增加。对于单体应用来说,代码的堆积意味着工程规模的迅速膨胀。而对于微服务来说,因为每个微服务承担的职责小而且单一,所以工程规模简洁可控。</p></li><li><p><strong>升级代价小</strong>。当一个产品所有的功能都集中在同一个应用时,对程序的升级会带来两方面的影响:一是项目工程过大造成的应用启动时间过久,我们见过有些产品一个应用实例的启动时间需要半个小时以上,这对业务来说显然不可行。二是每次hotfix都需要对整个应用重启,引起业务的不稳定,而微服务架构恰恰相反,每个服务独立升级,对业务整体的影响极小。</p></li><li><p><strong>扩展性好</strong>。对于互联网产品来说,产品迭代速度很重要。对于一个庞大的单体应用来说,牵一发而动全身,无论对产品功能的扩展性还是性能的扩展性来说,都是一个很大的挑战。通过微服务化,把业务中扩展性差的部分独立出去,不同的业务类型采用不同扩展方案,可以提升业务整体的可扩展性。</p></li><li><p><strong>稳定性好</strong>。单体应用的稳定性差主要体现在功能间的隔离性,对于单体应用而言,所有的功能都在同一个进程空间里,这意味着任何一个功能的bug可能会造成应用整个崩溃。微服务的好处是可以实现进程级别的隔离,单个服务异常很少会造成全局故障。</p></li><li><p><strong>人员变更影响小</strong>。项目中人员更迭并不少见,单体应用的交接要求被交接人员必须对整个项目非常熟悉,才有可能消化变更人员带来的负面影响,否则人员离职或转岗会影响项目的正常进度。实施微服务架构的团队往往同时也遵循“2 pizza&quot;原则的组织架构,团队小而精,人员变更影响小。</p></li><li><p><strong>技术栈丰富</strong>。单体应用因为都跑在同一个进程里,所以项目的整体技术栈就被这个进程锁死了,比如说一个Java单体应用,无论业界的其他编程语言和开发框架如何发展,我们也无法利用起来,无法实现技术反哺业务。而微服务可以采用一些通用的服务间通信方式(HTTP等)去集成,使得服务的实现方式不局限于某个技术栈，适合用最合适的技术实现功能。</p></li><li><p><strong>开发效率高</strong>。主要体现在为服务架构下项目的学习曲线平滑，因为工程规模小，所以开发人员能很快做到对项目有一个大而全的认识。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;微服务是当前互联网产品的一个技术架构趋势，近两年伴随着Docker容器计算的发展和普及,微服务架构在业界逐渐落地,那么驱动企业进行微服务化架构改造的根本推动力是什么?如何在企业内部实施微服务架构?实施微服务契构需要哪些技术框架和基础设施?这些都是企业技术人员需要了解并关心的问题,也是要具体讨论的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="yunke.science/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="架构" scheme="yunke.science/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>分布式服务架构</title>
    <link href="yunke.science/2018/05/16/architec-distri/"/>
    <id>yunke.science/2018/05/16/architec-distri/</id>
    <published>2018-05-16T05:11:57.000Z</published>
    <updated>2018-05-16T07:08:37.056Z</updated>
    
    <content type="html"><![CDATA[<p>一般而言,企业经过初创期和成长期两个阶段的发展,就基本确定了业务的发展方向,接下来只要面对竞争对手的跟随和大量用户访问请求的问题。这些企业也会提供各种不同的子产品模块功能来满足业务的多样性发展,比如产品会设计不同的产品功能体系,运营人员会设计不同的运营活动,客服人员会接到不同的用户反馈等。</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#%E4%B8%9A%E5%8A%A1%E9%9C%80%E6%B1%82">业务需求</a></li><li><a href="#%E5%BC%B9%E6%80%A7%E6%89%A9%E5%AE%B9">弹性扩容</a></li><li><a href="#%E6%9C%8D%E5%8A%A1%E5%8C%96">服务化</a></li></ul></p><p>这些需求叠加在一起,会导致整个业务越来越复杂,有些系统变得不再具有维护性,无法满足可用性的需求,只要一出现流量高峰,系统必定宕机,所以很多公司面临重构架构设计,甚至推翻重来,比如京东大部分的用户业务从.NET转到Java语言的重构,淘宝从PHP转到Java,升级2.0,再到3.0的架构转变,可以看到,这个时间点的转型或重构可能不会到生死攸关的地步,但其成本,是非常高的。因此,如何在一开始就能做好这种转变的预见性,对契构设计有相当大的挑战,本节先给出几点供读者来参考。</p><h2><span id="业务需求"> 业务需求</span></h2><p>通过云服务通常可以解决很多架构层面的问题,比如对象存储系统解决了文件分布式存储的问题, CDN解决了静态资源访问的性能问题,但实际上随着业务的不断发展,系统访问压力增大,还可能有很多的请求变慢或者超时,应用服务器或数据库服务的压力波动较大,只要不停地上线新业务,技术债就会越来越明显,业务的迭代也越来越跟不上产品需求。</p><p>为了解决这些问题,企业往往需要进行各种业务的拆分,把不同的功能模块拆分到不同的服务器上进行独立部署。比如用户模块、商品模块、购物车模块、订单模块和支付模块等,这些模块拆分并独立部署出来后,可以再进一步根据系统的瓶领进行细分。但是进行服务拆分之后,各模块之间的依赖又变得明显起来,比如数据库的建接效、数据的分<br>布式事务、数据库的性能开销等都是急切需要解决的问题。</p><p>同时,随着业务模块的拆分,除了上述的技术问题要解决外,还面临着工程实践的问题,比如在业务的不同分支中,需要保证开发人员、测试人员、运维人员快速地对开发环境、则试环境、预发布环境的搭建和发布。在高速发展的企业中迭代的频率非常高,以网易考拉平台为例,所有系统的日发布次数到达数千次,所以技术人员对效率的要求比较高。当扩大到一个公司多个产品线,整体的运行就要求像现代化工厂一样来运作,需要自动化<br>的平台去解决,纯手工根本无法满足企业的高效运转。</p><h2><span id="弹性扩容"> 弹性扩容</span></h2><p>随着需求和用户的不断增长,系统会出现波峰和波谷,为了更好地利用资源和成本预算,弹性扩容成了必要需求,在峰值的时候能够根据业务的压力自动扩容,分担流量,在压力低的时候目动缩容,藏少成本或提高资源的利用率,把缩容的资源做离线业务计算。</p><p>也许在过去是简单地通过垂直扩大规模能力来处理更多的需求,或者是购买更强的服务器,这在一定程度上是可行的,但过程很慢开且代价庞大,通过提前准备过多的资源,会导致只根据峰值使用量预测来规划能力值,比如根据服务器的最高计算能力购买硬件予以满足,这是不得已的做法。例如国外的黑色星期五、国内的双11等活动,当天请求非常高,需要足够的资源来满足业务请求,而平时这些服务器的使用率很低,所以,<strong>只有依赖云的弹性才能满足这种业务场景的需求</strong>。</p><h2><span id="服务化"> 服务化</span></h2><p>不管是互联网还是传统行业转型的企业,基本都是在原有基础业务上发展而来的,不可能把业务停止从零开始,因此,直接对原有系统进行微服务改造比较困难,风险也较大.这时基本上处于一种混合架构期,即新的业务会从头开发,逐步接入到老系统中,一步步替换老系统不满足的地方,通过不断地快速迭代来保证业务的可持续性,同时又保证新业务的快速需求。</p><p>著名的架构大师Martin Fowler从2013年正式提出微服务架构的综述文章。至今,都没有提供统一的最佳实践方案。现在微服务的架构实现方式也各种各样,通常根据应用的类型拆分成不同的微服务来实现,每个服务根据业务的特征采用不同的技术栈进行组合,把每个服务划分成可以独立部署的隔离进程来运行。目前,微服务的基本框架都类似,比如包括服务发现、降级、治理等方面。业务实现微服务的技术细节各不相同,没有统一的实现方案,比如服务发现有自建服务基础设施的,也有依赖第三方开源的,技术人员需要根据自己的场景来做选择。简化的架构模型如图所示。</p><img src="/2018/05/16/architec-distri/Distribute-Service-Architecture.png" title="Distribute-Service-Architecture"><hr><p>显然,这个架构模型只是整个业务服务架构的一部分,实际的系统可能要复杂几十倍。如果业务的迭代速度非常快,同时每个业务之间的依赖从设计、开发、测试、上线到运维都是一个非常庞大的复杂工程,因此,如何高效管理所依赖的服务和系统依赖,诊断并及时对业务反馈响应是对服务架构的考验。</p><p><strong>架构演化发展历程：</strong></p><ol><li><a href="/2018/05/16/architec-initial/">初创期架构</a></li><li><a href="/2018/05/16/architec-growth/">快速成长期架构</a></li><li><a href="/2018/05/16/architec-distri/">分布式服务架构</a></li></ol><p>参考：</p><ol><li>云原生应用架构实践 （网易云基础服务架构团队 著）</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一般而言,企业经过初创期和成长期两个阶段的发展,就基本确定了业务的发展方向,接下来只要面对竞争对手的跟随和大量用户访问请求的问题。这些企业也会提供各种不同的子产品模块功能来满足业务的多样性发展,比如产品会设计不同的产品功能体系,运营人员会设计不同的运营活动,客服人员会接到不同的用户反馈等。&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="yunke.science/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="架构" scheme="yunke.science/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>快速成长期架构</title>
    <link href="yunke.science/2018/05/16/architec-growth/"/>
    <id>yunke.science/2018/05/16/architec-growth/</id>
    <published>2018-05-16T05:08:29.000Z</published>
    <updated>2018-05-16T07:08:40.820Z</updated>
    
    <content type="html"><![CDATA[<p>初创公司随着业务的进一步发展,当DAU达到十万的时候,通常是最关键的时刻,既要保证业务的稳定运行,又要进行产品的快速迭代。到了这个阶段,由于业务模式得到了一定的验证和反馈,有可能会出现很多竞品或友商。一方面,随着风险资本的注入,会依赖更有质量的数据进行发展运营,另一方面,竞品的出现又导致了市场的加速前进.因此,能否在这个阶段保证业务与技术的和谐发展,是考验架构是否足够灵活的指标之一,本节主要说明几点。</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#%E5%89%8D%E7%AB%AF%E5%8A%A0%E9%80%9F%E4%BC%98%E5%8C%96">前端加速优化</a></li><li><a href="#%E6%B0%B4%E5%B9%B3%E6%89%A9%E5%B1%95">水平扩展</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E7%BC%93%E5%AD%98%E4%BC%98%E5%8C%96">数据库及缓存优化</a></li></ul></p><h2><span id="前端加速优化"> 前端加速优化</span></h2><p>首先基于浏览器端应用或者移动端应用,随着请求的不断增加,偶尔会看到Web服务出现性能瓶颈导致请求变慢或者失败,除去服务器本身的配置低外,更有可能由于架构设计或分离的原因,大量的Web并发请求被堵塞或者变慢,原因无非是服务器的CPU、磁盘、IO、带宽竞争激烈,导致相互影响,这时候我们就需要对架构进行前后端分解,合理配置或转发请求。</p><p>如果是前端的服务请求来不及处理或者有瓶颈,可以将图片、Js、 CSS、 HTML及应用服务相关的静态资源文件存储通过Nginx本地代理或者对象存储服务来进行物理加速,使用不同的域名来转发请求,并通过CDN将静态资源分布式缓存在各个节点实现“就近访问&quot;,主动或被动刷新CDN的缓存来加速前端服务。</p><p>如果是后端的动态请求压力过大或者有热点服务,可以把无状态的后端的服务再进一步水平扩展满足业务分担,有状态需要判断是否能通过垂直扩容来服务,否则只能进行代码、架构设计或者业务规划的调整来优化。</p><p>一般而言,通过将动态请求、静态请求的访问分离(“动静分离”),能有效解决服务器在CPU、磁盘IO、带宽方面的访问压力。当然,这需要在架构设计时采用一些方法来进行调整。</p><h2><span id="水平扩展"> 水平扩展</span></h2><p>上面提到垂直扩容能解决部分的问题,但由于业务和流量的快速增长及垂直资源有限,不同的应用场景需要依赖不同的策略分流,比如长连接的应用会依赖于4层的网络连接,互联网应用通常采用7层的模式来完成,甚至在游戏场景中,依赖UDP进行通信。</p><p>为了更多地分担服务器的压力和保证业务的高可用,负载均衡技术通常是这个阶段解决问题的一个方法,通过增加多台后端服务器就可以实现分流的功能,分流设计也面临很多原则与技巧,比如分流的路径、权重等。负戴均衡承担的角色也决定了后端的应用架构,比如无状态化设计才能实现水平扩展,另外还要考虑业务是否有亲缘性,同时在后端服务出现异常的情况下, 自动进行健康检查,异常的服务能及时进行下线操作,快速失败。</p><h2><span id="数据库及缓存优化"> 数据库及缓存优化</span></h2><p>数据库和缓存配合使用是解决后端结构化数据与非结构化问题的有效手段,根据不同的场景,要明白哪种数据使用结构化数据合适,哪种数据使用非结构化数据更合适,以及哪种方式在保证性能较好的情况下成本又可以接受。</p><p>同时,如何在数据库和缓存之间进行过渡也是需要考虑的,比如数据在更新的时候,如何保证缓存的一致性,如何保证热点数据一直被访问,提高缓存的命中率等。另外,当大量用户访问不存在的数据时,也有可能导致后端的压力非常大,甚至有可能造成雪崩效应。</p><p>每种服务独立承担对应的功能,各司其职,并且根据应用的特性区别提供不同的服务能力,比如应用服务器提供用户的接入服务,数据库服务专门承担结构化数据的存储,缓存承担或非结构化数据(KV键值对)的存储等,如果要提供搜索的功能,还需将数据进行分词、索引、检索等,不同服务器根据业务的功用需求来提供对应的服务。</p><hr><p>在这个阶段,除了必须保证满足业务的功能型需求,还要更多考虑非功能性需求。比如,通过前端负载均衡提供业务分流的能力,根据用户的特征进行不同的流量转发;数据库提供主备的能力,两者之间通过数据同步进行数据备份,当主数据库发生故障后,应用可以自动切换到备份的服务器来为用户提供服务;在用户体验方面,可能会引入缓存、CDN等基础服务来提供性能加速。</p><p><strong>架构演化发展历程：</strong></p><ol><li><a href="/2018/05/16/architec-initial/">初创期架构</a></li><li><a href="/2018/05/16/architec-growth/">快速成长期架构</a></li><li><a href="/2018/05/16/architec-distri/">分布式服务架构</a></li></ol><p>参考：</p><ol><li>云原生应用架构实践 （网易云基础服务架构团队 著）</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;初创公司随着业务的进一步发展,当DAU达到十万的时候,通常是最关键的时刻,既要保证业务的稳定运行,又要进行产品的快速迭代。到了这个阶段,由于业务模式得到了一定的验证和反馈,有可能会出现很多竞品或友商。一方面,随着风险资本的注入,会依赖更有质量的数据进行发展运营,另一方面,竞品的出现又导致了市场的加速前进.因此,能否在这个阶段保证业务与技术的和谐发展,是考验架构是否足够灵活的指标之一,本节主要说明几点。&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="yunke.science/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="架构" scheme="yunke.science/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>初创期架构</title>
    <link href="yunke.science/2018/05/16/architec-initial/"/>
    <id>yunke.science/2018/05/16/architec-initial/</id>
    <published>2018-05-16T04:28:03.000Z</published>
    <updated>2018-05-16T07:08:45.615Z</updated>
    
    <content type="html"><![CDATA[<p>创业公司在开始新业务的时期,基本处在试错或原型验证阶段,这个阶段更多是关注业务的本身是否有前景或商业模式,而不会把非常多的精力放在技术的系统架构上,尤其是对于非技术型或不确定型的项目立项阶段。尽管很多技术人员也预料到前期需要很多时间去好好设计系统,才能保证支撑后续可能的业务快速发展,但往往由于时间成本或人力等原因而无法很好地执行。</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#%E5%8D%95%E4%BD%93%E6%9E%B6%E6%9E%84">单体架构</a></li><li><a href="#%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%86%E7%A6%BB">服务器分离</a></li><li><a href="#%E4%B8%9A%E5%8A%A1%E6%A8%A1%E5%9E%8B">业务模型</a></li></ul></p><p>一般来讲,创业型的项目对时间的要求非常苛刻,需要在3到6个月时间内完成系统的上线,否则有可能由于业务无法快速上线验证,导致无法获取相关的原始数据进行下<br>个目标验证,更严重的有可能造成资金链的断裂。罗马不是一天建成的,因此这个阶段会使用相对简单的架构方式来进行设计,本节先从最主要的几点进行说明.</p><h2><span id="单体架构"> 单体架构</span></h2><p>对于创业型公司来说,由于人才、技术、资金等重要因素的影响,同时,技术人员为了配合产品的需求,会采用最简单的架构来完成最原始阶段开发,根据我们接触的不少用户反馈,有些企业考虑成本因素,甚至只使用一台服务器或者容器服务。另外,传统官网、论坛等应用,由于早期的设计采用了单体架构来实现,只需要一台服务器或容器来服务即可。对于其他的应用服务器、数据库、静态文件等资源,也是部署到同一台服务器或容器上来服务。最简单的架构模型如图所示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">PHP--&gt;Apache</span><br><span class="line">Apache--&gt;Mysql</span><br></pre></td></tr></table></figure><p>对于早期的单体应用,应用服务+数据库服务基本上就组成了最原始的架构模型,技术人更多会考虑技术的选型,包括编程语言、版本管理、数据库的类型等。比如PHP的开发者选择PHP-MySQL, Java的开发者采用Tomcat-MySOL等开发方式。</p><h2><span id="服务器分离"> 服务器分离</span></h2><p>根据线上运行经验,一般的业务的类型,如果每日的用户访问量在百万级别以内,只要进行简单的Web应用性能参数调优、数据库索引优化等,基本上就能保证服务的稳定运行,当然,随着访问量的不断增加,部署在同一台服务器上的应用及数据库服务,会造成服务器的CPU/内存/磁盘/带宽等系统资源竞争,从而相互影响,显然很容易出现性能瓶颈,如果这台服务器出现了宕机或无法恢复的错误,就有可能导致全站不可访问或者数据丢失等情况,后果非常严重,因此大部分产品会将Web应用服务器和数据库服务器进行物理分离,独立部署,相互热备提供服务,只需要增加很少的成本,就能解决对应性能和数据的可靠性等问题.</p><p>初期由于各种条件存在不能很好地进行新项目前景的预见,技术人员如果能在最小成本的情况下保证架构的合理性,还能很好地服务产品功能需求,甚至只要在部署架构上稍做调整,就可以防止出现灾难性的问题,这其中也包括很多技术架构上的考虑。</p><h2><span id="业务模型"> 业务模型</span></h2><p>一般而言,现阶段的业务比较简单,产品也比较单一,业务会随时根据其运营数据进行调整,因此,这时需要技术人能够较好地把不同的模块分离出来,对于偏业务相关的功能,需要有较好的心态接收随时变化的不确定性,对于后续可能复用或大量依赖的工程,需要进行较好的设计,否则可能在业务爆发时导致业务开发的进度越来越慢,甚至阻碍业务的发展,造成业务时常中断,即使有人力或时间来对系统进行重新设计,也会令技术人员产生抵触心理,同时也会引入较高的风险,因此,基于云原生应用的设计模式在最基础的阶段对架构也有很大的作用,包括考虑如何使用云的弹性,将不可变优势融合到系统的设计中,合理的业务模型分界也是确保后续能发展的重要步骤之一。</p><hr><p>总而言之,在早期项目原型验证或者快速试错阶段,采用单体架构具有很大的技术优势,产品的想法也在项目的初始阶段就能进行比较好的迭代开发,发布和部署也比较灵活。然而,随着业务的增长,如果架构还是一成不变的话,带来的技术风险就越来越高.<br>比如,代码行数的增加影响技术人员的学习成本、业务的变更速度、业务的可靠性、安全性及工程变大后的发布效率等,每次修改都必须反复测试,否则全站随时可能不可用,导致业务中断或者丢失市场的机会,因此,这部分的技术债务必须在业务快速发展的同时,进行技术架构的改造,使其能保证后期业务的支撑,所以,除了业务的发展判断外,对开发人员的技术能力储备和架构远见判断也将成为考虑的事情之一.</p><p><strong>架构演化发展历程：</strong></p><ol><li><a href="/2018/05/16/architec-initial/">初创期架构</a></li><li><a href="/2018/05/16/architec-growth/">快速成长期架构</a></li><li><a href="/2018/05/16/architec-distri/">分布式服务架构</a></li></ol><p>参考：</p><ol><li>云原生应用架构实践 （网易云基础服务架构团队 著）</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;创业公司在开始新业务的时期,基本处在试错或原型验证阶段,这个阶段更多是关注业务的本身是否有前景或商业模式,而不会把非常多的精力放在技术的系统架构上,尤其是对于非技术型或不确定型的项目立项阶段。尽管很多技术人员也预料到前期需要很多时间去好好设计系统,才能保证支撑后续可能的业务快速发展,但往往由于时间成本或人力等原因而无法很好地执行。&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="yunke.science/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="架构" scheme="yunke.science/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>filebeat 5.+ 与 6.+ 版本的主要更改</title>
    <link href="yunke.science/2018/05/04/beats56changelog/"/>
    <id>yunke.science/2018/05/04/beats56changelog/</id>
    <published>2018-05-04T07:56:52.000Z</published>
    <updated>2018-05-04T07:59:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>本节讨论如果将Beats从版本5.x升级到6.x时应注意的主要更改.</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#spooler-%E7%BB%84%E4%BB%B6%E5%88%A0%E9%99%A4">Spooler 组件删除</a></li><li><a href="#%E5%8F%AA%E8%83%BD%E5%90%AF%E7%94%A8%E4%B8%80%E4%B8%AA%E8%BE%93%E5%87%BA">只能启用一个输出</a></li><li><a href="#logstash%E7%B4%A2%E5%BC%95%E8%AE%BE%E7%BD%AE%E7%8E%B0%E5%9C%A8%E9%9C%80%E8%A6%81%E7%89%88%E6%9C%AC">Logstash索引设置现在需要版本</a></li><li><a href="#filebeat-prospector-%E7%B1%BB%E5%9E%8B-%E5%92%8C-document-%E7%B1%BB%E5%9E%8B-%E7%9A%84%E6%94%B9%E5%8F%98">Filebeat prospector 类型 和 document 类型 的改变</a></li><li><a href="#%E5%9C%A8%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E4%B8%AD%E7%A6%81%E7%94%A8filebeat%E9%BB%98%E8%AE%A4%E6%8E%A2%E5%8B%98%E5%99%A8">在配置文件中禁用Filebeat默认探勘器</a></li><li><a href="#%E5%85%B6%E4%BB%96%E6%9B%B4%E6%96%B0%E7%9A%84%E8%AE%BE%E7%BD%AE">其他更新的设置</a></li><li><a href="#%E5%AF%BC%E5%85%A5kibana%E4%BB%AA%E8%A1%A8%E6%9D%BF%E7%9A%84%E6%9B%B4%E6%94%B9">导入Kibana仪表板的更改</a></li><li><a href="#metricbeat%E8%BF%87%E6%BB%A4%E5%99%A8%E9%87%8D%E5%91%BD%E5%90%8D%E4%B8%BA%E5%A4%84%E7%90%86%E5%99%A8">Metricbeat过滤器重命名为处理器</a></li><li><a href="#%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%E6%98%AF%E9%92%88%E5%AF%B9libc%E5%8A%A8%E6%80%81%E7%BC%96%E8%AF%91%E7%9A%84">二进制文件是针对libc动态编译的</a></li></ul></p><p><a href="https://www.elastic.co/guide/en/beats/libbeat/master/breaking-changes-6.0.html" target="_blank" rel="noopener">Beats Platform Reference [master] » Breaking changes » Breaking changes in 6.0</a></p><h2><span id="spooler-组件删除"> Spooler 组件删除</span></h2><p><a href="https://www.elastic.co/guide/en/beats/libbeat/master/breaking-changes-6.0.html#breaking-changes-spooler-removed" target="_blank" rel="noopener">filebeat spooler removed</a></p><p>6.0版本为所有Beats的内部管道提供了一种新的体系结构。这种架构重构主要是内部的，但更明显的效果之一是Filebeat的Spooler组件被删除</p><ul><li><code>filebeat.spool_size</code></li><li><code>filebeat.publish_as</code></li><li><code>filebeat.idle_timeo</code></li><li><code>queue_size</code></li><li><code>bulk_queue_size</code></li></ul><p>前三个特定于<code>Filebeat</code>，而<code>queue_size</code>和<code>bulk_queue_size</code>存在于所有<code>Beats</code>中。如果设置了这些选项中的任何一个，<code>Filebeat 6.0+</code>将拒绝启动。</p><p>引入queue.mem设置，而不是上面的设置。如果您之前必须调整spool_size或queue_size，则可能需要在升级时调整queue.mem.events。但是，最好将queue.mem的其余部分保留为默认值，因为它们适用于所有负载。</p><p>publish_async选项（从5.3开始不推荐使用）被删除，因为新管道默认已经异步工作。</p><h2><span id="只能启用一个输出"> 只能启用一个输出</span></h2><p>6.0之前，您可以同时启用多个输出，但仅限于不同的类型。例如，您可以启用Elasticsearch和Logstash输出，但不能启用两个Logstash输出。</p><p>6.0+删除了同时启用多个输出的选项。这有助于简化管道并明确Beats中输出的范围。</p><p>如果您需要多个输出，您有以下选择：</p><ul><li>使用Logstash输出，然后使用Logstash将事件传送到多个输出</li><li>同一节点运行多个实例</li><li>如果您使用 <code>file or console</code> 输出进行调试，除了主输出之外，我们还建议使用<code>-d &quot;publish&quot;</code>选项，将发布的事件记录在Filebeat日志中。</li></ul><h2><span id="logstash索引设置现在需要版本"> Logstash索引设置现在需要版本</span></h2><p>如果使用 <code>Logstash output</code> 将数据从<code>Beats</code>发送到<code>Logstash</code>，则需要更新<code>Logstash</code>配置中的索引设置以包含<code>Beat</code>版本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output &#123;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    hosts =&gt; &quot;localhost:9200&quot;</span><br><span class="line">    manage_template =&gt; false</span><br><span class="line">    index =&gt; &quot;%&#123;[@metadata][beat]&#125;-%&#123;[@metadata][version]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在6.0之前，推荐的设置是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">index =&gt; &quot;%&#123;[@metadata][beat]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br></pre></td></tr></table></figure><p>6.0的索引模板应用于与模式 <code>[beat]-[version]-*</code> 匹配的新索引。 您必须更新您的<code>Logstash</code>配置，否则模板将不会被应用。</p><h2><span id="filebeat-prospector-类型-和-document-类型-的改变"> Filebeat prospector 类型 和 document 类型 的改变</span></h2><p><a href="https://www.elastic.co/guide/en/beats/libbeat/master/breaking-changes-6.0.html#breaking-changes-types" target="_blank" rel="noopener">Filebeat prospector type and document type changes</a></p><p>来自 <code>prospector</code> 配置的 <code>document_type</code> 设置已被删除，因为<code>_type</code>概念正在从<code>Elasticsearch</code>中移除。 您可以使用自定义字段，而不是<code>document_type</code>设置。</p><p>这也导致了将<code>input_type</code> 重命名为<code>type</code>。 这种改变是向后兼容的，因为旧的设置仍然有效。 但是，<code>input_type</code>输出字段已重命名为<code>prospector.type</code> 。</p><h2><span id="在配置文件中禁用filebeat默认探勘器"> 在配置文件中禁用Filebeat默认探勘器</span></h2><p>Filebeat的默认启动行为（基于包含的示例配置）是读取所有匹配 <code>/var/log/*.log</code> 模式的文件。</p><p>从版本6.0开始，Filebeat不会以默认配置读取任何文件。 但是，您可以轻松启用系统模块，例如使用CLI标志：</p><p><code>filebeat --modules = system</code></p><h2><span id="其他更新的设置"> 其他更新的设置</span></h2><p><code>outputs.elasticsearch.template.*</code> 移动到 <code>setup.template.*</code> 下面，其他保持不变。</p><p><code>dashboards.*</code> 移动到 <code>setup.dashboards.*</code> 下面，其他保持不变。</p><p>Filebeat 移除选项 <code>force_close_files</code> 和 <code>close_older</code> 。</p><h2><span id="导入kibana仪表板的更改"> 导入Kibana仪表板的更改</span></h2><p>用于在先前版本的Beats中加载Kibana仪表板的import_dashboards程序已由setup命令替换。 例如，以下命令：<br><code>./scripts/import_dashboards -user elastic -pass YOUR_PASSWORD</code><br>由以下命令代替：<br><code>./filebeat setup -E &quot;output.elasticsearch.username=elastic&quot; -E &quot;output.elasticsearch.password=YOUR_PASSWORD&quot;</code></p><p>请注意，只有在配置文件中尚未配置Elasticsearch输出的情况下才需要 <code>-E</code> 标志。</p><p>除了命令的更改之外，请注意，加载Kibana仪表板在6.0版本的堆栈中工作方式不同。 在6.0之前，仪表板被直接插入到.kibana Elasticsearch索引中。 从6.0开始，Beats使用Kibana服务器API。 这意味着加载仪表板的Beat需要直接访问Kibana，并且需要设置Kibana URL。 设置Kibana URL的选项是<code>setup.kibana.host</code>，您可以在配置文件中或通过<code>-E CLI</code>标志设置该选项：</p><p><code>./filebeat setup -E &quot;setup.kibana.host=http://kibana-host:5601&quot;</code></p><p>Kibana主机的默认值是 <code>localhost:5601</code>。</p><h2><span id="metricbeat过滤器重命名为处理器"> Metricbeat过滤器重命名为处理器</span></h2><p>在本模块级配置的“本地”处理器过去在Metricbeat中称为过滤器，但它们提供与全局处理器相似的功能。 两者之间的显着区别在于筛选器相对于度量标准集（例如，<code>mount_point</code>）访问字段，而处理器则以其全限定名称（例如<code>system.filesystem.mount_point</code>）引用字段。</p><p>从版本6.0开始，筛选器将重命名为处理器，并且只能使用完全限定名称访问这些字段。</p><h2><span id="二进制文件是针对libc动态编译的"> 二进制文件是针对libc动态编译的</span></h2><p>在6.0之前，使用Cgo编译<code>Metricbeat</code> 和 <code>Packetbeat</code>，而使用纯<code>Go</code>编译器编译<code>Filebeat</code>，<code>Winlogbeat</code>和<code>Heartbeat</code>。 编译Cgo的一个优势是libc是动态编译的。</p><p>从6.0开始，所有Beats都使用Cgo进行编译，因此可以针对libc进行动态编译。 这可以降低二进制文件的可移植性，但是所有受支持的平台都不受影响。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本节讨论如果将Beats从版本5.x升级到6.x时应注意的主要更改.&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="yunke.science/categories/elk/"/>
    
    
      <category term="elk" scheme="yunke.science/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>大规模Elasticsearch集群优化管理</title>
    <link href="yunke.science/2018/05/04/elk6-optimize/"/>
    <id>yunke.science/2018/05/04/elk6-optimize/</id>
    <published>2018-05-04T07:49:33.000Z</published>
    <updated>2018-05-04T07:52:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>简单的优化，让集群发挥更大的效率。</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#%E5%AE%89%E8%A3%85">安装</a></li><li><a href="#%E7%AE%A1%E7%90%86">管理</a><ul><li><a href="#%E5%AF%B9es%E7%9A%84%E7%BB%93%E7%82%B9%E5%81%9A%E8%A7%92%E8%89%B2%E5%88%92%E5%88%86%E5%92%8C%E9%9A%94%E7%A6%BB">对ES的结点做角色划分和隔离</a></li><li><a href="#%E9%81%BF%E5%85%8D%E8%BF%87%E9%AB%98%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6shard%E6%95%B0%E9%87%8F%E5%92%8Cthreadpool%E7%9A%84%E6%95%B0%E9%87%8F">避免过高的并发，控制shard数量和threadpool的数量</a></li><li><a href="#%E5%86%B7%E7%83%AD%E6%95%B0%E6%8D%AE%E6%9C%80%E5%A5%BD%E5%81%9A%E5%88%86%E7%A6%BB">冷热数据最好做分离</a></li><li><a href="#%E4%B8%8D%E5%90%8C%E6%95%B0%E6%8D%AE%E9%87%8F%E7%BA%A7%E7%9A%84shard%E6%9C%80%E5%A5%BD%E9%9A%94%E7%A6%BB%E5%88%B0%E4%B8%8D%E5%90%8C%E7%BB%84%E5%88%AB%E7%9A%84%E7%BB%93%E7%82%B9">不同数据量级的shard最好隔离到不同组别的结点</a></li><li><a href="#%E5%AF%B9%E7%B4%A2%E5%BC%95%E7%9A%84%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C">对索引的一些操作</a><ul><li><a href="#%E5%88%A0%E9%99%A4%E4%B8%8D%E7%94%A8%E7%9A%84%E7%B4%A2%E5%BC%95">删除不用的索引</a></li><li><a href="#%E5%85%B3%E9%97%AD%E7%B4%A2%E5%BC%95-%E6%96%87%E4%BB%B6%E4%BB%8D%E7%84%B6%E5%AD%98%E5%9C%A8%E4%BA%8E%E7%A3%81%E7%9B%98%E5%8F%AA%E6%98%AF%E9%87%8A%E6%94%BE%E6%8E%89%E5%86%85%E5%AD%98-%E9%9C%80%E8%A6%81%E7%9A%84%E6%97%B6%E5%80%99%E5%8F%AF%E4%BB%A5%E9%87%8D%E6%96%B0%E6%89%93%E5%BC%80">关闭索引 （文件仍然存在于磁盘，只是释放掉内存）。需要的时候可以重新打开。</a></li><li><a href="#%E5%AE%9A%E6%9C%9F%E5%81%9A%E7%B4%A2%E5%BC%95%E7%9A%84force-merge">定期做索引的force merge</a></li></ul></li><li><a href="#%E5%BD%93%E8%8A%82%E7%82%B9%E7%A6%BB%E5%BC%80%E6%97%B6%E5%BB%B6%E8%BF%9F%E5%88%86%E9%85%8D">当节点离开时延迟分配</a></li></ul></li><li><a href="#%E7%9B%91%E6%8E%A7">监控</a></li></ul></p><h2><span id="安装"> 安装</span></h2><ol><li><p>从一开始，哪怕就只有几个node，就应该使用分布式配置管理工具来做集群的部署。随着应用的成熟，集群规模的逐步扩大，效率的提升会凸显。 官方提供了 <a href="https://github.com/elastic/puppet-elasticsearch" target="_blank" rel="noopener">puppet-elasticsearch</a> 和 Chef <a href="https://github.com/elastic/cookbook-elasticsearch" target="_blank" rel="noopener">cookbook-elasticsearch</a> ，熟悉这两个工具的同学可以直接拿过来用。用熟这类工具，对于集群的初始部署，配置批量更改，集群版本升级，重启故障结点都会快捷和安全许多。</p></li><li><p>第二个必备利器就是sense插件。通过这个插件直接调用集群的restful API，在做集群和索引的状态查看，索引配置更改的时候非常方便。语法提示和自动补全功能更是实用，减少了翻看文档的频率。在Kibana5里面，sense已经成为一个内置的控制台，无需额外安装。</p></li></ol><h2><span id="管理"> 管理</span></h2><h3><span id="对es的结点做角色划分和隔离"> 对ES的结点做角色划分和隔离</span></h3><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html" target="_blank" rel="noopener">Elasticsearch Reference  » Modules » Node</a></p><p>默认情况下，每个节点都有成为主节点的资格，也会存储数据，还会处理客户端的请求。 这对于小型群集非常方便，但随着群集的增长，考虑将专用主节点从专用数据节点中分离出来非常重要。</p><p>对于一个规模较大，用户较多的集群，master和client在一些极端使用情况下可能会有性能瓶颈甚至内存溢出，从而使得共存的data node故障。data node的故障恢复涉及到数据的迁移，对集群资源有一定消耗，容易造成数据写入延迟或者查询减慢。如果将master和client独立出来，一旦出现问题，重启后几乎是瞬间就恢复的，对用户几乎没有任何影响。另外将这些角色独立出来的以后，也将对应的计算资源消耗从data node剥离出来，更容易掌握data node资源消耗与写入量和查询量之间的联系，便于做容量管理和规划。</p><p>所有节点都知道群集中的所有其他节点，并且可以将客户端请求转发到适当的节点。 除此之外，每个节点都有一个或多个目的：</p><ul><li>具有master资格的节点<br><code>node.master</code>设置为<code>true</code> （默认）的节点，这使得它有资格被选为控制集群的主节点 。</li><li>数据节点<br><code>node.data</code>设置为<code>true</code>节点（默认）。 数据节点保存数据并执行数据相关的操作，如CRUD，搜索和聚合。</li><li>数据提取节点,client节点<br><code>node.ingest</code>设置为<code>true</code>节点（默认）。 摄取节点可以执行由一个或多个摄取处理器组成的预处理流水线。 根据摄取处理器执行的操作类型和所需资源，具有专用摄入节点可能是有意义的，它只会执行此特定任务。负责处理用户请求，实现请求转发，负载均衡等功能。</li><li>多个集群查询请求分发节点<br>通过<code>tribe.*</code>设置配置的集群节点是一种特殊类型的协调节点，可以连接到多个群集并在所有连接的群集中执行搜索和其他操作。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">通过将对应的参数设置为 true ，来启用某个功能  </span><br><span class="line">node.master: true </span><br><span class="line">node.data: false </span><br><span class="line">node.ingest: false </span><br><span class="line">search.remote.connect: false </span><br><span class="line"></span><br><span class="line">node.master 角色默认启用。</span><br><span class="line">禁用 node.data 角色（默认情况下启用）。</span><br><span class="line">禁用 node.ingest 角色（默认情况下启用）。</span><br><span class="line">禁用跨群集搜索（默认启用）。</span><br></pre></td></tr></table></figure><h3><span id="避免过高的并发控制shard数量和threadpool的数量"> 避免过高的并发，控制shard数量和threadpool的数量</span></h3><p>在写入量和查询性能能够满足的前提下，为索引分配尽量少的分片。</p><p>分片过多会带来诸多负面影响，例如：每次查询后需要汇总排序的数据更多；过多的并发带来的线程切换造成过多的CPU损耗；索引的删除和配置更新更慢Issue#18776;<br>过多的shard也带来更多小的segment，而过多的小segment会带来非常显著的heap内存消耗，特别是如果查询线程配置得很多的情况下。</p><p>配置过大的threadpool更是会产生很多诡异的性能问题Issue#18161里所描述的问题就是我们所经历过的。 默认的Theadpool大小一般来说工作得很不错了。</p><h3><span id="冷热数据最好做分离"> 冷热数据最好做分离</span></h3><p>对于日志型应用来说，一般是每天建立一个新索引，当天的热索引在写入的同时也会有较多的查询。如果上面还存有比较长时间之前的冷数据，那么当用户做大跨度的历史数据查询的时候，过多的磁盘IO和CPU消耗很容易拖慢写入，造成数据的延迟。</p><p>所以我们用了一部分机器来做冷数据的存储，利用ES可以给结点配置自定义属性的功能，为冷结点加上&quot;boxtype&quot;:&quot;weak&quot;的标识，每晚通过维护脚本更新冷数据的索引路由设置index.routing.allocation.{require|include|exclude}，让数据自动向冷结点迁移。</p><p>冷数据的特性是不再写入，用户查的频率较低，但量级可能很大。比如我们有个索引每天2TB，并且用户要求保持过去90天数据随时可查。保持这么大量的索引为open状态，并非只消耗磁盘空间。ES为了快速访问磁盘上的索引文件，需要在内存里驻留一些数据(索引文件的索引)，也就是所谓的segment memory。</p><p>稍微熟悉ES的同学知道，JVM heap分配不能超过32GB，对于我们128GB RAM, 48TB磁盘空间的机器而言，如果只跑一个ES实例，只能利用到32GB不到的heap，当heap快用饱和的时候，磁盘上保存的索引文件还不到10TB，这样显然是不经济的。 因此我们决定在冷结点上跑3个ES实例，每个分配31GB heap空间，从而可以在一台物理服务器上存储30多TB的索引数据并保持open状态，供用户随时搜索。 实际使用下来，由于冷数据搜索频率不高，也没有写入，即时只剩余35GB内存给os做文件系统缓存，查询性能还是可以满足需求的。</p><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/shard-allocation-filtering.html" target="_blank" rel="noopener">索引分片分配过滤</a><br>碎片分配过滤允许您指定允许哪些节点托管特定索引的碎片。</p><p>在启动时可以为每个节点分配任意的元数据属性。 例如，可以为节点分配一个rack和一个size属性，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># elasticsearch.yml</span><br><span class="line">node.attr.rack: rack1</span><br><span class="line">node.attr.size: big</span><br></pre></td></tr></table></figure><p>这些元数据属性可以与<code>index.routing.allocation.*</code>设置一起使用，以将索引分配给特定的一组节点。   例如，我们可以将索引test移动到big或medium节点，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PUT test/_settings</span><br><span class="line">&#123;</span><br><span class="line">  &quot;index.routing.allocation.include.size&quot;: &quot;big,medium&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>或者，我们可以使用exclude规则将索引test 从 small节点移开：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PUT test/_settings</span><br><span class="line">&#123;</span><br><span class="line">  &quot;index.routing.allocation.exclude.size&quot;: &quot;small&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以指定多个规则，在这种情况下必须满足所有条件。 例如，我们可以使用以下命令将索引test移动到rack1 big节点上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">PUT test/_settings</span><br><span class="line">&#123;</span><br><span class="line">  &quot;index.routing.allocation.include.size&quot;: &quot;big&quot;,</span><br><span class="line">  &quot;index.routing.allocation.include.rack&quot;: &quot;rack1&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>如果某些条件不能满足，则碎片不会被移动。</em><br>以下设置是动态的 ，允许将活动索引从一组节点移动到另一组节点：</p><ul><li><code>index.routing.allocation.include.{attribute}</code> 将索引分配给<code>{attribute}</code>至少有一个逗号分隔值的节点。</li><li><code>index.routing.allocation.require.{attribute}</code> 将索引分配给<code>{attribute}</code>具有所有逗号分隔值的节点。</li><li><code>index.routing.allocation.exclude.{attribute}</code> 将索引分配给<code>{attribute}</code>包含任何逗号分隔值的节点。</li></ul><p>这些特殊属性也受支持：</p><table><thead><tr><th>attr</th><th>说明</th></tr></thead><tbody><tr><td><code>_name</code></td><td>按节点名称匹配节点</td></tr><tr><td><code>_host_ip</code></td><td>通过主机IP地址匹配节点（与主机名关联的IP）</td></tr><tr><td><code>_publish_ip</code></td><td>通过发布IP地址匹配节点</td></tr><tr><td><code>_ip</code></td><td>匹配<code>_host_ip</code>或<code>_publish_ip</code></td></tr><tr><td><code>_host</code></td><td>通过主机名匹配节点</td></tr></tbody></table><p>所有属性值都可以用通配符指定，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PUT test/_settings</span><br><span class="line">&#123;</span><br><span class="line">  &quot;index.routing.allocation.include._ip&quot;: &quot;192.168.2.*&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3><span id="不同数据量级的shard最好隔离到不同组别的结点"> 不同数据量级的shard最好隔离到不同组别的结点</span></h3><p>大家知道ES会自己平衡shard在集群的分布，这个自动平衡的逻辑主要考量三个因素。</p><p>其一同一索引下的shard尽量分散到不同的结点;其二每个结点上的shard数量尽量接近;其三结点的磁盘有足够的剩余空间。<br>这个策略只能保证shard数量分布均匀，而并不能保证数据大小分布均匀。</p><p>实际应用中，我们有200多种索引，数据量级差别很大，大的一天几个TB，小的一个月才几个GB，并且每种类型的数据保留时长又千差万别。抛出的问题，就是如何能比较平衡并充分的利用所有节点的资源。</p><p>针对这个问题，我们还是<strong>通过对结点添加属性标签来做分组，结合index routing控制的方式来做一些精细化的控制</strong>。<strong>尽量让不同量级的数据使用不同组别的结点，使得每个组内结点上的数据量比较容易自动平衡</strong>。</p><h3><span id="对索引的一些操作"> 对索引的一些操作</span></h3><p>ES的查询速度和缓存相关，对于内存的消耗，和很多因素相关，诸如数据总量、mapping设置、查询方式、查询频度等等。<br>那么有哪些途径减少data node上的segment memory占用呢？ 总结起来有三种方法:</p><h4><span id="删除不用的索引"> 删除不用的索引</span></h4><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-delete.html" target="_blank" rel="noopener">Delete API</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DELETE /twitter/_doc/1</span><br></pre></td></tr></table></figure><h4><span id="关闭索引-文件仍然存在于磁盘只是释放掉内存-需要的时候可以重新打开"> 关闭索引 （文件仍然存在于磁盘，只是释放掉内存）。需要的时候可以重新打开。</span></h4><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.2/indices-open-close.html#indices-open-close" target="_blank" rel="noopener">Open / Close Index API</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">POST /my_index/_close</span><br><span class="line"></span><br><span class="line">POST /my_index/_open</span><br></pre></td></tr></table></figure><p>所有操作可以使用<code>_all</code>作为索引名称或指定识别它们的模式（例如<code>*</code> ），立即打开或关闭。</p><h4><span id="定期做索引的force-merge"> 定期做索引的force merge</span></h4><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-forcemerge.html" target="_blank" rel="noopener">Elasticsearch Reference  » Indices APIs » Force Merge</a></p><p>强制合并API允许强制通过API合并一个或多个索引。 合并涉及Lucene索引在每个分片中保存的分段数量。 强制合并操作允许通过合并它们来减少分段的数量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">POST /twitter/_forcemerge?max_num_segments=1</span><br></pre></td></tr></table></figure><p>强制合并API可以通过一次调用应用于多个索引，甚至可以<code>_all</code>所有索引。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">POST /kimchy,elasticsearch/_forcemerge</span><br><span class="line"></span><br><span class="line">POST /_forcemerge</span><br></pre></td></tr></table></figure><p><em>max_num_segments<br>要合并到的段数。 要完全合并索引，请将其设置为1 。 默认仅仅检查一个合并是否需要执行，如果是的话，执行它。</em></p><p>合并示例：</p><p><code>GET /_cat/indices?v</code><br>原始索引 segment的memory占用情况,size.memory之和为 <code>1147554</code><br><code>GET /_cat/segments/nginx-mse-2018.04.19?h=index,shard,segment,size,size.memory</code></p><p>索引 <code>nginx-mse-2018.04.19</code> 的 <code>segments</code> 数量：<br><code>GET /_cat/segments/nginx-mse-2018.04.19?v</code><br>共有 81 行。</p><p><strong>执行合并操作：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">POST /nginx-mse-2018.04.18,nginx-mse-2018.04.19,nginx-mse-2018.04.20/_forcemerge?max_num_segments=1</span><br></pre></td></tr></table></figure><p>segment的memory占用情况比较，size.memory之和为 <code>492797</code>,减少<code>1/2</code>多一点<br><code>GET /_cat/segments/nginx-mse-2018.04.19?h=index,shard,segment,size,size.memory</code></p><p>查看索引的 <code>segments</code>数量：<br>合并后，segments实际个数为logstash客户端个数。</p><p>查看节点 <code>segments</code> 内存占用情况<br><code>get /_cat/nodes?v&amp;h=segments.count,segments.memory,segments.index_writer_memory,segments.version_map_memory,segments.fixed_bitset_memory</code></p><h3><span id="当节点离开时延迟分配"> 当节点离开时延迟分配</span></h3><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/delayed-allocation.html#delayed-allocation" target="_blank" rel="noopener">Elasticsearch Reference » Index Modules » Index Shard Allocation » Delaying allocation when a node leaves</a></p><p>当一个节点出于任何原因离开集群时，主动地或以其他方式作出反应：</p><ul><li>将副本分片提升为主节点以替换节点上的任何初选。</li><li>分配副本分片以替换缺失的副本（假设有足够的节点）。</li><li>将剩余节点上的碎片均衡重新平衡。</li></ul><p>这些操作旨在通过确保每个碎片尽快完全复制来保护集群免遭数据丢失。</p><p>尽管我们在节点级别和集群级别上节省并发恢复，但这种“碎片整理”仍然会给群集带来很多额外的负载，如果缺少的节点很快可能会返回，这可能不是必需的。</p><p>由于节点已离开而变成未分配副本碎片的分配可以通过<code>index.unassigned.node_left.delayed_timeout</code>动态设置进行延迟，默认值为1m 。</p><p>此设置可以在实时索引（或所有索引）上更新：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PUT _all/_settings</span><br><span class="line">&#123;</span><br><span class="line">  &quot;settings&quot;: &#123;</span><br><span class="line">    &quot;index.unassigned.node_left.delayed_timeout&quot;: &quot;5m&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>永久删除节点<br>如果一个节点不会返回，并且您希望Elasticsearch立即分配丢失的碎片，只需将超时更新为零即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PUT _all/_settings</span><br><span class="line">&#123;</span><br><span class="line">  &quot;settings&quot;: &#123;</span><br><span class="line">    &quot;index.unassigned.node_left.delayed_timeout&quot;: &quot;0&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="监控"> 监控</span></h2><p>不差钱没空折腾的建议还是买官方的xpack省心，有精力折腾的，利用ES各种丰富的stats api，用自己熟悉的监控工具采集数据，可视化出来就好了。</p><p>那么多监控指标，最最关键的还是以下几类:</p><p>各类Thread pool的使用情况，active/queue/reject 可视化出来。 <code>get /_nodes/stats</code></p><p>判断集群是否有性能瓶颈了，看看业务高峰期各类queue是不是很高，reject是不是经常发生，基本可以做到心里有数。</p><p>JVM的heap used%以及old GC的频率，如果old GC频率很高，并且多次GC过后heap used%几乎下不来，说明heap压力太大，要考虑扩容了。（也有可能是有问题的查询或者聚合造成的，需要结合用户访问记录来判断)。</p><p>Segment memory大小和Segment的数量。节点上存放的索引较多的时候，这两个指标就值得关注，要知道segment memory是常驻heap不会被GC回收的，因此当heap压力太大的时候，可以结合这个指标判断是否是因为节点上存放的数据过多，需要扩容。Segement的数量也是比较关键的，如果小的segment非常多，比如有几千，即使segment memory本身不多，但是在搜索线程很多的情况下，依然会吃掉相当多的heap，原因是lucene为每个segment会在thread local里记录状态信息，这块的heap内存开销和(segment数量* thread数量)相关。</p><p>很有必要记录用户的访问记录。我们只开放了http api给用户，前置了一个nginx做http代理，将用户第三方api的访问记录通过access log全部记录下来。通过分析访问记录，可以在集群出现性能问题时，快速找到问题根源，对于问题排查和性能优化都很有帮助。</p><p>参考链接：</p><ol><li><a href="https://elasticsearch.cn/article/110" target="_blank" rel="noopener">Day1: 大规模Elasticsearch集群管理心得</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/delayed-allocation.html" target="_blank" rel="noopener">Delaying allocation when a node leaves</a></li><li><a href="http://www.cnblogs.com/liang1101/p/7284205.html" target="_blank" rel="noopener">http://www.cnblogs.com/liang1101/p/7284205.html</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/shard-allocation-filtering.html" target="_blank" rel="noopener">Shard Allocation Filtering</a></li><li><a href="https://elasticsearch.cn/article/32" target="_blank" rel="noopener">ES内存那点事</a></li><li><a href="http://docs.flycloud.me/docs/ELKStack/elasticsearch/monitor/api/node-stats.html" target="_blank" rel="noopener">节点状态监控接口</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简单的优化，让集群发挥更大的效率。&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="yunke.science/categories/elk/"/>
    
    
      <category term="elk" scheme="yunke.science/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>elasticsearch 加快搜索速度</title>
    <link href="yunke.science/2018/05/01/elk-searchspeed/"/>
    <id>yunke.science/2018/05/01/elk-searchspeed/</id>
    <published>2018-05-01T08:58:03.000Z</published>
    <updated>2018-05-04T03:01:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>Elastic Stack 在最近两年迅速崛起，成为机器数据分析，或者说实时日志处理领域，开源界的第一选择。和传统的日志处理方案相比，Elastic Stack 具有如下几个优点：</p><ol><li>处理方式灵活,实时全文索引;</li><li>配置简易上手,采用 JSON 接口;</li><li>检索性能高效,实时计算基本可以达到全天数据查询的秒级响应；</li><li>集群线性扩展;</li><li>Kibana仪表板操作炫丽.</li></ol><a id="more"></a><h1><span id="elasticsearch-加快搜索速度的几种方法"> elasticsearch 加快搜索速度的几种方法</span></h1><p><ul class="markdownIt-TOC"><li><a href="#%E7%BB%99%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%BC%93%E5%AD%98%E7%95%99%E5%87%BA%E5%86%85%E5%AD%98">给文件系统缓存留出内存</a></li><li><a href="#%E4%BD%BF%E7%94%A8%E6%9B%B4%E5%BF%AB%E7%9A%84%E7%A1%AC%E4%BB%B6">使用更快的硬件</a></li><li><a href="#%E6%96%87%E6%A1%A3%E5%BB%BA%E6%A8%A1">文档建模</a></li><li><a href="#%E9%A2%84%E5%85%88%E7%B4%A2%E5%BC%95%E6%95%B0%E6%8D%AE">预先索引数据</a></li><li><a href="#%E6%98%A0%E5%B0%84">映射</a></li><li><a href="#%E9%81%BF%E5%85%8D%E8%84%9A%E6%9C%AC">避免脚本</a></li><li><a href="#%E6%90%9C%E7%B4%A2%E6%97%A5%E6%9C%9F%E8%8C%83%E5%9B%B4">搜索日期范围</a></li><li><a href="#%E5%BC%BA%E5%88%B6%E5%90%88%E5%B9%B6%E5%8F%AA%E8%AF%BB%E7%B4%A2%E5%BC%95">强制合并只读索引</a></li><li><a href="#%E5%B0%86%E5%85%A8%E5%B1%80%E5%BA%8F%E5%8F%B7%E5%8A%A0%E5%85%A5%E7%83%AD%E6%95%B0%E6%8D%AE">将全局序号加入热数据</a></li><li><a href="#%E9%A2%84%E7%83%AD%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%BC%93%E5%AD%98">预热文件系统缓存</a></li><li><a href="#%E5%B0%86%E6%A0%87%E8%AF%86%E7%AC%A6%E6%98%A0%E5%B0%84%E4%B8%BAkeyword">将标识符映射为<code>keyword</code></a></li><li><a href="#%E4%BD%BF%E7%94%A8%E7%B4%A2%E5%BC%95%E6%8E%92%E5%BA%8F%E6%9D%A5%E5%8A%A0%E9%80%9F%E8%BF%9E%E6%8E%A5">使用索引排序来加速连接。</a></li><li><a href="#%E4%BD%BF%E7%94%A8-preference-%E6%9D%A5%E4%BC%98%E5%8C%96%E7%BC%93%E5%AD%98%E5%88%A9%E7%94%A8%E7%8E%87">使用 <code>preference</code> 来优化缓存利用率</a></li><li><a href="#%E5%89%AF%E6%9C%AC%E5%8F%AF%E8%83%BD%E6%9C%89%E5%8A%A9%E4%BA%8E%E5%90%9E%E5%90%90%E9%87%8F%E4%BD%86%E5%B9%B6%E9%9D%9E%E6%80%BB%E6%98%AF%E5%A6%82%E6%AD%A4">副本可能有助于吞吐量，但并非总是如此</a></li><li><a href="#%E6%89%93%E5%BC%80%E8%87%AA%E9%80%82%E5%BA%94%E5%89%AF%E6%9C%AC%E9%80%89%E6%8B%A9">打开自适应副本选择</a></li></ul></p><h2><span id="给文件系统缓存留出内存"> 给文件系统缓存留出内存</span></h2><p>Elasticsearch严重依赖文件系统缓存来快速搜索。<br>通常，您应该确保至少为文件系统缓存预留一半的可用内存，以便Elasticsearch可以将索引的热数据保留在物理内存中。</p><h2><span id="使用更快的硬件"> 使用更快的硬件</span></h2><p>如果您的搜索依赖大量的 I/O，您应该研究为文件系统缓存提供更多内存（见上文）或购买更快的磁盘IO。 SSD磁盘比普通磁盘执行得更好。</p><p>始终使用本地存储，应避免使用远程文件系统（如NFS或SMB） 。 还要注意虚拟化存储，如亚马逊的Elastic Block Storage 。<br>虚拟化存储在Elasticsearch上运行得非常好，而且它非常吸引人，因为它设置起来非常快速且简单，但与专用本地存储相比，它本质上不太稳定。<br>如果您在EBS上创建索引，请务必使用预配置的IOPS，否则操作可能会迅速受到限制。</p><p>如果您的搜索是CPU依赖型的，则应该购买更快的CPU。</p><h2><span id="文档建模"> 文档建模</span></h2><p>文件应该建模，以便搜索时间操作尽可能简洁。</p><p>特别是，应该避免使用joins 。 nested可以使查询慢几倍， parent-child关系可以使查询速度降低数百倍。<br>因此，如果同样的问题可以通过规范化文档来解决，那么就可以获取显著的速度。</p><h2><span id="预先索引数据"> 预先索引数据</span></h2><p>您应该利用查询中的模式来优化数据的索引方式。</p><p>例如，如果所有的文档都有一个price字段，并且大多数查询在一个固定的范围列表上运行range聚合，那么您可以通过将范围预先索引到索引中并使用terms聚合来加快这个聚合。</p><p>例如，如果文档看起来像这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">PUT index/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  &quot;designation&quot;: &quot;spoon&quot;,</span><br><span class="line">  &quot;price&quot;: 13</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>搜索请求如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">GET index/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;price_ranges&quot;: &#123;</span><br><span class="line">      &quot;range&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;price&quot;,</span><br><span class="line">        &quot;ranges&quot;: [</span><br><span class="line">          &#123; &quot;to&quot;: 10 &#125;,</span><br><span class="line">          &#123; &quot;from&quot;: 10, &quot;to&quot;: 100 &#125;,</span><br><span class="line">          &#123; &quot;from&quot;: 100 &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后可以在索引时通过 price_range 字段丰富文档，该字段应该映射为<code>keyword</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">PUT index</span><br><span class="line">&#123;</span><br><span class="line">  &quot;mappings&quot;: &#123;</span><br><span class="line">    &quot;_doc&quot;: &#123;</span><br><span class="line">      &quot;properties&quot;: &#123;</span><br><span class="line">        &quot;price_range&quot;: &#123;</span><br><span class="line">          &quot;type&quot;: &quot;keyword&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PUT index/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  &quot;designation&quot;: &quot;spoon&quot;,</span><br><span class="line">  &quot;price&quot;: 13,</span><br><span class="line">  &quot;price_range&quot;: &quot;10-100&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后搜索请求可以聚合这个新字段<code>price_range</code>，而不是在<code>price</code>字段上运行<code>range</code> 聚合。</p><h2><span id="映射"> 映射</span></h2><p>一些数据是数字的事实并不意味着它应该总是被映射为数字字段 <code>numeric field</code> 。 通常，存储标识符(如ISBN或标识来自另一个数据库的记录的任何数字的字段)映射为<code>keyword</code>而非<code>integer</code>或<code>long integer</code>,或许会更好 。</p><h2><span id="避免脚本"> 避免脚本</span></h2><p>一般来说，脚本应该避免。 如果他们绝对需要，你应该更喜欢<code>painless</code>和<code>expressions</code>引擎。</p><h2><span id="搜索日期范围"> 搜索日期范围</span></h2><p>now使用的日期字段的查询通常无法缓存，因为匹配的范围一直在变化。然而，在用户体验方面，切换到一个整数日期通常是可以接受的，并且有更好地使用查询缓存的好处。</p><p>例如下面的查询：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">PUT index/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  &quot;my_date&quot;: &quot;2016-05-11T16:30:55.328Z&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">GET index/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;constant_score&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: &#123;</span><br><span class="line">        &quot;range&quot;: &#123;</span><br><span class="line">          &quot;my_date&quot;: &#123;</span><br><span class="line">            &quot;gte&quot;: &quot;now-1h&quot;,</span><br><span class="line">            &quot;lte&quot;: &quot;now&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以替换为以下查询：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">GET index/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;constant_score&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: &#123;</span><br><span class="line">        &quot;range&quot;: &#123;</span><br><span class="line">          &quot;my_date&quot;: &#123;</span><br><span class="line">            &quot;gte&quot;: &quot;now-1h/m&quot;,</span><br><span class="line">            &quot;lte&quot;: &quot;now/m&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这种情况下，我们四舍五入到分钟，所以如果当前时间是<code>16:31:29</code>，范围查询将匹配my_date字段的值介于<code>15:31:00</code>和<code>16:31:59</code>之间的所有内容。<br>如果多个用户在同一分钟内运行包含此范围的查询，则查询缓存可以帮助加快速度。<br>用于舍入的时间间隔越长，查询缓存可以提供的帮助就越多，但要注意过于积极的舍入也可能会伤害用户体验。</p><p><em>注意<br>为了能够利用查询缓存，可能很容易将范围分割成一个大的可缓存部分和较小的不可缓存部分，如下所示：</em></p><p>以下查询分为三个部分：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">GET index/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;constant_score&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: &#123;</span><br><span class="line">        &quot;bool&quot;: &#123;</span><br><span class="line">          &quot;should&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">              &quot;range&quot;: &#123;</span><br><span class="line">                &quot;my_date&quot;: &#123;</span><br><span class="line">                  &quot;gte&quot;: &quot;now-1h&quot;,</span><br><span class="line">                  &quot;lte&quot;: &quot;now-1h/m&quot;</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">              &quot;range&quot;: &#123;</span><br><span class="line">                &quot;my_date&quot;: &#123;</span><br><span class="line">                  &quot;gt&quot;: &quot;now-1h/m&quot;,</span><br><span class="line">                  &quot;lt&quot;: &quot;now/m&quot;</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">              &quot;range&quot;: &#123;</span><br><span class="line">                &quot;my_date&quot;: &#123;</span><br><span class="line">                  &quot;gte&quot;: &quot;now/m&quot;,</span><br><span class="line">                  &quot;lte&quot;: &quot;now&quot;</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但是，这种做法可能会使查询在某些情况下运行速度变慢，因为bool查询引入的开销可能会因更好地利用查询缓存而失败。</p><h2><span id="强制合并只读索引"> 强制合并只读索引</span></h2><p>只读的索引的多个分段合并到单个分段中将会更好的查询速度。 基于时间的索引通常就是这种情况：只有当前时间帧的索引正在获取新文档，而旧索引是只读的。<br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-forcemerge.html" target="_blank" rel="noopener">merged down to a single segment</a> 。</p><p><em>重要<br>不要强制合并仍在写入的索引 - 将合并留给后台合并进程。</em></p><h2><span id="将全局序号加入热数据"> 将全局序号加入热数据</span></h2><p>全局序号是用于在<code>keyword</code>字段上运行<code>terms</code>聚合的数据结构。<br>由于Elasticsearch不知道哪些字段将用于<code>terms</code>聚合以及哪些字段不会使用，所以它们在内存中被延迟加载。<br>您可以通过如下所述配置映射来告诉Elasticsearch在刷新时刻加载全局序号：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">PUT index</span><br><span class="line">&#123;</span><br><span class="line">  &quot;mappings&quot;: &#123;</span><br><span class="line">    &quot;_doc&quot;: &#123;</span><br><span class="line">      &quot;properties&quot;: &#123;</span><br><span class="line">        &quot;foo&quot;: &#123;</span><br><span class="line">          &quot;type&quot;: &quot;keyword&quot;,</span><br><span class="line">          &quot;eager_global_ordinals&quot;: true</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="预热文件系统缓存"> 预热文件系统缓存</span></h2><p>如果运行Elasticsearch的计算机重新启动，则文件系统缓存将为空，因此操作系统将索引的热区域加载到内存中需要一些时间，以便搜索操作很快。 您可以使用<code>index.store.preload</code>设置根据文件扩展名明确告诉操作系统哪些文件应该加载到内存中。</p><p><em>警告<br>如果文件系统缓存不够大，无法容纳所有数据，那么将数据加载到文件系统缓存中的索引过多或文件太多会使搜索速度变慢。 谨慎使用。</em></p><h2><span id="将标识符映射为keyword"> 将标识符映射为<code>keyword</code></span></h2><p>当你的文档中有数字标识符时，很容易将它们映射为数字，这与他们的json类型一致。<br>但是，Elasticsearch索引编号优化范围查询的方式，<code>keyword</code>字段在词条查询方面更好。 由于范围查询中从不使用标识符，因此应将其映射为<code>keyword</code>。</p><h2><span id="使用索引排序来加速连接"> 使用索引排序来加速连接。</span></h2><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-index-sorting.html" target="_blank" rel="noopener">索引排序</a>是有用的，以便使连接更快而以略微慢的索引为代价。在<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-index-sorting-conjunctions.html" target="_blank" rel="noopener">索引分类文档</a>中阅读更多信息。</p><h2><span id="使用-preference-来优化缓存利用率"> 使用 <code>preference</code> 来优化缓存利用率</span></h2><p>有多个缓存可以帮助提高搜索性能，例如文件系统缓存，请求缓存或查询缓存。<br>然而，所有这些缓存都维护在节点级别，这意味着如果您连续运行两次相同的请求，请使用一个或多个副本并使用循环，默认路由算法，那么这两个请求将转到不同的分片副本 ，阻止节点级别的缓存帮助。</p><p>由于搜索应用程序的用户一个接一个地运行类似请求很常见，例如为了分析索引的较窄子集，使用标识当前用户或会话的首选项值可以帮助优化高速缓存的使用。</p><h2><span id="副本可能有助于吞吐量但并非总是如此"> 副本可能有助于吞吐量，但并非总是如此</span></h2><p>除了提高弹性外，副本可以帮助提高吞吐量。例如，如果您有单个分片索引和三个节点，则需要将副本数设置为2，以便共有3个分片副本，以便使用所有节点。</p><p>现在想象一下你有一个2个分片索引和两个节点。在一种情况下，副本数量为0，这意味着每个节点拥有一个分片。在第二种情况下，副本的数量是1，这意味着每个节点都有两个碎片。哪种设置在搜索性能方面表现最佳？通常，每个节点总共拥有更少分片的设置将会表现更好。原因在于它将可用文件系统缓存的份额提高到了每个碎片，文件系统缓存可能是Elasticsearch的1号性能因子。同时，要注意，没有副本的设置在单节点故障的情况下会出现故障，因此需要在吞吐量和可用性之间进行权衡。</p><p>那么副本的合理的数量是多少？如果您有一个具有<code>num_nodes</code>节点的群集，则总数为<code>num_primaries</code>主分片，如果您希望能够一次处理<code>max_failures</code>节点故障，那么适合您的副本数量为<code>max(max_failures, ceil(num_nodes / num_primaries) - 1)</code>。</p><p><em>默认为五个分片，2个副本</em></p><h2><span id="打开自适应副本选择"> 打开自适应副本选择</span></h2><p>当存在多个数据副本时，elasticsearch可以使用一组称为<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html#search-adaptive-replica" target="_blank" rel="noopener">自适应副本选择</a>的条件，根据包含碎片每个副本的节点的响应时间，服务时间和队列大小选择最佳数据副本。 这可以提高查询吞吐量并减少搜索量大的应用程序的延迟。</p><p>参考链接：</p><ol><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-search-speed.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-search-speed.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Elastic Stack 在最近两年迅速崛起，成为机器数据分析，或者说实时日志处理领域，开源界的第一选择。和传统的日志处理方案相比，Elastic Stack 具有如下几个优点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;处理方式灵活,实时全文索引;&lt;/li&gt;
&lt;li&gt;配置简易上手,采用 JSON 接口;&lt;/li&gt;
&lt;li&gt;检索性能高效,实时计算基本可以达到全天数据查询的秒级响应；&lt;/li&gt;
&lt;li&gt;集群线性扩展;&lt;/li&gt;
&lt;li&gt;Kibana仪表板操作炫丽.&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="elk" scheme="yunke.science/categories/elk/"/>
    
    
      <category term="elk" scheme="yunke.science/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>微服务部署：蓝绿部署、滚动部署、AB测试发布、灰度发布</title>
    <link href="yunke.science/2018/05/01/deploy-method/"/>
    <id>yunke.science/2018/05/01/deploy-method/</id>
    <published>2018-05-01T02:39:58.000Z</published>
    <updated>2018-05-01T02:43:38.544Z</updated>
    
    <content type="html"><![CDATA[<p>在项目迭代的过程中，不可避免需要<code>上线</code>。上线对应着部署，或者重新部署；部署对应着修改；修改则意味着风险。<br>目前有很多用于部署的技术，有的简单，有的复杂；有的需要停机，有的不需要停机即可完成部署。本文的目的就是将目前常用的布署方案做一个总结。</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#%E8%93%9D%E7%BB%BF%E9%83%A8%E7%BD%B2-bluegreen-deployment">蓝绿部署 Blue/Green Deployment</a></li><li><a href="#%E6%BB%9A%E5%8A%A8%E9%83%A8%E7%BD%B2-rolling-update">滚动部署 Rolling update</a></li><li><a href="#ab-%E6%B5%8B%E8%AF%95%E5%8F%91%E5%B8%83-ab-testing">A/B 测试发布 A/B Testing</a></li><li><a href="#%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E9%87%91%E4%B8%9D%E9%9B%80%E5%8F%91%E5%B8%83">灰度发布／金丝雀发布</a></li></ul></p><h2><span id="蓝绿部署-bluegreen-deployment"> 蓝绿部署 Blue/Green Deployment</span></h2><ol><li><p>定义<br>蓝绿部署是不停老版本，部署新版本然后进行测试，确认OK，将流量切到新版本，然后老版本同时也升级到新版本。<br>简单来说，你需要准备两个相同的环境（基础架构），在蓝色环境运行当前生产环境中的应用，也就是旧版本应用，另一个环境运行新版本应用。</p></li><li><p>特点<br>蓝绿部署无需停机，并且风险较小。</p></li><li><p>部署过程<br>旧版本应用，如图中 App1 version1 、 App2 version1 、 App3 version3 。</p></li></ol><img src="/2018/05/01/deploy-method/Blue-green-deployment-01.png" title="Blue-green-deployment-01"><p>当你想要升级 App2 到 version2 ，在蓝色环境中进行操作，即部署新版本应用，并进行测试。如果测试没问题，就可以把负载均衡器／反向代理／路由指向蓝色环境了。</p><img src="/2018/05/01/deploy-method/Blue-green-deployment-02.png" title="Blue-green-deployment-02"><ol start="3"><li><p>小结<br>从过程不难发现，在部署的过程中，我们的应用始终在线。并且，新版本上线的过程中，并没有修改老版本的任何内容，在部署期间，老版本的状态不受影响。<br>这样风险很小，并且，只要老版本的资源不被删除，理论上，我们可以在任何时间回滚到老版本。</p></li><li><p>蓝绿发布的注意事项<br>当你切换到蓝色环境时，需要妥当处理未完成的业务和新的业务。如果你的数据库后端无法处理，会是一个比较麻烦的问题；</p></li></ol><ul><li>可能会出现需要同时处理<code>微服务架构应用</code>和<code>传统架构应用</code>的情况，如果在蓝绿部署中协调不好这两者，还是有可能会导致服务停止。</li><li>需要提前考虑数据库与应用部署同步迁移 /回滚的问题。</li><li>蓝绿部署需要有基础设施支持。</li><li>在非隔离基础架构（ VM 、 Docker 等）上执行蓝绿部署，蓝色环境和绿色环境有被摧毁的风险。</li></ul><h2><span id="滚动部署-rolling-update"> 滚动部署 Rolling update</span></h2><ol><li><p>定义<br>滚动发布：一般是取出一个或者多个服务器停止服务，执行更新，并重新将其投入使用。周而复始，直到集群中所有的实例都更新成新版本。</p></li><li><p>特点<br>这种部署方式相对于蓝绿部署，更加节约资源——它不需要运行两个集群、两倍的实例数。我们可以部分部署，例如每次只取出集群的20%进行升级。</p></li></ol><img src="/2018/05/01/deploy-method/k8s服务更新过程.png" title="k8s服务更新过程"><p>这种方式也有很多缺点，例如：</p><ol><li>没有一个确定OK的环境。使用蓝绿部署，我们能够清晰地知道老版本是OK的，而使用滚动发布，我们无法确定。</li><li>修改了现有的环境。</li><li>如果需要回滚，很困难。举个例子，在某一次发布中，我们需要更新100个实例，每次更新10个实例，每次部署需要5分钟。当滚动发布到第80个实例时，发现了问1. 需要回滚，这个回滚却是一个痛苦，并且漫长的过程。</li><li>有的时候，我们还可能对系统进行动态伸缩，如果部署期间，系统自动扩容/缩容了，我们还需判断到底哪个节点使用的是哪个代码。尽管有一些自动化的运维工具1. 是依然令人心惊胆战。</li><li>因为是逐步更新，那么我们在上线代码的时候，就会短暂出现新老版本不一致的情况，如果对上线要求较高的场景，那么就需要考虑如何做好兼容的问题。</li></ol><h2><span id="ab-测试发布-ab-testing"> A/B 测试发布 A/B Testing</span></h2><p>A/B 测试跟蓝绿部署完全是两码事。</p><p>A/B 测试是用来测试应用功能表现的方法，例如可用性、受欢迎程度、可见性等等。 A/B 测试通常用在应用的前端上，不过当然需要后端来支持。</p><img src="/2018/05/01/deploy-method/AB-test.png" title="AB-test"><p>A/B 测试与蓝绿部署的区别在于， A/B 测试目的在于通过科学的实验设计、采样样本代表性、流量分割与小流量测试等方式来获得具有代表性的实验结论，并确信该结论在推广到全部流量可信；蓝绿部署的目的是安全稳定地发布新版本应用，并在必要时回滚。</p><p>A/B 测试和蓝绿部署可以同时使用。</p><h2><span id="灰度发布金丝雀发布"> 灰度发布／金丝雀发布</span></h2><p>灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式。<br>AB test就是一种灰度发布方式，让一部分用户继续用A，一部分用户开始用B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来。<br>灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。</p><img src="/2018/05/01/deploy-method/Grayscale-publishing.png" title="Grayscale-publishing"><p>灰度发布／金丝雀发布由以下几个步骤组成：</p><ul><li>准备好部署各个阶段的工件，包括：构建工件，测试脚本，配置文件和部署清单文件。</li><li>从负载均衡列表中移除掉“金丝雀”服务器。</li><li>升级“金丝雀”应用（排掉原有流量并进行部署）。</li><li>对应用进行自动化测试。</li><li>将“金丝雀”服务器重新添加到负载均衡列表中（连通性和健康检查）。</li><li>如果“金丝雀”在线使用测试成功，升级剩余的其他服务器。（否则就回滚）</li></ul><p>除此之外灰度发布还可以设置路由权重，动态调整不同的权重来进行新老版本的验证。</p><p>参考链接：</p><ol><li><a href="https://www.jianshu.com/p/022685baba7d" target="_blank" rel="noopener">https://www.jianshu.com/p/022685baba7d</a></li><li><a href="https://www.v2ex.com/t/344341" target="_blank" rel="noopener">https://www.v2ex.com/t/344341</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在项目迭代的过程中，不可避免需要&lt;code&gt;上线&lt;/code&gt;。上线对应着部署，或者重新部署；部署对应着修改；修改则意味着风险。&lt;br&gt;
目前有很多用于部署的技术，有的简单，有的复杂；有的需要停机，有的不需要停机即可完成部署。本文的目的就是将目前常用的布署方案做一个总结。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="yunke.science/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="yunke.science/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Redis AOF文件 重写操作导致的磁盘和内存不足的问题</title>
    <link href="yunke.science/2018/04/30/redis-aof/"/>
    <id>yunke.science/2018/04/30/redis-aof/</id>
    <published>2018-04-30T04:02:47.000Z</published>
    <updated>2018-04-30T04:09:10.227Z</updated>
    
    <content type="html"><![CDATA[<p>Redis AOF文件 重写操作导致的磁盘和内存不足的问题</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#%E9%97%AE%E9%A2%98%E8%AF%B4%E6%98%8E">问题说明</a></li><li><a href="#%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5">问题排查</a></li><li><a href="#%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86">问题处理</a></li><li><a href="#%E9%99%84bgrewriteaof%E5%91%BD%E4%BB%A4">附：BGREWRITEAOF命令</a></li><li><a href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5">参考链接</a></li></ul></p><h2><span id="问题说明"> 问题说明</span></h2><p>收到 redis 服务器磁盘报警，内存不足报警。</p><h2><span id="问题排查"> 问题排查</span></h2><p>登录服务器，查看磁盘使用率已经接近 100% 。<br>redis data目录存在大量的文件 temp-rewriteaof.aof ，把磁盘撑爆了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-rw-rw-r-- 1 myuser myuser 18044223152 4月   8 12:01 appendonly.aof</span><br><span class="line">-rw-rw-r-- 1 myuser myuser  3603981186 4月   8 12:01 temp-rewriteaof-8116.aof</span><br><span class="line">-rw-rw-r-- 1 myuser myuser  4083774382 4月   8 11:46 temp-rewriteaof-8117.aof</span><br><span class="line">-rw-rw-r-- 1 myuser myuser  4326578230 4月   8 11:21 temp-rewriteaof-8118.aof</span><br><span class="line">-rw-rw-r-- 1 myuser myuser  3603981186 4月   8 12:01 temp-rewriteaof-8119.aof</span><br><span class="line">-rw-rw-r-- 1 myuser myuser  4083774382 4月   8 11:46 temp-rewriteaof-8120.aof</span><br><span class="line">-rw-rw-r-- 1 myuser myuser  4326578230 4月   8 11:21 temp-rewriteaof-8121.aof</span><br><span class="line">-rw-rw-r-- 1 myuser myuser  3603981186 4月   8 12:01 temp-rewriteaof-8122.aof</span><br><span class="line">-rw-rw-r-- 1 myuser myuser  4083774382 4月   8 11:46 temp-rewriteaof-8123.aof</span><br><span class="line">-rw-rw-r-- 1 myuser myuser  4326578230 4月   8 11:21 temp-rewriteaof-8124.aof</span><br></pre></td></tr></table></figure><p>网上查询资料，temp-rewriteaof 是redis在进行 BGREWRITEAOF 时产生的临时文件。</p><p>BGREWRITEAOF命令 执行一个 AOF文件重写操作。重写会创建一个当前 AOF 文件的体积优化版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">AOF 文件的体积优化的原理：</span><br><span class="line"></span><br><span class="line">随着写操作的执行，AOF变得越来越大。 </span><br><span class="line">例如，如果您将计数器递增100次，则最终数据集中将包含一个包含最终值的单个键，但AOF中将包含100个条目，这些条目中其余99个不需要的记录将被优化掉。</span><br><span class="line">如果在某一段时间添加了大量的key，在某一时间又删除了这些key，这些添加和删除的操作将被优化掉。</span><br><span class="line"></span><br><span class="line">因此，Redis支持一个有趣的功能：它能够在不中断向客户端提供服务的情况下在后台重建AOF。 </span><br><span class="line">每当您发出BGREWRITEAOF Redis时，都会编写在内存中重建当前数据集所需的最短命令序列。 </span><br><span class="line">如果您在Redis 2.2中使用AOF，则需要不时运行BGREWRITEAOF。 Redis 2.4能够自动触发日志重写（有关更多信息，请参阅2.4示例配置文件）。</span><br></pre></td></tr></table></figure><p>查看日志文件 /var/log/messages ，非常多的OOM日志。如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Apr 19 09:31:17 kernel: Out of memory: Kill process 27359 (redis-server) score 912 or sacrifice child</span><br><span class="line">Apr 19 09:31:17 kernel: Killed process 27359 (redis-server) total-vm:11344292kB, anon-rss:3321344kB, file-rss:0kB, shmem-rss:0kB</span><br><span class="line">Apr 19 09:31:30 kernel: redis-server invoked oom-killer: gfp_mask=0x200da, order=0, oom_score_adj=0</span><br><span class="line">Apr 19 09:31:30 kernel: redis-server cpuset=da6138be06a7a917780f1890812508644db4f530d5e16a507109ad542192d05e mems_allowed=0</span><br></pre></td></tr></table></figure><p>查看监控，内存使用情况</p><img src="/2018/04/30/redis-aof/memory_usage.png" title="系统剩余内存监控"><img src="/2018/04/30/redis-aof/redis_used_memory_rss.png" title="redis内存使用监控"><p>temp-rewriteaof-8116.aof 中8116 为rewrite子进程的pid，多个temp文件，时间相近，PID 相近。</p><p>判断 redis-server 在做AOF文件重写操作BGREWRITEAOF时，由于内存不足，被系统kill掉 rewrite 子进程，而 redis-server又重新启动新的rewrite子进程。</p><p>为什么aof重写会导致内存爆涨？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AOF 是redis的一种持久化方式，用来记录所有的写操作，但是随着时间增加，aof文件会越来越大，所以需要进行重写，将内存中的数据重新以命令的方式写入aof文件。</span><br><span class="line">在重写的过程中，由于redis还会有新的写入，为了避免数据丢失，会开辟一块内存用于存放重写期间产生的写入操作，等到重写完毕后会将这块内存中的操作再追加到aof文件中。</span><br></pre></td></tr></table></figure><p>从原理中可以了解到，如果在重写过程中redis的写入很频繁或写入量很大，就会导致占用大量额外的内存来缓存写操作，导致内存爆涨。</p><h2><span id="问题处理"> 问题处理</span></h2><ol><li>对 redis keys总数进行监控 <code>DBSIZE</code> , 对redis list长度进行监控 <code>llen listName</code> , 对redis 内存监控<code>info</code>,数量或者增量超过一定值触发报警。</li><li>调整 BGREWRITEAOF 的频率，默认当达到100%增长时触发BGREWRITEAOF ，根据实际业务增加或者降低触发频率。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379[6]&gt; config get auto-aof-rewrite-percentage</span><br><span class="line">1) &quot;auto-aof-rewrite-percentage&quot;</span><br><span class="line">2) &quot;100&quot;</span><br><span class="line">127.0.0.1:6379[6]&gt; config set auto-aof-rewrite-percentage 800</span><br><span class="line">OK</span><br><span class="line">127.</span><br></pre></td></tr></table></figure><ol start="3"><li>根据业务需要，判断是否需要持久化。取消AOF <code>config set appendonly no</code>， 使用 rdb。</li><li>使用 kafka 代替 redis 。</li></ol><h2><span id="附bgrewriteaof命令"> 附：BGREWRITEAOF命令</span></h2><p><strong>BGREWRITEAOF</strong></p><p>执行一个 AOF文件 重写操作。重写会创建一个当前 AOF 文件的体积优化版本。</p><p>即使 BGREWRITEAOF 执行失败，也不会有任何数据丢失，因为旧的 AOF 文件在 BGREWRITEAOF 成功之前不会被修改。</p><p>重写操作只会在没有其他持久化工作在后台执行时被触发，也就是说：</p><p>如果 Redis 的子进程正在执行快照的保存工作，那么 AOF 重写的操作会被预定(scheduled)，等到保存工作完成之后再执行 AOF 重写。在这种情况下， BGREWRITEAOF 的返回值仍然是 OK ，但还会加上一条额外的信息，说明 BGREWRITEAOF 要等到保存操作完成之后才能执行。在 Redis 2.6 或以上的版本，可以使用 INFO 命令查看 BGREWRITEAOF 是否被预定。</p><p>如果已经有别的 AOF 文件重写在执行，那么 BGREWRITEAOF 返回一个错误，并且这个新的 BGREWRITEAOF 请求也不会被预定到下次执行。<br>从 Redis 2.4 开始， AOF 重写由 Redis 自行触发， BGREWRITEAOF 仅仅用于手动触发重写操作。</p><h2><span id="参考链接"> 参考链接</span></h2><ol><li><a href="http://blog.51cto.com/xiao987334176/1881015" target="_blank" rel="noopener">http://blog.51cto.com/xiao987334176/1881015</a></li><li><a href="http://sparkgis.com/uncategorized/2018/04/%E4%BC%98%E5%8C%96-redis-aof%E9%87%8D%E5%86%99%E5%AF%BC%E8%87%B4%E7%9A%84%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98-%E5%8E%9F-%E8%8D%90-%E4%BC%98%E5%8C%96-redis-aof%E9%87%8D%E5%86%99%E5%AF%BC%E8%87%B4-3/" target="_blank" rel="noopener">http://sparkgis.com/uncategorized/2018/04/优化-redis-aof重写导致的内存问题-原-荐-优化-redis-aof重写导致-3/</a></li><li><a href="https://redis.io/topics/persistence" target="_blank" rel="noopener">https://redis.io/topics/persistence</a></li><li><a href="http://redisdoc.com/server/bgrewriteaof.html" target="_blank" rel="noopener">http://redisdoc.com/server/bgrewriteaof.html</a></li><li><a href="http://redisdoc.com/server/info.html" target="_blank" rel="noopener">http://redisdoc.com/server/info.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Redis AOF文件 重写操作导致的磁盘和内存不足的问题&lt;/p&gt;
    
    </summary>
    
      <category term="redis" scheme="yunke.science/categories/redis/"/>
    
    
      <category term="redis" scheme="yunke.science/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>Nginx 指令的执行顺序</title>
    <link href="yunke.science/2018/04/28/nginx-process/"/>
    <id>yunke.science/2018/04/28/nginx-process/</id>
    <published>2018-04-28T01:39:27.000Z</published>
    <updated>2018-04-28T01:40:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>nginx的处理过程总共分为11个阶段，分别是<code>post-read</code>、<code>server-rewrite</code>、<code>find-config</code>、<code>rewrite</code>、<code>post-rewrite</code>、<code>preaccess</code>、<code>access</code>、<code>post-access</code>、<code>try-files</code>、<code>content</code>, <code>log</code>。</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#post-read">post-read</a></li><li><a href="#server-rewrite">server-rewrite</a></li><li><a href="#find-config">find-config</a></li><li><a href="#rewrite">rewrite</a></li><li><a href="#post-rewrite">post-rewrite</a></li><li><a href="#preaccess-%E9%98%B6%E6%AE%B5">preaccess 阶段</a></li><li><a href="#access-%E9%98%B6%E6%AE%B5">access 阶段</a></li><li><a href="#post-access-%E9%98%B6%E6%AE%B5">post-access 阶段</a></li><li><a href="#try-files-%E9%98%B6%E6%AE%B5">try-files 阶段</a></li><li><a href="#content-%E9%98%B6%E6%AE%B5">content 阶段</a></li><li><a href="#log-%E9%98%B6%E6%AE%B5">log 阶段</a></li></ul></p><h2><span id="post-read"> post-read</span></h2><p>在nginx读取解析请求头后执行: <code>set_real_ip_from</code> ,<code>read_ip_head</code></p><p>该阶段是在Nginx读取并解析完请求头（request headers）之后就立即开始执行的，标准模块的ngx_realip就是在post-read阶段注册了处理程序。它的功能是迫使 Nginx 认为当前请求的来源地址是指定的某一个请求头的值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">        listen 8080;</span><br><span class="line"></span><br><span class="line">        set_real_ip_from 127.0.0.1;</span><br><span class="line">        real_ip_header   X-My-IP;</span><br><span class="line"></span><br><span class="line">        location /test &#123;</span><br><span class="line">            set $addr $remote_addr;</span><br><span class="line">            echo &quot;from: $addr&quot;;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这里的配置是让 Nginx 把那些来自 127.0.0.1 的所有请求的来源地址，都改写为请求头 X-My-IP 所指定的值。同时该例使用了标准内建变量 $remote_addr 来输出当前请求的来源地址，以确认是否被成功改写。</p><h2><span id="server-rewrite"> server-rewrite</span></h2><p>当 ngx_rewrite 模块的配置指令直接书写在 server 配置块中时，基本上都是运行在 server-rewrite 阶段</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">        listen 8080;</span><br><span class="line"></span><br><span class="line">        location /test &#123;</span><br><span class="line">            set $b &quot;$a, world&quot;;</span><br><span class="line">            echo $b;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        set $a hello;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>配置语句 set $a hello 直接写在了 server 配置块中，因此它就运行在 server-rewrite 阶段</p><h2><span id="find-config"> find-config</span></h2><p>紧接在 server-rewrite 阶段后边的是 find-config 阶段。这个阶段并不支持 Nginx 模块注册处理程序，而是由 Nginx 核心来完成当前请求与 location 配置块之间的配对工作。换句话说，在此阶段之前，请求并没有与任何 location 配置块相关联。因此，对于运行在 find-config 阶段之前的 post-read 和 server-rewrite 阶段来说，只有 server 配置块以及更外层作用域中的配置指令才会起作用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">location /hello &#123;</span><br><span class="line">    echo &quot;hello world&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果启用了 Nginx 的“调试日志”，那么当请求 /hello 接口时，便可以在 error.log 文件中过滤出下面这一行信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ grep &apos;using config&apos; logs/error.log</span><br><span class="line">[debug] 84579#0: *1 using configuration &quot;/hello&quot;</span><br></pre></td></tr></table></figure><h2><span id="rewrite"> rewrite</span></h2><p>运行在 find-config 阶段之后的便是我们的老朋友 rewrite 阶段。由于 Nginx 已经在 find-config 阶段完成了当前请求与 location 的配对，所以从 rewrite 阶段开始，location 配置块中的指令便可以产生作用。前面已经介绍过，当 ngx_rewrite 模块的指令用于 location 块中时，便是运行在这个 rewrite 阶段。</p><h2><span id="post-rewrite"> post-rewrite</span></h2><p>rewrite 阶段再往后便是所谓的 post-rewrite 阶段。这个阶段也像 find-config 阶段那样不接受 Nginx 模块注册处理程序，而是由 Nginx 核心完成 rewrite 阶段所要求的“内部跳转”操作（如果 rewrite 阶段有此要求的话）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">        listen 8080;</span><br><span class="line"></span><br><span class="line">        location /foo &#123;</span><br><span class="line">            set $a hello;</span><br><span class="line">            rewrite ^ /bar;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location /bar &#123;</span><br><span class="line">            echo &quot;a = [$a]&quot;;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这里在 location /foo 中通过 rewrite 指令把当前请求的 URI 无条件地改写为 /bar，同时发起一个“内部跳转”，最终跳进了 location /bar 中。这里比较有趣的地方是内部跳转的工作原理。内部跳转 本质上其实就是把当前的请求处理阶段强行倒退到 find-config 阶段，以便重新进行请求 URI 与 location 配置块的配对。比如上例中，运行在 rewrite 阶段的 rewrite 指令就让当前请求的处理阶段倒退回了 find-config 阶段。由于此时当前请求的 URI 已经被 rewrite 指令修改为了 /bar，所以这一次换成了 location /bar 与当前请求相关联，然后再接着从 rewrite 阶段往下执行。</p><p>不过这里更有趣的地方是，倒退回 find-config 阶段的动作并不是发生在 rewrite 阶段，而是发生在后面的 post-rewrite 阶段。上例中的 rewrite 指令只是简单地指示 Nginx 有必要在 post-rewrite 阶段发起“内部跳转”。这个设计对于 Nginx 初学者来说，或许显得有些古怪：“为什么不直接在 rewrite 指令执行时立即进行跳转呢？”答案其实很简单，那就是为了在最初匹配的 location 块中支持多次反复地改写 URI，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">location /foo &#123;</span><br><span class="line">        rewrite ^ /bar;</span><br><span class="line">        rewrite ^ /baz;</span><br><span class="line"></span><br><span class="line">        echo foo;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /bar &#123;</span><br><span class="line">        echo bar;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /baz &#123;</span><br><span class="line">        echo baz;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这里在 location /foo 中连续把当前请求的 URI 改写了两遍：第一遍先无条件地改写为 /bar，第二遍再无条件地改写为 /baz. 而这两条 rewrite 语句只会最终导致 post-rewrite 阶段发生一次“内部跳转”操作，从而不至于在第一次改写 URI 时就直接跳离了当前的 location 而导致后面的 rewrite 语句没有机会执行。请求 /foo 接口的结果证实了这一点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl localhost:8080/foo</span><br><span class="line">baz</span><br></pre></td></tr></table></figure><p>从输出结果可以看到，上例确实成功地从 /foo 一步跳到了 /baz 中。如果启用 Nginx “调试日志”的话，还可以从 find-config 阶段生成的 location 块的匹配信息中进一步证实这一点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ grep &apos;using config&apos; logs/error.log</span><br><span class="line">[debug] 89449#0: *1 using configuration &quot;/foo&quot;</span><br><span class="line">[debug] 89449#0: *1 using configuration &quot;/baz&quot;</span><br></pre></td></tr></table></figure><p>我们看到，对于该次请求，Nginx 一共只匹配过 /foo 和 /baz 这两个 location，从而只发生过一次“内部跳转”。</p><p>当然，如果在 server 配置块中直接使用 rewrite 配置指令对请求 URI 进行改写，则不会涉及“内部跳转”，因为此时 URI 改写发生在 server-rewrite 阶段，早于执行 location 配对的 find-config 阶段。比如下面这个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">        listen 8080;</span><br><span class="line"></span><br><span class="line">        rewrite ^/foo /bar;</span><br><span class="line"></span><br><span class="line">        location /foo &#123;</span><br><span class="line">            echo foo;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location /bar &#123;</span><br><span class="line">            echo bar;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这里，我们在 server-rewrite 阶段就把那些以 /foo 起始的 URI 改写为 /bar，而此时请求并没有和任何 location 相关联，所以 Nginx 正常往下运行 find-config 阶段，完成最终的 location 匹配。如果我们请求上例中的 /foo 接口，那么 location /foo 根本就没有机会匹配，因为在第一次（也是唯一的一次）运行 find-config 阶段时，当前请求的 URI 已经被改写为 /bar，从而只会匹配 location /bar.</p><h2><span id="preaccess-阶段"> preaccess 阶段</span></h2><p>标准模块 ngx_limit_req 和 ngx_limit_zone 就运行在此阶段，前者可以控制请求的访问频度，而后者可以限制访问的并发度。</p><h2><span id="access-阶段"> access 阶段</span></h2><p>标准模块 ngx_access、第三方模块 ngx_auth_request 以及第三方模块 ngx_lua 的 access_by_lua 指令就运行在这个阶段。</p><h2><span id="post-access-阶段"> post-access 阶段</span></h2><p>post-access 阶段主要用于配合 access 阶段实现标准 ngx_http_core 模块提供的配置指令 satisfy 的功能。<br>对于多个 Nginx 模块注册在 access 阶段的处理程序， satisfy 配置指令可以用于控制它们彼此之间的协作方式。比如模块 A 和 B 都在 access 阶段注册了与访问控制相关的处理程序，那就有两种协作方式，一是模块 A 和模块 B 都得通过验证才算通过，二是模块 A 和模块 B 只要其中任一个通过验证就算通过。第一种协作方式称为 all 方式（或者说“与关系”），第二种方式则被称为 any 方式（或者说“或关系”）。默认情况下，Nginx 使用的是 all 方式。下面是一个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">location /test &#123;</span><br><span class="line">        satisfy all;</span><br><span class="line"></span><br><span class="line">        deny all;</span><br><span class="line">        access_by_lua &apos;ngx.exit(ngx.OK)&apos;;</span><br><span class="line"></span><br><span class="line">        echo something important;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这里，我们在 /test 接口中同时配置了 ngx_access 模块和 ngx_lua 模块，这样 access 阶段就由这两个模块一起来做检验工作。其中，语句 deny all 会让 ngx_access 模块的处理程序总是拒绝当前请求，而语句 access_by_lua ‘ngx.exit(ngx.OK)’ 则总是允许访问。当我们通过 satisfy 指令配置了 all 方式时，就需要 access 阶段的所有模块都通过验证，但不幸的是，这里 ngx_access 模块总是会拒绝访问，所以整个请求就会被拒.</p><h2><span id="try-files-阶段"> try-files 阶段</span></h2><p>这个阶段专门用于实现标准配置指令 try_files 的功能，并不支持 Nginx 模块注册处理程序。<br>try_files 指令接受两个以上任意数量的参数，每个参数都指定了一个 URI. 这里假设配置了 N 个参数，则 Nginx 会在 try-files 阶段，依次把前 N-1 个参数映射为文件系统上的对象（文件或者目录），然后检查这些对象是否存在。一旦 Nginx 发现某个文件系统对象存在，就会在 try-files 阶段把当前请求的 URI 改写为该对象所对应的参数 URI（但不会包含末尾的斜杠字符，也不会发生 “内部跳转”）。如果前 N-1 个参数所对应的文件系统对象都不存在，try-files 阶段就会立即发起“内部跳转”到最后一个参数（即第 N 个参数）所指定的 URI.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">root /var/www/;</span><br><span class="line"></span><br><span class="line">    location /test &#123;</span><br><span class="line">        try_files /foo /bar/ /baz;</span><br><span class="line">        echo &quot;uri: $uri&quot;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /foo &#123;</span><br><span class="line">        echo foo;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /bar/ &#123;</span><br><span class="line">        echo bar;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /baz &#123;</span><br><span class="line">        echo baz;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这里通过 root 指令把“文档根目录”配置为 /var/www/，如果你系统中的 /var/www/ 路径下存放有重要数据，则可以把它替换为其他任意路径，但此路径对运行 Nginx worker 进程的系统帐号至少有可读权限。我们在 location /test 中使用了 try_files 配置指令，并提供了三个参数，/foo、/bar/ 和 /baz. 根据前面对 try_files 指令的介绍，我们可以知道，它会在 try-files 阶段依次检查前两个参数 /foo 和 /bar/ 所对应的文件系统对象是否存在。</p><p>不妨先来做一组实验。假设现在 /var/www/ 路径下是空的，则第一个参数 /foo 映射成的文件 /var/www/foo 是不存在的；同样，对于第二个参数 /bar/ 所映射成的目录 /var/www/bar/ 也是不存在的。于是此时 Nginx 会在 try-files 阶段发起到最后一个参数所指定的 URI（即 /baz）的“内部跳转”。实际的请求结果证实了这一点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl localhost:8080/test</span><br><span class="line">baz</span><br></pre></td></tr></table></figure><p>显然，该请求最终和 location /baz 绑定在一起，执行了输出 baz 字符串的工作。上例中定义的 location /foo 和 location /bar/ 完全不会参与这里的运行过程，因为对于 try_files 的前 N-1 个参数，Nginx 只会检查文件系统，而不会去执行 URI 与 location 之间的匹配。</p><p>try-files 阶段发生的事情：Nginx 依次检查了文件 /var/www/foo 和目录 /var/www/bar，末了又处理了最后一个参数 /baz. 这里最后一条“调试信息”容易产生误解，会让人误以为 Nginx 也把最后一个参数 /baz 给映射成了文件系统对象进行检查，事实并非如此。当 try_files 指令处理到它的最后一个参数时，总是直接执行“内部跳转”，而不论其对应的文件系统对象是否存在。</p><h2><span id="content-阶段"> content 阶段</span></h2><p>content阶段主要是处理内容输出的阶段。这里要注意的是绝大多数 Nginx 模块在向 content 阶段注册配置指令时，本质上是在当前的 location 配置块中注册所谓的“内容处理程序”（content handler）。每一个 location 只能有一个“内容处理程序”，因此，当在 location 中同时使用多个模块的 content 阶段指令时，只有其中一个模块能成功注册“内容处理程序”。例如 echo 和 content_by_lua 如果同时注册，最终只会有一个生效，但具体是哪一个生效是不稳定的。</p><h2><span id="log-阶段"> log 阶段</span></h2><p>日志的记录阶段</p><p>参考链接：</p><ol><li><a href="https://www.jianshu.com/p/0cc0f4c9a005" target="_blank" rel="noopener">https://www.jianshu.com/p/0cc0f4c9a005</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;nginx的处理过程总共分为11个阶段，分别是&lt;code&gt;post-read&lt;/code&gt;、&lt;code&gt;server-rewrite&lt;/code&gt;、&lt;code&gt;find-config&lt;/code&gt;、&lt;code&gt;rewrite&lt;/code&gt;、&lt;code&gt;post-rewrite&lt;/code&gt;、&lt;code&gt;preaccess&lt;/code&gt;、&lt;code&gt;access&lt;/code&gt;、&lt;code&gt;post-access&lt;/code&gt;、&lt;code&gt;try-files&lt;/code&gt;、&lt;code&gt;content&lt;/code&gt;, &lt;code&gt;log&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="nginx" scheme="yunke.science/categories/nginx/"/>
    
    
      <category term="nginx" scheme="yunke.science/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title>创建NGINX重写规则</title>
    <link href="yunke.science/2018/04/26/nginx-rewrite/"/>
    <id>yunke.science/2018/04/26/nginx-rewrite/</id>
    <published>2018-04-26T08:50:50.000Z</published>
    <updated>2018-04-26T09:04:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>在这篇文章中，我们讨论如何创建NGINX重写规则return, rewrite,和try_files指令。</p><a id="more"></a><p><ul class="markdownIt-TOC"><li><a href="#%E5%AF%B9%E6%AF%94return-rewrite%E5%92%8Ctry_files%E6%8C%87%E4%BB%A4">对比return, rewrite,和try_files指令</a><ul><li><a href="#return%E6%8C%87%E4%BB%A4">return指令</a></li><li><a href="#rewrite%E6%8C%87%E4%BB%A4">rewrite指令</a><ul><li><a href="#rewrtie%E5%9B%9B%E7%A7%8Dflag">rewrtie四种flag</a></li></ul></li><li><a href="#try_files-%E6%8C%87%E4%BB%A4">try_files 指令</a></li></ul></li><li><a href="#%E7%A4%BA%E4%BE%8B-%E6%A0%87%E5%87%86%E5%8C%96%E5%9F%9F%E5%90%8D">示例 – 标准化域名</a><ul><li><a href="#%E4%BB%8E%E6%97%A7%E7%9A%84%E5%90%8D%E7%A7%B0%E9%87%8D%E5%AE%9A%E5%90%91%E5%88%B0%E6%96%B0%E7%9A%84%E5%90%8D%E7%A7%B0">从旧的名称重定向到新的名称</a></li><li><a href="#%E6%B7%BB%E5%8A%A0%E5%92%8C%E5%88%A0%E9%99%A4www%E5%89%8D%E7%BC%80">添加和删除www前缀</a></li><li><a href="#%E9%87%8D%E5%AE%9A%E5%90%91%E6%89%80%E6%9C%89%E7%9A%84%E6%B5%81%E9%87%8F%E5%88%B0%E6%AD%A3%E5%B8%B8%E7%9A%84%E5%9F%9F%E5%90%8D">重定向所有的流量到正常的域名</a></li></ul></li><li><a href="#%E7%A4%BA%E4%BE%8B%E5%BC%BA%E5%88%B6%E6%89%80%E6%9C%89%E8%AF%B7%E6%B1%82%E4%BD%BF%E7%94%A8-ssltls">示例：强制所有请求使用 SSL/TLS</a></li><li><a href="#%E7%A4%BA%E4%BE%8B-%E4%B8%BAwordpress%E7%BD%91%E7%AB%99%E5%90%AF%E7%94%A8%E6%BC%82%E4%BA%AE%E7%9A%84%E5%9B%BA%E5%AE%9A%E9%93%BE%E6%8E%A5">示例 – 为WordPress网站启用漂亮的固定链接</a></li><li><a href="#%E7%A4%BA%E4%BE%8B-%E4%B8%A2%E5%BC%83%E4%B8%8D%E5%8F%97%E6%94%AF%E6%8C%81%E7%9A%84%E6%96%87%E4%BB%B6%E6%89%A9%E5%B1%95%E5%90%8D%E7%9A%84%E8%AF%B7%E6%B1%82">示例 – 丢弃不受支持的文件扩展名的请求</a></li><li><a href="#%E7%A4%BA%E4%BE%8B-%E9%85%8D%E7%BD%AE%E8%87%AA%E5%AE%9A%E4%B9%89%E9%87%8D%E6%96%B0%E8%B7%AF%E7%94%B1">示例 – 配置自定义重新路由</a></li></ul></p><p>重写规则会更改客户请求中的部分或全部网址，通常用于以下两个目的之一：</p><ul><li><strong>通知客户端他们请求的资源现在位于不同的位置。</strong><br>示例用例是您的网站的域名发生更改时，您希望客户端使用规范网址格式（带或不带www前缀），以及当您想要捕获和更正您的域名的常见拼写错误时。 返回和重写指令适合于这些目的。</li><li><strong>控制NGINX和NGINX Plus中的处理流程。</strong><br>例如，在需要动态生成内容时，将请求转发到应用程序服务器。 try_files指令通常用于此目的。</li></ul><p><em>注意：学习如何转换Apache重写规则到Nginx重写规则，查看另一片文章<br>Converting Apache Rewrite Rules to NGINX Rewrite Rules.<br>我们假设你已经熟悉HTTP响应码及正则表达式相关知识（nginx和nginx plus使用Perl语法）。</em></p><h2><span id="对比return-rewrite和try_files指令"> 对比return, rewrite,和try_files指令</span></h2><p>通用NGINX重写的两个指令是return 和 rewrite，而try_files指令是将请求定向到应用程序服务器的一种方便的方法。 让我们回顾一下指令的作用以及它们之间的区别。</p><h3><span id="return指令"> return指令</span></h3><p>return指令是两个通用指令中的更简单的，因此我们建议在可能的情况下使用它而不是rewrite（更多的是为什么和什么时候）。 将返回值放在指定要重写的URL的服务器或位置上下文中，并定义客户端在将来对资源的请求中使用的已更正（重写）的URL。</p><p>这有一个简单的示例，重定向客户端到一个新的域名：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    listen 443 ssl;</span><br><span class="line">    server_name www.old-name.com;</span><br><span class="line">    return 301 $scheme://www.new-name.com$request_uri;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>listen指令意味着server 块应用于HTTP和HTTPS流量。</li><li>server_name指令匹配具有域名www.old-name.com的请求URL。 - return指令告诉NGINX停止处理请求，并立即向客户端发送代码301（Moved Permanently）和指定的重写URL。 - 重写的URL使用两个NGINX变量来捕获和复制原始请求URL中的值</li><li>$ scheme是协议（http或https）</li><li>$ request_uri是包含参数的完整URI</li></ul><p>对于3xx系列中的代码，url参数定义新的（重写的）URL。<br><code>return (301 | 302 | 303 | 307) url;</code><br>对于其他的代码，你可以选择定义一个用于显示在响应正文的文本字符串（HTTP代码的标准文本，例如404的Not Found，仍包含在标题中）。这个文本字符串可以包含nginx变量。<br><code>return (1xx | 2xx | 4xx | 5xx) [&quot;text&quot;];</code><br>如下，对于非法的认证，当拒绝请求时返回的内容：<br><code>return 401 &quot;Access denied because token is expired or invalid&quot;;</code><br>还有一些可以使用的语法快捷方式，例如省略302的代码; 请参阅return指令的参考文档。<br>（在某些情况下，您可能希望返回一个比在文本字符串中可以实现的更复杂或更细微的响应。使用error_page指令，您可以为每个HTTP代码返回完整的自定义HTML页面，以及更改 响应代码或执行重定向。）<br>所以 <strong>return指令使用很简单，适合当重定向满足两个条件：重写的URL适合于每个匹配server 或location 的请求，您可以使用标准的NGINX变量构建重写的URL。</strong></p><h3><span id="rewrite指令"> rewrite指令</span></h3><p>但是，如果你需要测试更复杂的URL之间的区别，捕获原始URL中没有相应的NGINX变量的元素，或者更改或添加路径中的元素，怎么办？ 在这种情况下，可以使用rewrite伪指令。</p><p>类似return指令，在server或这两个指令与类似的更加不同，并且rewrite指令可能更复杂地使用正确。<br>它的语法很简单：在server 或location块中添加rewrite指令用于定义重定向的URL。另外，这两个指令与类似的更加不同，并且rewrite指令要使用正确可能更复杂。它的语法很简单：</p><p><code>rewrite regex URL [flag];</code></p><ul><li>第一个参数regex意味着NGINX Plus和NGINX只有在匹配指定的正则表达式（除了匹配server 或location - 指令之外）才重写URL。 附加测试意味着NGINX必须做更多的处理。</li><li>第二个不同是rewrite指令只能返回301或302代码。要返回其他的代码，你需要在rewrite指令之后包含一个return指令（看- 下面的例子）。</li><li>最后，rewrite指令不一定会阻止NGINX对返回请求的处理，并且不一定向客户端发送重定向。</li></ul><p>除非你明确指出（使用标志或URL的语法）你希望NGINX停止处理或发送重定向，它运行通过整个配置寻找在Rewrite模块中定义的指令（break，if，return，rewrite ，set），并按顺序处理它们。 如果重写的URL与重写模块中的后续指令匹配，NGINX对重写的URL执行指示的操作（通常再次重写）。<br>这就是让人疑惑的地方，你需要仔细计划如何编写指令以获得所需的结果。 例如，如果原始location和NGINX重写规则在其中匹配重写的URL，NGINX可以进入一个循环，应用重写覆盖到内置的限制10次。 要了解所有详细信息，请参阅Rewrite模块的文档。 如前所述，我们建议尽可能使用return指令。<br>下面是一个使用rewrite指令的nginx重写规则。他匹配以 /download起始的字符串，后面跟/media/  或者 /audio/  目录及后面其他的内容。他使用/mp3/ 和增加文件扩展名.mp3 或者.ra 替换以上的元素. $1 和 $2 变量捕获不变的路径元素。例如，/download/cdn-west/media/file1 变为 /download/cdn-west/mp3/file1.mp3.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    ...</span><br><span class="line">    rewrite ^(/download/.*)/media/(.*)\..*$ $1/mp3/$2.mp3 last;</span><br><span class="line">    rewrite ^(/download/.*)/audio/(.*)\..*$ $1/mp3/$2.ra  last;</span><br><span class="line">    return  403;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们上面提到你可以添加标志到rewrite指令来控制处理的流程。 示例中的最后一个标志是其中之一：它指示NGINX跳过当前服务器或位置块中的任何后续Rewrite-module伪指令，并开始搜索与重写的URL匹配的新位置。<br>在这个例子中的最终return指令意味着如果URL不匹配rewrite指令，代码403返回到客户端。</p><h4><span id="rewrtie四种flag"> rewrtie四种flag</span></h4><p>对于rewrtie有四种不同的flag，分别是redirect、permanent、break和last。<br>其中前两种是跳转型的flag，后两种是代理型。跳转型是指有客户端浏览器重新对新地址进行请求，代理型是在WEB服务器内部实现跳转的。</p><ul><li>redirect：302跳转到rewrtie后面的地址。</li><li>permanent：301永久调整到rewrtie后面的地址，即当前地址已经永久迁移到新地址，一般是为了对搜索引擎友好。</li><li>last：<mark>将rewrite后的地址重新在server标签执行。</mark></li><li>break：<mark>将rewrite后地址重新在当前的location标签执行。</mark></li></ul><h3><span id="try_files-指令"> try_files 指令</span></h3><p>类似return和 rewrite指令，try_files  指令位于server和location块中。它需要一个或多个文件和目录以及最终URI的列表作为参数：</p><p><code>try_files file … uri;</code></p><p>NGINX按顺序检查文件和目录的存在（从 root 和alias指令的设置构造每个文件的完整路径），并提供它找到的第一个。 要指定目录，请在元素名称的末尾添加一个斜杠。 如果没有文件或目录存在，NGINX将执行内部重定向到由最后一个元素（uri）定义的URI。</p><p>为了使try_files指令正常工作，您还需要定义一个捕获内部重定向的location 块，如下面的示例所示。 最终元素可以是命名位置，由初始at符号（@）指示。</p><p>try_files指令通常使用$uri 变量，它表示域名后的URL部分。</p><p>在以下示例中，如果客户端请求的文件不存在，则NGINX提供默认的GIF文件。 当客户端请求（例如）<a href="http://www.domain.com/images/image1.gif%E6%97%B6%EF%BC%8CNGINX%E9%A6%96%E5%85%88%E5%9C%A8%E7%94%B1%E6%A0%B9%E6%88%96alias%E6%8C%87%E4%BB%A4%E6%8C%87%E5%AE%9A%E7%9A%84%E6%9C%AC%E5%9C%B0%E7%9B%AE%E5%BD%95%E4%B8%AD%E6%9F%A5%E6%89%BE%E9%80%82%E7%94%A8%E4%BA%8E%E8%AF%A5%E4%BD%8D%E7%BD%AE%E7%9A%84%E5%9B%BE%E5%83%8F%EF%BC%88%E6%9C%AA%E7%A4%BA%E5%87%BA%EF%BC%89" target="_blank" rel="noopener">http://www.domain.com/images/image1.gif时，NGINX首先在由根或alias指令指定的本地目录中查找适用于该位置的图像（未示出）</a> 在代码段中）。 如果image1.gif不存在，NGINX寻找image1.gif /，如果不存在，它会重定向到/images/default.gif。 该值与第二个位置指令完全匹配，因此处理停止，NGINX提供该文件并将其标记为缓存30秒。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">location /images/ &#123;</span><br><span class="line"> try_files $uri $uri/ /images/default.gif;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">location = /images/default.gif &#123;</span><br><span class="line"> expires 30s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="示例-标准化域名"> 示例 – 标准化域名</span></h2><p><strong>NGINX重写规则最常见的用途之一是捕获弃用或非标准版本的网站域名，并将其重定向到当前域名。</strong> 有几个相关的用例。</p><h3><span id="从旧的名称重定向到新的名称"> 从旧的名称重定向到新的名称</span></h3><p>此示例NGINX重写规则将请求从 <a href="http://www.old-name.com" target="_blank" rel="noopener">www.old-name.com</a> 和 <a href="http://old-name.com" target="_blank" rel="noopener">old-name.com</a> 永久重定向到 <a href="http://www.new-name.com" target="_blank" rel="noopener">www.new-name.com</a> ，使用的两个NGINX变量是从原始请求URL中捕获的值。 scheme是原始协议 （http或https），request_uri是完整的URI（域名后），包括参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    listen 443 ssl;</span><br><span class="line">    server_name www.old-name.com old-name.com;</span><br><span class="line">    return 301 $scheme://www.new-name.com$request_uri;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因为$ request_uri捕获了域名后面的URL的部分，所以如果新旧网站之间存在一一对应的页面，则此重写是合适的（例如， <a href="http://www.new-name.com/about" target="_blank" rel="noopener">www.new-name.com/about</a>  与 <a href="http://www.old-name.com/about" target="_blank" rel="noopener">www.old-name.com/about</a> 相同的基本内容）。<br>如果您在更改域名之外重新组织了网站，则可以通过省略 $request_uri来将所有请求重定向到主页，这样更安全：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    listen 443 ssl;</span><br><span class="line">    server_name www.old-name.com old-name.com;</span><br><span class="line">    return 301 $scheme://www.new-name.com;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一些其他关于在NGINX中重写URL的博客对这些用例使用rewrite指令，像这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># NOT RECOMMENDED</span><br><span class="line">rewrite ^ $scheme://www.new-name.com$request_uri permanent;</span><br></pre></td></tr></table></figure><p><strong>这比return指令效率低，因为它需要处理NGINX正则表达式，虽然是一个简单的（符号[^]，它匹配完整的原始URL）。</strong><br>对应的return指令对于读者来说也更容易解释：return 301更清楚地指示NGINX返回代码301而不是rewrite …permanent 。</p><h3><span id="添加和删除www前缀"> 添加和删除www前缀</span></h3><p>如下示例添加和删除www前缀：</p><p>添加’www’</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    listen 443 ssl;</span><br><span class="line">    server_name domain.com;</span><br><span class="line">    return 301 $scheme://www.domain.com$request_uri;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>删除’www’</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line"> listen 80;</span><br><span class="line"> listen 443 ssl;</span><br><span class="line"> server_name www.domain.com;</span><br><span class="line"> return 301 $scheme://domain.com$request_uri;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同样，<strong>return 效率更高于rewrite</strong>，如下。 重写需要解释正则表达式 – ^（。*）$ – 并创建一个自定义变量（$ 1），实际上等价于内置的 $request_uri 变量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># NOT RECOMMENDED</span><br><span class="line">rewrite ^(.*)$ $scheme://www.domain.com$1 permanent;</span><br></pre></td></tr></table></figure><h3><span id="重定向所有的流量到正常的域名"> 重定向所有的流量到正常的域名</span></h3><p>这是一种特殊情况，当请求网址与任何服务器和位置块不匹配时，可能是因为域名拼写错误，将入站流量重定向到网站的主页。 它的工作原理是将default_server参数与listen指令组合，将下划线 _ 作为server_name指令的参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80 default_server;</span><br><span class="line">    listen 443 ssl default_server;</span><br><span class="line">    server_name _;</span><br><span class="line">    return 301 $scheme://www.domain.com;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们使用下划线作为server_name的参数，以避免无意中匹配真实的域名 – 可以安全地假设任何站点都不会有下划线作为其域名。 但是，与配置中的任何其他服务器块不匹配的请求都在这里，而且监听的default_server参数告诉NGINX为它们使用此块。 通过从重写的URL中省略$ request_uri变量，我们将所有请求重定向到主页，这是一个好主意，因为具有错误域名的请求尤其可能使用网站上不存在的URI。</p><h2><span id="示例强制所有请求使用-ssltls"> 示例：强制所有请求使用 SSL/TLS</span></h2><p>此server 块强制所有访问者对您的网站使用安全（SSL / TLS）连接。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name www.domain.com;</span><br><span class="line">    return 301 https://www.domain.com$request_uri;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一些关于NGINX重写规则的其他博客使用if测试和rewrite指令来处理这个用例，像这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># NOT RECOMMENDED</span><br><span class="line">if ($scheme != &quot;https&quot;) &#123;</span><br><span class="line">    rewrite ^ https://www.mydomain.com$uri permanent;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但是此方法需要额外的处理，因为NGINX必须同时判断if条件并处理rewrite指令中的正则表达式。</p><h2><span id="示例-为wordpress网站启用漂亮的固定链接"> 示例 – 为WordPress网站启用漂亮的固定链接</span></h2><p>NGINX和NGINX Plus是使用WordPress的网站非常受欢迎的应用交付平台。 下面的try_files指令告诉NGINX检查文件uri，然后是目录uri/的存在。 如果文件或目录不存在，NGINX将返回到/index.php的重定向并传递query-string参数，这些参数由$args参数捕获。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">    try_files $uri $uri/ /index.php?$args;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="示例-丢弃不受支持的文件扩展名的请求"> 示例 – 丢弃不受支持的文件扩展名的请求</span></h2><p>由于种种原因，您的网站可能会收到以与您未运行的应用程序服务器相对应的文件扩展名结尾的请求网址。 在本示例中，从Engine Yard博客，应用程序服务器是Ruby on Rails，因此无法处理由其他应用程序服务器（活动服务器页面，PHP，CGI等）处理的文件类型的请求，并且需要拒绝它们。 在将动态生成的资源的任何请求传递给应用程序的server 块中，此位置指令在非Rails文件类型的请求到达Rails队列之前丢弃该请求。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">location ~ \.(aspx|php|jsp|cgi)$ &#123;</span><br><span class="line">    return 410;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>严格地说，响应代码410（Gone）意在用于在所请求的资源在该URL处可用但不再是可用的情况，并且服务器不知道其当前位置（如果有的话）。 它相对于响应代码404的优点是它明确地指示资源永久不可用，因此客户端不会再次发送请求。</p><p>您可能希望通过返回响应代码403（禁止）和诸如“服务器仅处理Ruby请求”这样的解释作为文本字符串，为客户端提供更准确的失败原因指示。 作为替代，deny all伪指令返回403，不作解释：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">location ~ \.(aspx|php|jsp|cgi)$ &#123;</span><br><span class="line">    deny all;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码403隐式地确认所请求的资源存在，因此如果想要通过向客户端提供尽可能少的信息来实现“通过模糊的安全性”，则代码404可能是更好的选择。 缺点是客户端可能会反复重试请求，因为404不指示失败是临时还是永久。</p><h2><span id="示例-配置自定义重新路由"> 示例 – 配置自定义重新路由</span></h2><p>在本示例中，您有一个资源，作为一组URL的控制器。 您的用户可以为资源使用更易读的名称，并重写（不重定向）它由控制器在listing.html处理。</p><p><code>rewrite ^/listings/(.*)$ /listing.html?listing=$1 last;</code></p><p>作为示例，用户友好的URL <a href="http://mysite.com/listings/123" target="_blank" rel="noopener">http://mysite.com/listings/123</a> 被重写为由 listing.html 控制器处理的URL， <a href="http://mysite.com/listing.html?listing=123" target="_blank" rel="noopener">http://mysite.com/listing.html?listing=123</a> 。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">翻译自：https://www.nginx.com/blog/creating-nginx-rewrite-rules/</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在这篇文章中，我们讨论如何创建NGINX重写规则return, rewrite,和try_files指令。&lt;/p&gt;
    
    </summary>
    
      <category term="nginx" scheme="yunke.science/categories/nginx/"/>
    
    
      <category term="nginx" scheme="yunke.science/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title>对于大多数普通人而言，情商比智商更重要？</title>
    <link href="yunke.science/2018/04/25/meiwen02/"/>
    <id>yunke.science/2018/04/25/meiwen02/</id>
    <published>2018-04-25T02:10:35.000Z</published>
    <updated>2018-04-25T02:14:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>如何在麻将桌上赢你们公司总裁五块钱？</p><p>打五块钱一胡的麻将。你运筹帷幄，察言观色，声东击西，最后凭借高超的牌技和一点点运气，赢回这五块钱。观者都为你赞叹，牌技真好。</p><p>但我们的问题是，如何在麻将桌上赢你们公司总裁五块钱？里面最大的挑战其实是：你的总裁凭什么要跟你坐一桌打麻将。</p><a id="more"></a><p>没有想到这隐含一问的人，都觉得牌技是取胜的关键。想到的人才知道，想秀出你的牌技，得先上得了桌才行。很简单的道理，人有时候意识不到，因为大多数人开始思考这个问题的时候，他们已经坐定在某张桌子跟前，眼前就只有这张桌上的一局牌了。他们看见左手边的牌友艺高人胆大，打得风声水起，看见右边的牌友缩手缩脚，输得一塌糊涂，于是他们觉得牌技是这个世界上最重要的事情，就像他们觉得情商是生活中最重要的东西一样。不知道在人生中的某一个时刻，他们会不会自问：如果这个世界上还存在其它生活方式呢？</p><p>智商本身意义不大，但是智商与努力能带给人更好的专业技术，把人带到更高的平台上。情商决定你怎么和人打交道，但是你的技术和专业从最一开始就决定你和谁打交道。情商高永远不会吃亏的，它特别有用，情商低很危险，会限制人的发展。但这一切的前提是，99%的人，只能把他们的情商施展在自己身处的层次上。</p><p>某A顶尖高校毕业，在银行业工作了十年，而后MBA，会三国语言，在多个国家工作过。之前和他的执行经理说起过这个人。因为我和执行经理私交比较好，他比较隐晦的提出，这个人工作还行，但和整个团队都不是很融洽，简而言之就是有点儿“奇葩”。当时我内心也是唏嘘了一阵，这么漂亮的简历，可能就因为和人都处不来，这辈子也就是小中层到头了，情商真的很重要。但是我写这个故事，是想告诉大家，这个人情商不够，受到限制，所以这辈子只能是年薪八十万港币的小职员了，只能在青衣买一个二手小房。你明白吗？大多数自认情商很高的人，这辈子有没有可能达到年薪八十万港币的水准？奋斗一辈子有没有可能在香港买房？没有。</p><p>和一个做教授的朋友聊天，说起某人最近在学界混得风声水起，朋友非常不忿，说其研究品味不高，发的都是灌水文章，因为“会来事儿”，受到大佬赏识，“给”了他一个关键的委员会的位子和其它一些好处，后来就顺风顺水了。朋友就是那种闷头做研究的闷葫芦，有时候说话太直，我也接受不了。他读博的时候就因为这样的原因换过一次老板，七八年才毕业，现在还在等副教授。没到副教授，年年都得跟系里汇报自己的工作进度，接受评审，大抵是有些不得志的。我写这个故事，就是告诉大家，我这个朋友情商不行，哪怕是在学界也不能如鱼得水，没有大佬提携，自己一个人苦苦打拼，可能终其一生，也不过就是211高校的一名普通教授，而已。大多数自认情商很高的人，终其一生，能成为高校教授吗？不能。</p><p>上面的例子可能太玄乎了，说个现实点儿的。之前跟京城某企业家吃饭，说起家里的子侄小辈，企业家很感慨，说自己哥哥一辈子不容易，他要把哥哥的两个儿子照顾好。说大侄子“特别好”，怎么好？对爷爷奶奶孝顺，有眼色。企业家在北京多年，不能回家尽孝，大侄子就经常陪着爷爷奶奶来北京小住几天，照顾得妥妥帖帖，他看得心里喜欢，大侄子县城的房子是他给出的钱。二侄子呢？“这孩子实在”。吃饭的时候二侄子也在，我也看出来了，小子确实“实在”，吃饭的时候也不说话，干坐着，席间他叔提醒了他多次“xxx, 给咱们把酒倒上”。故事讲完了，看出来问题在哪儿了吗？大侄子有眼色，在县城加油站工作。二侄子没眼色，却给安排在北京工作，跟我们坐一桌吃饭。因为什么？大侄子没念出书来，二侄子是本科毕业。再护犊子，再偏袒，再喜欢，没念出书来，给点儿钱还行，但真没办法培养你。</p><p>大家都是MBA，你情商高，升职就比别人快。大家都拿到教职，你情商高，你就能整合更多的资源。大家都是名校毕业专业对口的本科生，你情商高，你就能拿到更好的工作。可即便是这些人，也只能把目光局限在自己眼跟前的这一摊事情上，而忘了，可能80%的成败，已经由你所能到达的平台所决定了。</p><p>自己觉得情商很高，靠情商做成很多事情的朋友，也可以深刻反思一下，你之所以能搞定这么多人，谈成这么多合作，真的是全凭一张嘴？你们自己觉得情商高，可是全中国情商比你们高的人不知道有多少，领导的司机，服装店卖衣服的明星销售员，论察言观色，论把握人的心理，他们不知道比你高到哪里去了。对高大上的知乎用户说，虽然你只是个普通人，但经你手的资源，你接触的人，肯定还是和上述这些摸爬滚打的人不一样的。他们过得很辛苦，而你有可能会小有成就。区别真的不是拿情商解释得了的，尤其是在今天，市场经济愈发成熟，产业不断升级转型，小学毕业一夜爆富越来越不可能。身无一技之长，空有和人打交道的本事的人，生存空间会越来越小。</p><p>天赋异禀 的人，即便早年走了弯路，读书没读好，早期的几步没走好，也能迎头赶上，突破平台对自己的限制。而越是普通人，突破平台对自己的限制的可能就越低，你上到的平台有多高，你大概就能成就多高。再加上如今甚嚣尘上的阶级固化说，你再不靠自己的智力劳动，再不重视专业资本的积累，再不上到一个适合发展的平台，你要到哪里去贩卖你的情商？</p><p>生活中有很多后辈就情商这个问题问过我，我都礼貌并隐晦地指出，不要瞎想那么多，先好好努力，好好提高自己，考上本科，学好知识，工作里面好好积累经验，以后争取在你们当地的业内有一点儿话语权。</p><p>不礼貌一点的说法，就是要争取上桌，先上了桌再说。</p><p>跟总裁打麻将输了的人，那也是跟总裁打麻将输了。跟阿土伯打麻将赢了的人，也不过就是赢了阿土伯而已。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">作者：公子V</span><br><span class="line">链接：https://www.zhihu.com/question/20252336/answer/375473310</span><br><span class="line">来源：知乎</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如何在麻将桌上赢你们公司总裁五块钱？&lt;/p&gt;
&lt;p&gt;打五块钱一胡的麻将。你运筹帷幄，察言观色，声东击西，最后凭借高超的牌技和一点点运气，赢回这五块钱。观者都为你赞叹，牌技真好。&lt;/p&gt;
&lt;p&gt;但我们的问题是，如何在麻将桌上赢你们公司总裁五块钱？里面最大的挑战其实是：你的总裁凭什么要跟你坐一桌打麻将。&lt;/p&gt;
    
    </summary>
    
      <category term="美文" scheme="yunke.science/categories/%E7%BE%8E%E6%96%87/"/>
    
    
      <category term="情商" scheme="yunke.science/tags/%E6%83%85%E5%95%86/"/>
    
      <category term="智商" scheme="yunke.science/tags/%E6%99%BA%E5%95%86/"/>
    
  </entry>
  
</feed>
